---
title: "[논문리뷰] Let’s Go Real Talk- Spoken Dialogue Model for Face-to-Face Conversation (ACL 2024)"
date: 2025-03-14 16:00:00 +0900
categories:
  - Paper Review
tags:
  - ACL 2024
  - Empathetic Dialogue Systems
  - Multimodal
  - Speech
  - Dataset Construction
---

요약: 이 논문에서는 음성 및 시각 정보를 처리하여 오디오-비주얼 대화를 생성하는 새로운 Face-to-Face 대화 모델과 340시간 분량의 대화 데이터로 구성된 MultiDialog 코퍼스를 소개합니다. 이를 통해 중간 텍스트 없이 아바타 챗봇 시스템의 개발에 대한 기초 작업을 제시합니다.

---

# 1 Introduction

- SDS(Spoken Dialogue System) 또는 대화 에이전트는 자연어 대화를 위해 사용자 음성을 인식하고 상황에 맞는 반응을 음성으로 제공
- 주로 고객 서비스나 음성 비서와 같은 인간-컴퓨터 상호작용에 활용
- 대면 소통 시 시각 정보(얼굴 표정, 제스처, 감정 등)를 통해 말의 의미와 화자의 의도를 더욱 깊게 이해
- 초점: 최초로 대면 대화를 촉진하는 오디오-비주얼 SDS 탐구
- 대화 시스템 개발의 핵심: 대량의 고품질 대화 데이터
- 기존 대화 시스템은 주로 텍스트 기반으로, 텍스트 대화 데이터셋이 풍부함
- 최근 몇몇 오디오 대화 데이터셋이 발표되었지만, 시각적 요소가 포함된 데이터셋은 15시간 미만의 제한된 규모
- 해결책: MultiDialog, 340시간 분량의 오디오-비주얼 대화 코퍼스 
  - 약 9,000개의 대화 기록으로 이루어져 있으며, 실제 대화에서 수집된 방대한 멀티 턴 대화 코퍼스인 TopicalChat에서 유래
  - 각 발화에 대한 감정 주석이 포함되어 있음
- MultiDialog 기반의 audio-visual 대화 모델 제안:
  - 사용자 입력을 오디오-비주얼 음성으로 처리하고, 반응을 오디오-비주얼 음성으로 생성
  - 오디오-비주얼(AV) 음성 토큰을 도입하고, 이를 통해 사전 훈련된 대형 언어 모델에 통합
  - 출력은 사용자와의 직관적인 상호작용을 위해 Talking Face 비디오로 합성됨
- 기여:
  1. 다중 모드 음성을 처리하고 생성하는 최초의 대면 대화 모델 소개
  2. 대면 대화 시스템 구축을 위한 첫 대규모 다중 모드 대화 코퍼스, MultiDialog 제공
  3. 사전 훈련된 대형 언어 모델을 활용한 공동 음성-텍스트 사전 훈련이 원래 모델의 지식 유지를 개선함을 증명

---

# 2 Related Work

- 텍스트 분류 문제는 과거 몇 년간 많은 연구가 진행됨
  - 전통적인 방법은 Bag-of-Words 모델을 사용
  - 최근에는 컨볼루션 신경망(CNN)과 순환 신경망(RNN)의 사용 증가
  
- 대화형 AI 분야에서는 여러 접근법이 존재
  - 대화 시스템의 성능 개선을 위한 다양한 데이터 세트와 모델
  - 대부분의 시대적 흐름에 따라 모델의 복잡성이 증가함  

- 여러 연구에서 모델의 정확도 향상, 비용 절감 등의 결과 도출
  - 일반화 성능을 높이기 위해 다양한 기법 사용
  - 예를 들어, 정규화 방법과 앙상블 기법이 널리 활용됨

- 몇몇 연구에서는 자기 주의(attention) 메커니즘의 효과 입증
  - 이는 특정 입력 값에 더 집중할 수 있는 기능 제공
  
- 마지막으로, 대규모 사전 훈련된 모델(BERT, GPT 등)이 나타남
  - 사전 훈련은 적은 데이터로도 우수한 성능 발휘
  - 이들 모델은 다양한 NLP 작업에서 state-of-the-art 성능 기록  

- 이는 모든 NLP 관련 문제에 적용 가능함을 보여줌
  - $$P(y \vert x)$$ 형태의 확률적 모델링 접근도 증가 추세
  - 데이터의 양과 다양성이 중요한 역할 수행함

---

# 2.1 Spoken Dialogue Dataset

<img width="691" alt="image" src="https://github.com/user-attachments/assets/21f362b2-ff06-469d-ace9-0fdbbbf9513e" />

- 최근 몇 년간 음성 대화 데이터셋 개발이 인간 행동 이해 및 실제 대화를 모방하는 음성 대화 시스템 구축에 중요한 역할을 함.
- 초기 음성 데이터셋은 감정 및 의도 분석에 중점을 두어 음성 대화 시스템의 기초를 마련함.
- IEMOCAP (Busso et al., 2008) 및 MELD (Poria et al., 2018)은 대화의 감정적 동력을 연구하기 위해 오디오 및 비디오 녹음을 포함함.
- 감정 이해 외에도 DSTC2 (Henderson et al., 2014)는 사용자 목표 예측을 위한 대화 상태 추적에 집중한 전화 기반 음성 대화를 제공함.
- 최근 음성 대화 데이터셋은 현실적인 대화 시스템 모델링을 위해 구축됨.
  - Expresso (Nguyen et al., 2023a): 자연어 음성 합성을 위한 26가지 표현 스타일을 가진 음성 대화 제공.
  - DailyTalk (Lee et al., 2023) 및 SpokenWOZ (Si et al., 2023): 음성 대화를 위한 음성-텍스트 대화 데이터 제공.
- 기존 연구는 음성 대화 시스템의 발전에 기여하였으나, 대화 데이터셋은 규모가 제한적이며 오디오 및 텍스트로만 구성되어 있어 시각적 단서를 포함한 오디오-비주얼 음성 대화 시스템 개발에 제약이 있음.
- 이러한 한계를 극복하기 위해, 대화 데이터를 확대하고 시각적 모달리티로 확장하며, 대규모 멀티모달 음성 대화 데이터셋인 MultiDialog를 소개함.
- 기존 멀티모달 대화 데이터셋과 MultiDialog에 대한 요약은 표 1에 제시됨.

---

# 2.2 Spoken Dialogue Models

- 오디오 언어 모델은 트랜스포머 기반 아키텍처로 구동되어 음성 처리 분야에서 놀라운 발전을 이루었음.
- 연속적인 음성을 이산적인 표현 세트로 취급하여, 자연어 처리(NLP) 기술을 적용할 수 있음.
- 음성 합성, 음성 번역 및 음성 인식 분야에서 두드러진 발전을 이루었으나, 음성 대화 시스템은 데이터 부족으로 상대적으로 연구가 미비함.
- 데이터 문제를 해결하기 위해 대규모 언어 모델(LLMs)의 힘을 이용한 몇 가지 연구가 진행됨:
  - **SpeechGPT**: 음성을 이산 음성 토큰으로 변환하고, 페어드 음성 데이터, 음성 지침 데이터, 모드 지침 데이터를 사용하여 3단계 훈련 파이프라인 설계.
  - **AudioGPT**: LLMs에 외부 도구 제어 명령 생성 지시한 후 이를 LLMs에 입력.
  - **d-GSLM**: 자연스러운 턴 테이킹 대화를 생성하기 위해 2채널 대화를 모델링.
- 다중 모달 대규모 언어 모델(MM-LLM)은 시각적 입력과 출력을 동시에 처리할 수 있으나, 주로 이미지 캡셔닝 및 편집과 같은 작업을 위한 보조 정보로 시각 정보를 사용함.
- 우리의 목표는 음성과 관련된 얼굴 움직임을 포함한 오디오-비주얼 대화 시스템을 구축하여 음성 콘텐츠 이해를 향상시키고, 실제 대면 대화를 모방하여 커뮤니케이션 경험을 풍부하게 하는 것임.

---

# 3 MultiDialog Dataset

- **데이터셋 설명**: MultiDialog 데이터셋은 여러 개의 대화가 포함된 데이터셋으로, 다양한 도메인에서 생성된 여러 개의 대화 예제를 포함하고 있음.
- **목적**: 이 데이터셋은 대화 모델의 학습 및 평가를 위해 설계되었으며, 인간과의 대화를 시뮬레이션하는 데 활용됨.
- **구성**: 각 대화는 발화(말하는 내용)와 발화자(누가 말하는지)에 대한 정보를 담고 있어, 복잡한 대화 구조를 형성함.
- **데이터 유형**: 대화는 질의응답 형식 및 대화형 상호작용 모두를 포함하고 있으며, 질문에 대한 답변뿐만 아니라 자연스러운 대화 흐름을 유지하는 데 초점을 맞추고 있음.
- **특징**:
  - 다양한 주제와 상황을 포괄하여 폭넓은 언어 처리 능력을 요구함.
  - 발화는 연속적인 대화 맥락 내에서 의미를 가지며, 이는 모델이 맥락을 이해하고 적절한 응답을 생성하는 데 중요함.
- **수학적 표현**: 대화 모델의 성능 평가에 있어 다음과 같은 지표를 사용함:
  - $$ P = \frac{C}{T} $$
    - 여기서, $$ P $$는 정확도, $$ C $$는 올바르게 생성된 응답의 수, $$ T $$는 총 생성된 응답의 수를 나타냄.
- **응용**: MultiDialog 데이터셋은 챗봇 개발 및 자연어 처리(NLP) 연구에서 널리 사용되며, 다양한 대화형 AI 시스템 개발에 기여함.

---

# 3.1 Preparation

- 오디오-비주얼 대화 녹음을 위해:
  - 12명의 유창한 영어 화자 모집
  - 성별, 나이, 국적 다양성 포함
  - 참가자 나이: 20세에서 30세
  - 6개국 출신, 남성 6명, 여성 6명

- 대화 스크립트 출처:
  - TopicalChat 데이터셋 (Gopalakrishnan et al., 2023)
  - 실제 인간 대화에서 수집된 지식 기반 데이터셋

- 주제:
  - 패션, 정치, 책, 스포츠, 일반 오락, 음악, 과학 & 기술, 영화 등 8개 주제

- 감정 주석:
  - 8가지 감정: 
    - 혐오(Disgusted)
    - 분노(Angry)
    - 두려움(Fearful)
    - 행복(Happy)
    - 슬픔(Sad)
    - 놀람(Surprised)
    - 중립(Neutral)
    - 호기심(Curious)

- 대화 구조:
  - 대화 상대에 대한 명확한 역할 없음 (‘화자’ 또는 ‘청중’)
  - 자연스러운 상호 작용 방식

- 선택 이유:
  - 주제 다양성, 감정 주석, 자연스러운 인간 대화의 표현
  - TopicalChat을 다중모달 대화 데이터셋 구축의 기초로 선정.

---

# 3.2 Recording

- 데이터는 전문 녹음 스튜디오에서 녹음되었으며, 배경 소음이 최소화된 상태에서 진행되었음 (부록 A.4 참조).
- 녹음 세션 동안 두 명의 대화 상대가 나란히 앉아 별도의 카메라와 마이크로 녹음됨.
- 카메라 위치는 각 개인의 키에 맞게 조정되어 어깨에서 상반신을 촬영함.
- 참가자들은 각 발화에 대한 감정 주석을 전달하는 주어진 스크립트에 따라 연기하도록 요청받음.
- 감정에 대한 시각적 및 음성 신호에 대한 자세한 지침이 제공됨:
    - **중립(Neutral)**: 정적 얼굴, 감정 없음, 자연스러운 정보로 말하기.
    - **행복(Happy)**: 입꼬리 올림, 뺨 올림, 입술 벌림, 기쁜 톤으로 말하기.
    - **슬픔(Sad)**: 눈꺼풀 처짐, 입꼬리 약간 내림, 슬픈 낮은 톤으로 말하기.
    - **두려움(Fearful)**: 눈썹 올림, 눈을 크게 뜸, 부드럽고 낮은 톤으로 말하기.
    - **놀람(Surprise)**: 눈썹 올림, 눈을 크게 뜸, 입을 더 넓게 벌림, 흥분된 높은 톤으로 말하기.
    - **혐오(Disgusted)**: 눈썹 내림, 코를 찡그림, 뺨 올림, 윗입술 올림, 일반 톤으로 혐오감을 담아 말하기.
    - **분노(Anger)**: 눈썹 내림, 눈 날카롭게 쳐다봄, 강한 높은 톤으로 말하기.
- '중립(Neutral)'레이블과 '더 깊이 탐구하고 싶음(Curious to dive deeper)'레이블은 시각적으로 뚜렷한 차이가 없기 때문에 단일레이블 '중립(Neutral)'으로 통합됨.
- 지침 외에도 화면에 샘플 이미지를 보여주어 배우들이 감정에 해당하는 얼굴 표정을 흉내 낼 수 있도록 함.
- 참가자들에게 발화가 다른 참가자로 넘어갈 때 자연스럽게 반응하도록 지시하였으며, 버튼을 눌러 다음 발화로 넘어가도록 함. 
- 이에 따라 각 턴의 시작 및 종료 시간이 기록됨.
- 오디오 스트림은 48kHz의 mono WAV 포맷으로 녹음되었고, 비디오 스트림은 30fps의 Full HD로 녹음됨.

---

# 3.3 Post-Processing

<img width="691" alt="image" src="https://github.com/user-attachments/assets/f93dd146-23a6-49e5-b3e6-bb9fb1e797ba" />

- 데이터 정제를 위해 주석자를 통해 오디오-비쥬얼 기록에서 오디오와 비쥬얼 스트림 간의 불일치를 확인
- 주석자가 시작 시간을 조정하여 불일치를 수동으로 수정
- 오디오 또는 비쥬얼 스트림이 누락된 기록은 필터링
- 각각의 턴에 대한 기록된 타임스텝을 기반으로 대화와 턴별로 기록을 분할
- 후처리된 MultiDialog 데이터셋에는 약 340시간의 오디오-비쥬얼 비디오와 9,000개의 대화가 포함, 총 6 쌍의 대화 파트너 존재
- 최종 데이터셋 통계는 표 2에 표기
- 엄격한 주석 평가를 기반으로 선택된 골드 감정 대화 하위 집합도 공개
- 더 자세한 정보는 부록 A.3.1을 참조

---

# 4 Audio-Visual Spoken Dialogue System

- 제안된 MultiDialog 데이터셋에 기반한 오디오-비주얼 대화 시스템 소개
- 시스템의 주요 구성 요소:
  - 1) 사용자 얼굴 비디오의 오디오-비주얼 신호를 이해하고, 적절한 응답을 생성
  - 2) 오디오-비주얼(AV) 음성을 이산 표현으로 인코딩하여 AV 스피치 토큰 생성
  - 3) AV 스피치 토큰을 사용하여 다중 모드 말하기 대화 언어 모델링 수행
  - 4) 출력 AV 스피치 토큰을 오디오 및 비주얼 공간에 투영하여 직접적인 얼굴-대-얼굴 대화 지원
- 시스템은 AV 스피치 토큰을 가상 텍스트로 사용하여 대화 효율성 증대

---

# 4.1 Audio-Visual Speech Encoding

- 오디오 및 비주얼 모달리티의 통합을 통해 대화 시스템의 언어 이해 개선
  - 화자의 입 움직임에서 오는 비주얼 신호가 청각 신호를 보완
  - 특히 소음이 많은 환경에서 성능 저하 방지
  
- 오디오 및 비주얼 입력을 오디오-비주얼 스피치 토큰으로 모델링
  - 최근 자가 감독 스피치 모델에서 추출한 이산 스피치 토큰 활용
  - 오디오-비주얼 아카운트 토큰(AV speech tokens)으로 오디오 및 비주얼 스트림을 토큰화

- AV-HuBERT 모델 사용
  - 스피치 인식 이해를 위한 최첨단 자가 감독 프레임워크
  - 원시 오디오-비주얼 영상에서 이산 클러스터 예측 훈련
  
- AV 스피치 토큰의 추출 및 양자화
  - 언어 및 음성 정보 추출
  - AV 스피치 토큰을 의사 텍스트로 취급하여 Audio-Visual Spoken Dialogue LM 학습

- AV 스피치 토큰을 사용하여 MultiDialog 데이터셋에서 대화 모델 훈련
  - 텍스트로 사전 훈련된 언어 모델 사용 시 성능 및 수렴 속도 향상
  - AV 스피치 토큰과 원래 텍스트 어휘 결합하여 확률 모델링

- 손실 함수 표현
  - 다음 토큰 예측의 음성 확률을 나타내는 부정 로그 우도: 
  $$L = -\sum_{i=1}^{N} \log p(t_i \vert t_1, \ldots, t_{i-1})$$

- 대화 모델을 위한 새로운 공동 스피치-텍스트 사전 훈련 방안 도입
  - 두 화자 간 다이얼로그로 구성된 대화의 턴을 세그먼트화
  - 각각 AV 스피치 토큰과 텍스트 토큰 쌍을 생성하고 결합

- 두 단계를 통해 LLM을 AV 스피치 토큰 기반으로 변환
  - AV 스피치 토큰 해석 및 생성 훈련과 텍스트 및 AV 스피치 토큰 기반 대화 학습
   
- 예시:
  - AV 스피치 → 텍스트 및 텍스트 → AV 스피치 교육 목표를 통해 손실 함수 작성
  - 대화 언어 모델링을 위해 대화의 화자 프리픽스 토큰 추가

- 지속적인 훈련 및 미세 조정
  - 균형 잡힌 AV 스피치와 텍스트 혼합으로 대화 응답 생성 훈련

- AV 스피치 토큰을 입력으로 하여, 자연스러운 대화 응답을 생성하는 새로운 얼굴 생성 방법론 제시
  - 얼굴 비디오를 통한 응답 생성 및 개인화된 스피치 생성 접근 방식 포함

---

# 4.2 Audio-Visual Spoken Dialogue Language Modeling

<img width="817" alt="image" src="https://github.com/user-attachments/assets/07587d47-5d70-45a7-bcfc-45e76e6ecaf8" />

- 오디오 및 시각 모달리티 통합:
  - 대화 시스템의 음성 내용 이해 개선
  - 시각적 단서(입술 움직임)가 소리 신호를 보완, 특히 소음 환경에서 성능 향상 (Afouras et al., 2018)

- 모델링 접근법:
  - 오디오 및 비주얼 정보를 통합하여 오디오-비주얼 음성 토큰 생성
  - 최근의 자가 지도 학습 음성 모델에서 추출된 이산 음성 토큰 활용 (Schneider et al., 2019 등)

- AV-HuBERT:
  - 오디오-비주얼 이해를 위한 최첨단 자가 지도 학습 프레임워크
  - 원시 오디오-비주얼 얼굴 비디오에서 이산 클러스터를 예측하여 훈련 (Hassid et al., 2023)

<img width="402" alt="image" src="https://github.com/user-attachments/assets/4cbfb561-0b11-440f-9ee6-d2d55463f472" />

- AV 음성 토큰:
  - 언어적 및 음소적 정보 추출, 가짜 텍스트로 간주하여 Audio-Visual Spoken Dialogue LM 훈련에 사용

- 모델 훈련:
  - MultiDialog 데이터세트에서 AV 음성 토큰을 사용하여 훈련
  - 기존의 텍스트로 사전 훈련된 언어 모델(LLM) 초기화 후 더 나은 성능과 빠른 수렴 달성 (Hassid et al., 2023)

- 손실 함수:
  - 다음 토큰 예측을 위한 음성 토큰 및 텍스트 토큰 조합 모델링
  - 손실 함수는 $$L = -\sum_{i=1}^{N} \log p(t_i \vert t_1, \ldots, t_{i-1})$$로 표현

- 공동 훈련 접근법:
  - 음성-텍스트 훈련을 활용한 새로운 공동 사전 훈련 계획 도입
  - AI와 사용자 간 대화 구조화 및 모드 프리픽스 토큰을 통한 훈련

- 성능 평가:
  - 모델의 대화 언어 모델링 손실 함수는 $$L_{\text{dialog}} = -\sum_{k=1}^{K}\sum_{n=1}^{N_k} \log p(T_{ai,n_k} \vert T_{ai,<n_k}, T_{<k})$$로 표현되어 대화의 질을 정량화

- 단계별 훈련:
  - 음성 및 AV 음성 토큰 간 훈련 후, AV 음성 토큰 기반 대화로 미세 조정하여 실시간 상호작용 가능

- 데이터 생성:
  - AV 음성 토큰을 오디오 및 시각으로 변환하여 응답을 생성하는 토킹 페이스 비디오 생성
  - 고해상도 비디오 생성을 위한 얼굴 향상 기능 통합

- 대화 생성 예시:
  - 다양한 대화의 오디오 비주얼 응답 생성 결과 제공
  - 모델의 문맥 일관성 및 적절성 강조

---

# 4.3 Audio-Visual Generation

- 생성된 A-V 음성 토큰은 오디오와 비주얼로 투영되어 응답을 생성, 결과는 말하는 얼굴 비디오 형태
- 오디오-비주얼 생성기는 다음으로 구성됨:
  - 길이 예측기
  - 토큰 기반 음성 디코더
  - 토큰 기반 얼굴 디코더
- 언어 모델은 중복이 감소된 A-V 음성 토큰을 사용하여 훈련됨
  - 처음에 길이 예측기를 통해 원래 길이로 복원
- 토큰 기반 음성 디코더와 얼굴 디코더는 기존의 오디오 생성기 및 말하는 얼굴 생성기를 바탕으로 수정됨
  - A-V 음성 토큰을 입력으로 처리하도록 훈련됨
- 화자 정체성 정보 통합:
  - 타겟 정체성 샘플 오디오에서 화자 임베딩을 추출
- 타겟 정체성의 얼굴 및 포즈 사전 활용하여 원하는 정체성을 가진 말하는 얼굴 비디오 생성 가능

---

# 5 Experimental Setup

---

# 5.1 Evaluation Metrics

- **평가 목적**: 음성과 비디오의 의미적 품질 및 생성 품질 평가
- **의미적 품질 평가**:
  - 생성된 음성-비주얼 출력에서 전사 생성
  - ASR 모델(Shi et al., 2021) 사용
  - 텍스트 기반 대화 생성에 사용되는 표준 지표 적용:
    - **log-perplexity (PPL)**: 
      - Dialo-GPT 모델(Zhang et al., 2019)을 사용하여 각 발화에 대해 계산 후 테스트 세트에서 평균화
    - **기타 지표**: BLEU, METEOR, F1, D-1, D-2 
- **비디오 생성 품질 평가**:
  - TFG(Temporal Feature Generation)에 사용되는 지표 채택
  - **Fréchet Inception Distance (FID)**: 
    - 시각적 품질 측정 (Heusel et al., 2017)
  - **LSE-C 및 LSE-D**: 
    - 오디오-비주얼 동기화 측정 (Prajwal et al., 2020)
- **음향 품질 평가**:
  - **화자 유사도 (SIM)**: 
    - WavLM-Base 모델을 사용하여 주어진 타겟 샘플과 생성된 음성 간의 유사도 계산 (Chen et al., 2022)
- **부록 참조**: 각 지표에 대한 자세한 설명 제공

---

# 5.2 Implementation Details

- **비디오 전처리**:
  - 비디오를 얼굴 영역(96×96)으로 잘라냄
  - 얼굴 감지기(Deng et al., 2020) 및 얼굴 랜드마크 감지기(Bulat and Tzimiropoulos, 2017) 사용
  - 오디오를 16kHz로 리샘플링

- **모델 훈련**:
  - 영어로 훈련된 AV-HuBERT(Shi et al., 2021) 사용
  - HuBERT 토크나이저(Hassid et al., 2023)와 함께 100k 스텝 동안 훈련
  - 총 500 클러스터, 25Hz 작동
  - 최대 토큰 길이: 2000
  - 6 A6000 GPU에서 훈련

- **모델 초기화 및 훈련 전략**:
  - OPT-1.3B(Zhang et al., 2022)로 초기화
  - 입력 임베딩 레이어 및 프로젝션 레이어를 AVSR 및 TTS 목표로 200K 스텝 동안 사전 훈련
  - 텍스트와 AV 음성 토큰 대화 혼합으로 5K 스텝 동안 모델 훈련
  - AV 음성 토큰 대화만으로 추가 3K 스텝 동안 미세 조정
  - 4 A6000 GPU에서 최대 토큰 길이: 700

- **오디오-비주얼 생성기 훈련**:
  - 실제 AV 음성 토큰을 사용하여 훈련
  - 토큰 기반 음성 디코더와 길이 예측기를 450K 스텝 동안 배치 크기 32로 공동 훈련
  - AV 토큰 기반 얼굴 디코더 훈련을 위해 재프로그래밍 전략(Choi et al., 2023) 사용
  - 두 개의 트랜스포머 인코더 레이어로 구성된 어댑터 레이어 훈련
  - AV 음성 토큰과 TFG 모델의 오디오 특징 간의 연결
  - 250K 스텝 동안 배치 크기 256으로 훈련

- **얼굴 비디오 향상**:
  - 고해상도 비디오 생성을 위해 얼굴 향상기(Wang et al., 2021b) 통합

---

# 5.3 Baselines
- 직접적인 오디오-비주얼 대화 합성을 수행하는 이전 방법이 없음.
- 최근에 제안된 대화 시스템인 Speech-GPT (Zhang et al., 2023a)와 d-GSLM (Nguyen et al., 2023b)과 비교.
  - 두 시스템 모두 입력 및 출력에서 오디오 음성만 지원.
- 다양한 사전 학습된 모델을 통합한 캐스케이드 시스템을 구축:
  - VSR (Anwar et al., 2023)
  - LM (Tang et al., 2022)
  - TTS (Casanova et al., 2022)
  - TFG (Prajwal et al., 2020)
- 캐스케이드 방법과의 비교 목적:
  - 최고 성능을 목표로 하지 않음.
  - 제안된 시스템의 성능을 직접 전략을 통해 얼마나 달성할 수 있는지 평가.
- 공정한 비교를 위해 Speech-GPT 및 d-GSLM을 MultiDialog 데이터셋에서 파인튜닝.
- 캐스케이드 시스템의 LM으로 TopicalChat에서 훈련된 대화 언어 모델 사용 (Tang et al., 2022).

---

# 6 Results

- 실험은 다양한 조건에서 수행되었으며, 설정된 목표를 달성하였습니다.
- 데이터 분석을 통해 다음과 같은 주요 결과를 도출하였습니다:
  - 평균 성능 지표는 $$P_{avg} = \frac{1}{N} \sum_{i=1}^{N} P_i$$로 계산되었으며, 이때 $N$은 실험 반복 횟수입니다.
  - 조건별 성능 변화는 $$\Delta P = P_{final} - P_{initial}$$로 나타났습니다.
- 통계적 검증을 통해 결과의 신뢰성을 확보하였습니다.
- 결과는 기존 연구와 비교하였으며, 통계적으로 유의미한 차이를 보였습니다.
- 추가적으로, 결과의 시각화를 위한 다양한 그래프를 작성하여 효과적으로 전달하였습니다.

---

# 6.1 Semantic Evaluation

- 생성된 응답의 의미적 품질을 정확하게 평가하기 위해 텍스트 기반 대화 모델의 평가 전략을 사용.
- MultiDialog의 테스트 세트에서 모델이 대화의 각 턴에 대해 응답을 순차적으로 생성하도록 프롬프트.
- 생성된 응답을 텍스트로 전사하고, 이를 기준 응답과 비교하여 의미적 품질 평가.
- 결과는 다음과 같음:
  - 제안된 방법이 최신 구어 대화 시스템인 SpeechGPT 및 d-GSLM과 비교하여 BLEU, D-1, D-2 지표에서 가장 우수한 성능을 보임.
  - 이는 제안된 방법이 맥락적으로 일관되고 다양한 응답을 생성할 수 있음을 나타냄.
  
- **SpeechGPT 특성:**
  - 가장 높은 PPL 값을 기록, 이는 대량의 음성 데이터로 훈련되고 PEFT로 다듬어졌기 때문.
  - 더 유창한 음성을 생성할 수 있으나, 낮은 BLEU 점수로 기준 응답에 맞지 않음.
  - 텍스트 응답을 생성하기 위해 입력의 텍스트 전사가 필요.

- **제안된 방법의 장점:**
  - 오디오와 비주얼 스피치 비디오에서 직접 응답을 인식하고 생성 할 수 있는 첫 번째 접근 방식.
  - 중간 텍스트 생성을 필요로 하지 않음.

- 실험 결과:
  - Ground Truth: 
    - PPL: 1054.643, BLEU: 76.326, D-1: 0.947, D-2: 0.996
  - 새로운 구술 대화 시스템과 비교: 
    - Cascaded System, Spoken Dialogue System, Audio-Visual Spoken Dialogue System 등의 결과 제공.

- 또한, 오디오 및 비주얼 생성 품질 평가:
  - 제안된 방법은 재구성된 오디오 및 비주얼 출력에 대한 높아진 SIM 값(0.624)으로 확인됨.

---

# 6.2 Ablation on the Pretraining Scheme

- 오디오-비주얼 대화 모델의 프리트레인 스킴 분석 
- 실험 결과:
  - 텍스트로 프리트레인된 LLM으로 모델 초기화 시 의미적 품질 향상
  - AVSR/TTS 프리트레인을 추가하면 더욱 개선됨
- 임베딩 레이어 및 프로젝션 레이어를 훈련하여 대응하는 AV 음성 토큰과 텍스트 토큰 예측 시 응답 개선
- 혼합 텍스트-AV 음성 토큰 프리트레인을 도입 시 전반적인 의미적 품질 향상 관찰
  - AV 음성 토큰을 LLM에 점진적으로 적응시키는 효과성 검증
- PPL 점수는 약간 감소하였으며, 이는 모델의 복잡성 증가와 멀티모달 입력에 대한 적응성을 반영할 수 있음

---

# 6.3 Audio and Visual Evaluation

<img width="861" alt="image" src="https://github.com/user-attachments/assets/94500091-df59-4a58-9c29-d8e620dc170e" />

- 오디오 및 비주얼 생성을 평가함.
- 발표자 음성 유사성(SIM) 측면에서 제안된 방법이 캐스케이드 시스템 및 구어 대화 시스템보다 우수함.
  - 이는 AV 토큰 기반의 음성 디코더가 효과적임을 보여줌.
  - 발표자 정보를 참조 비디오에서 잘 유지함.
- 비주얼 품질 평가 시, 동일한 TFG 모델(Prajwal et al., 2020)을 사용하는 캐스케이드 시스템과 비교.
  - FID 점수는 유사하지만, 오디오-비주얼 동기화에서 우수함.
  - 이점은 이산화된 오디오-비주얼 토큰의 사용으로 인해, 오디오와 비주얼 요소 간의 정렬이 더 명확하기 때문임.
- 그림 4에서는 두 파트너 간의 생성된 오디오-비주얼 응답과 ASR로 생성된 전사본을 보여줌(Shi et al., 2021).
  - 대화 맥락을 고려하여 다음 응답을 생성하며, 적절하고 일관성 있음.
  - 예시 (그림 4 (a))에서 사용자의 이전 질문에 대답하고 NFL에 대한 대화 주제에 맞는 응답을 줌.
  - 참조 얼굴의 발음 관련 움직임을 성공적으로 합성하여 매끄러운 말하는 얼굴 비디오를 생성함.
- 추가 데모는 참고를 위해 제공됨.

---

# 6.4 Robustness to Acoustic Noise

- 표 5에서는 대화 시스템에 추가적인 시각 모드를 통합하는 효과를 분석함.
- Shi 외(2021)의 방법에 따라 다양한 신호 대 잡음 비율(SNR) 수준(-5, 0, 5, 클린)에서 입력 음성을 무작위 잡음으로 오염시킴.
- 오디오만 사용하는 입력에 비해, 오디오-비주얼 입력이 잡음 하에서 성능 저하가 적어 시스템의 강인성을 강화함.
- 이는 시각적 모달리티가 음향 잡음의 영향을 받지 않아 오디오 모달리티에서 놓친 정보를 보완할 수 있음을 의미함.
- 이로 인해 음성 내용을 더 잘 인식하고 응답을 출력할 수 있음.
- 결과적으로, 우리의 시스템이 불안정한 음성 입력 상황에서도 현실적인 사용이 가능함을 보여줌.

---

# 7 Conclusion and Limitation

- 새로운 대면 음성 대화 모델을 소개함
  - 사용자 입력의 오디오-비주얼 음성을 직접 처리하고 오디오-비주얼 음성 응답 생성
  - 텍스트 없이 대화형 얼굴 아바타 챗봇 시스템 구축을 위한 첫 단계

- MultiDialog 데이터셋 출시
  - 가장 큰 다중 모달 대화 데이터셋 
  - 세 가지 모달리티(오디오, 비주얼, 텍스트)의 대화 데이터 포함
  - 실제 사람 간의 대화를 포괄적으로 담아 폭넓은 연구 기회 제공

- 연구 기회
  - 대화형 얼굴 합성, 다중 모달 대화 언어 모델링 등 다양한 분야에서 활용 가능

- 한계점
  - 데이터셋은 감정 레이블 포함하나, 현재 사용하지 않음
  - 향후 연구에서 사용자 얼굴 표정의 감정 인식을 통합할 계획
    - 더 감정 인식 있는 응답을 생성하기 위해
  - 발표자와 청중의 병렬 녹음을 통해 두 얼굴의 생성 모델링 가능
    - 더 자연스럽고 즉흥적인 대화 촉진 가능
