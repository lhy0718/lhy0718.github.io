---
title: "[논문리뷰] Playing repeated games with Large Language Models (Nature 2025)"
date: 2025-05-26 10:01:05 +0900
categories:
  - Paper Review
tags:
  - Nature 2025
  - LLM in Game Theory
---

- GPT-4, text-davinci-002, text-davinci-003, Claude 2, 그리고 Llama 2 70B 모델을 사용하여 다양한 2 × 2 게임을 평가함.
- 두 가지 특정 게임에 대해, 모든 LLM과 인간과 유사한 전략들이 서로 대결하도록 설정함.
- 연구는 특히 협력(cooperation)과 조정(coordination) 게임에서 LLM들의 행동 양상에 중점을 둠.

---

# Analysing behaviour across families of games

- 세 가지 LLM(대규모 언어 모델)들이 서로 다른 게임 군(families)에서 게임을 하도록 실험을 시작함.
- 실험에 사용한 게임 군은 다음과 같음:
  - Win–win (상호 이익)
  - Biased (편향된)
  - Second-best (차선책)
  - Cyclic (순환)
  - Unfair (불공평)
  - Prisoner’s Dilemma 가족군
- 각 게임 유형별 예시 보상 행렬은 그림 2에 제시됨.
- 모든 LLM들이 자기 자신을 포함해 서로를 상대로 각 게임을 10라운드 반복 플레이하며, 플레이어 1과 플레이어 2 역할을 모두 수행함.
- 총 1,224개 게임이 진행됨:
  - Win–win: 324개
  - Prisoner’s Dilemma: 63개
  - Unfair: 171개
  - Cyclic: 162개
  - Biased: 396개
  - Second-best: 108개
- 게임 군의 특징:
  - Win–win: 두 플레이어 모두 상호 이익을 얻는 결과
  - Prisoner’s Dilemma: 개인 이익과 집단 이익 간의 갈등
  - Unfair: 한쪽 플레이어에게 편향된 결과
  - Cyclic: 선호가 순환하는 결과
  - Biased: 한쪽에 내재적 우위가 존재함

- 각 게임에서 LLM 성능 측정 방법:
  - 실제 얻은 점수를 이상적 조건(플레이어가 매 라운드 최대 이익을 올릴 때) 하의 최대 점수로 나누어 계산
- 성능 결과(테이블 1 참고):
  - 전반적으로 모든 모델이 준수한 성능을 보임.
  - 모델 크기가 클수록(파라미터 수) 성능이 좋음.
  - GPT-4가 가장 뛰어난 성능을 보여 Claude 2, davinci-003, davinci-002, Llama 2 등을 통계적으로 유의미하게 앞섬.  
    - 예: GPT-4와 Claude 2 비교에서 t(287) = 3.34, $$P < 0.001$$, Cohen's $$d = 0.20$$, Bayes factor = 14.8
- LLM별 강점과 약점:
  - LLM들은 Win–win 게임에서 가장 높은 성능을 보임 → 최적 선택이 명확하기 때문.
  - Prisoner’s Dilemma 게임에서도 인간에게 도전적인 게임임에도 준수한 성능을 냄.
  - 최적 선택이 자신의 선호와 일치하지 않는 상황(예: 편향 혹은 비순응적 게임)에서는 모든 LLM의 성능이 낮아짐.
- 인간은 보통 이런 어려운 게임을 '관습 형성(convention formation)'을 통해 해결하는데,
  이에 관한 대표적 게임인 'Battle of the Sexes'를 본문에 이어 자세히 분석할 예정임.

---

# Cooperation and coordination games

- 본 섹션에서는 앞서 LLM들이 상대적으로 잘하거나 못했던 극단 사례들을 분석하기 위해, 대표적인 죄수의 딜레마와 Battle of the Sexes 게임에서의 LLM 행태를 자세히 살펴봄.

- **죄수의 딜레마 (Prisoner’s Dilemma)**  
  - 경쟁과 배신 요소가 포함된 게임에서 LLM들이 비교적 좋은 성과를 냄.  
  - 두 플레이어는 협력하거나 배신(defect)할 수 있음.  
  - 여러 번 반복되는 상황에서는 배신 후 보복(retaliation) 전략을 평가하기 적합한 환경임.  
  - 고전적 죄수의 딜레마의 페이오프 행렬 (pay-off matrix):

    $$
    \begin{array}{c|cc}
      & \text{협력} & \text{배신} \\
      \hline
      \text{협력} & (8,8) & (0,10) \\
      \text{배신} & (10,0) & (5,5)
    \end{array}
    $$

  - 조건: 상호 협력이 상호 배신보다 이득이 크지만, 배신이 모든 플레이어에겐 우월 전략임.  
  - 합리적 에이전트는 단일 게임뿐 아니라 반복 횟수가 알려진 유한 반복 게임에서도 항상 배신을 선호함. 이는 상대가 배신하면 큰 손실을 보기 때문임.  
  - 무한 반복 혹은 반복 횟수가 알려지지 않은 경우에는 반협력적(dynamically semi-cooperative) 전략이 이론상 이익을 가져올 수 있음.

- 실험환경 및 결과  
  - GPT-4, text-davinci-003, text-davinci-002, Claude 2, Llama 2 등 LLM들이 서로 경쟁함.  
  - 세 가지 단순 전략 추가:
    - 무조건 협력하는 플레이어
    - 무조건 배신하는 플레이어
    - 첫 라운드에만 배신하고 이후에는 모두 협력하는 플레이어 (신뢰 형성 가능성 평가 목적)  
  - 결과:  
    - GPT-4가 다른 모든 에이전트에 비해 더 좋은 성과를 냄 (통계적 유의성: $$t(153.4) = 3.91, P < 0.001, d=0.33$$, 95% 신뢰구간 $$0.10 - 0.55$$, 베이지안 요소 7.1).  
    - 매우 중요한 점: GPT-4는 상대가 첫 라운드에 배신하고 이후 협력하는 전략에도 절대로 다시 협력하지 않고 배신을 지속함. 즉, GPT-4는 매우 용서하지 않는(unforgiving) 성향을 보임.  
    - GPT-4의 강점은 대부분 상대가 한 번이라도 배신하면 협력하지 않고 배신을 선택하는 데 있음.

- **강건성(robustness) 검증**  
  - 주장된 용서하지 않는 특성이 특정 프롬프트(prompt)에 기인한 것인지 확인하기 위해 다양한 조건에서 실험 수행:  
    - 선택지 순서 랜덤화  
    - 선택지 라벨 변경  
    - 보상 단위를 점수, 달러, 동전으로 다양하게 표현  
    - 서로 다른 주제 커버 이야기 사용  
    - 명시적인 목표 추가  
    - 더 긴 게임 반복 횟수 적용  
    - 수치 결과를 텍스트로 표현 (Supplementary Fig. 3 참고)  
  - 모든 경우에 GPT-4는 여전히 용서하지 않는 태도를 보임. 즉, 프롬프트 특징 때문이 아님.

- **추가 실험**  
  - GPT-4에게 상대가 한 번 배신 후 다시 협력한다고 명시적으로 알려줌.  
  - 결과: GPT-4는 모든 라운드에서 계속 배신을 선택, 자신의 점수를 극대화함.

- 결론:  
  - GPT-4는 죄수의 딜레마 게임에서 상대가 신뢰 회복을 시도해도 용서하지 않고 배신 전략을 고수함.  
  - 이는 GPT-4가 이 게임에서 보복적이며 자기 이익 극대화에 엄격하다는 것을 보여줌.

---

# Prompting techniques to improve observed behaviour

- 죄수의 딜레마(Prisoner’s Dilemma)에서 결함(defect) 선택은 반복 게임에서도 최적, 효용 극대화, 균형 전략으로 보일 수 있음.
- 그러나 GPT-4가 용서를 시작하고 협력하는 시나리오를 탐색하기 위해 추가 시뮬레이션을 수행함.
  - 인간의 죄수의 딜레마 용서 연구(ref. 11)에 착안하여, 상대방이 실수를 할 수도 있다는 정보를 제공하면 다시 협력할 가능성이 높아짐.
  - GPT-4도 유사하게 협력 행동을 재개함을 관찰함.

- **성별전쟁 게임(Battle of the Sexes)**
  - 협력을 필요로 하는 조정 게임의 일종이며, 완전한 이해상충은 없으나 보상은 약간 다름.
  - 한 쌍이 함께 시간을 보내고자 하지만 선호 활동이 다름 (예: 아내는 축구, 남편은 발레).
  - 같은 활동을 같이 해야 효용이 발생하며, 다른 활동을 같이 할 경우 어느 한 쪽은 더 높은 효용을 얻음.
  
  - 해당 게임의 보상 매트릭스:
    $$
    \begin{pmatrix}
    (10,7) & (0,0) \\
    (0,0) & (7,10)
    \end{pmatrix}
    $$
  
- 분석 대상은 GPT 시리즈, Claude 2, Llama 2 및 세 가지 단순 전략으로 구성됨.
  - 단순 전략 중 일부는 항상 한 가지 옵션만 선택.
  - 인간 플레이어는 주로 교대로(turn-taking) 선호 옵션을 번갈아 선택하는 패턴을 보임. 이는 완벽한 균형은 아니지만 효율적인 조정 및 높은 공동복지 실현에 도움됨.
  
- 결과
  - GPT-4가 전체적으로 가장 좋은 성과를 보임 (통계: t(128.28) = 2.83, p=0.005, d=0.28, 95% CI [0.07–0.50], BF=3.56).
  - 그러나 GPT-4는 자신의 선호 옵션을 지속 선택하여 교대 패턴을 따르는 단순 에이전트와는 협력이 실패함.
  - GPT-4는 타 플레이어에 맞춰 선택을 조정하지 않고 고집하는 행동 결함을 보임.

- 강건성 검증
  - 옵션 순서, 명칭, 효용 단위를 변경해도(GPT-4가 요리 대회, 협업 프로젝트 상황에 있다고 가정한 다른 프롬프트 등) 교대 실패 현상 계속됨.
  - 보상 매트릭스(예: 축구와 발레 선호도 점진적 변경)도 변경했으나 동일한 행동 관찰됨.

- 예측 시나리오
  - GPT-4에게 게임 참여자가 아닌 관찰자 역할로 이전 라운드 기반 상대방의 행동 예측을 시킴.
  - GPT-4는 5라운드부터 교대 패턴을 올바르게 예측하고, 외부 관찰자로 역할만 바꾸면 3라운드부터 정확한 예측 가능.
  - 즉, 교대 패턴을 인지하나 행동 선택에서는 반영하지 않음. 이는 자폐 아동에서 관찰되는 사회적 비사회적 표현 격차와 유사한 현상.

- SCoT (Sequential Chain of Thought) 프롬프트 기법 도입
  - GPT-4가 상대방 행동을 예측하고, 그 결과를 바탕으로 자신의 행동을 합리적으로 결정하는 사고의 사슬(chain-of-thought) 유도 기법.
  - 인간의 반복 게임 및 타인의 신념 추론 능력 향상과 유사하며, 자폐 아동 사회적 추론 훈련, 일반 의사결정 개선에 영감을 받음.
  - SCoT 적용으로 GPT-4가 5라운드 이후 교대 행동을 시작하며 성능이 개선됨.

요약하자면, GPT-4는 단순 행동 패턴으로 인해 협력적 교대 행동을 실패하지만, 상대방 행동 예측 능력은 갖추고 있으며, 적절한 사고 유도 프롬프트(SCoT)를 통해 사회적 협력 능력을 강화할 수 있음.

---

# Human experiments

- GPT-4의 다양한 게임 내 행동 특징을 관찰한 후, 실제 인간 참가자들이 이러한 에이전트와 플레이할 때 어떻게 행동하는지에 대해 실험을 진행함.
- 실험에는 195명의 참가자가 참여했으며, 이들은 Battle of the Sexes 와 Prisoner’s Dilemma 게임에서 LLM과 대결함.
- 행동 실험에선 GPT-4의 행동을 안정적으로 변화시키는 SCoT(Socially Conditioned Thought) 프롬프트 방식을 사용함.
- 참가자들은 각 게임에서 10번 반복 플레이하며, 각 게임 후 상대가 인간인지 인공지능인지를 맞춰야 했음.
- 참가자에게는 플레이 순서가 무작위로 지정되었으며, 모두 실제로는 LLM과 플레이했음.
- 참가자는 두 그룹으로 나뉘었으며,
  - 한 그룹은 기본 GPT-4 모델과 플레이,
  - 다른 그룹은 상대방 행위를 예측하고 그에 따라 행동하는 SCoT 프롬프트된 GPT-4와 플레이.
- 각 참가자는 두 게임을 모두 같은 GPT-4 버전과 플레이했고, 게임 간 프롬프트는 초기화 됨.
- 실험 설계 개요는 Fig. 7a에 제시되어 있음.
- 참가자는 Prolific에서 모집되었고, 실험 종료 후 충분히 설명됨.
- 연구 관심사는 인간이 LLM과 어떻게 플레이하는지, 그리고 SCoT 프롬프트가 행동 개선에 도움이 되는지 여부였음.

- 주요 결과:
  - Battle of the Sexes 게임에서 SCoT 프롬프트 조건이 기본 조건보다 참가자 평균 점수가 유의하게 높았음  
    $$
    \beta = 0.74, \quad t(193) = 3.49, \quad P < 0.001, \quad 95\% \, CI = [0.32, 1.15], \quad BF=80.6
    $$
  - Prisoner’s Dilemma 게임에서는 점수 차이가 유의미하지 않았음  
    $$
    \beta = 0.10, \quad t(193) = 0.47, \quad P = 0.64, \quad 95\% \, CI = [-0.31, 0.51], \quad BF=0.2
    $$
  - Battle of the Sexes에서 SCoT 프롬프트는 성공적인 협조(두 플레이어가 같은 선택)를 증가시킴  
    $$
    \beta = 0.33, \quad z = 3.59, \quad P < 0.001, \quad 95\% \, CI = [0.15, 0.51], \quad BF=13.4
    $$
  - Prisoner’s Dilemma에서는 공동 협력(두 플레이어 모두 협력)이 다소 증가함  
    $$
    \beta = 0.24, \quad z = 2.54, \quad P = 0.01, \quad 95\% \, CI = [0.05, 0.42], \quad BF=6.5
    $$
  - 참가자들은 SCoT 프롬프트된 모델이 기본 GPT-4보다 인간 플레이어로 인식될 가능성이 높았음  
    $$
    \beta = 0.54, \quad z = 8.31, \quad P < 0.001, \quad 95\% \, CI = [0.05, 0.42], \quad BF=17.6
    $$

- 추가 분석 결과는 부록(Supplementary Information)에 포함되어 있음.
- 요약: SCoT 프롬프트는 Prisoner’s Dilemma와 같이 이기적 행동이 중요한 경우에는 점수를 변화시키지 않으면서도 협력 및 조정 행동을 증가시킴. 특히 조정 문제인 Battle of the Sexes에서는 성과를 향상시킴.

---

# Discussion

- 대형 언어 모델(LLM)은 매우 빠르게 채택된 기술로, 수주 내에 수백만 명의 사용자와 상호작용함. 이에 따라 이러한 시스템이 사람 및 서로 어떻게 상호작용하는지 원리적으로 이해하는 것이 시급한 과제임.
- 본 연구는 행동 게임 이론에서 인간 상호작용을 이해하기 위해 통제된 이론적 게임을 사용하는 것과 같이, LLM 상호작용 연구에 이와 같은 게임 이론적 접근을 제안함.
- 2×2 게임 전반에 걸친 대규모 분석 결과 최신 LLM들이 개별 보상 측면에서 좋은 성과를 내며 특히 명시적 협력 없이도 잘 수행함을 보여줌. 이는 LLM의 비약적 현상(emergent phenomena)을 보여주는 기존 문헌에 더해진 중요한 시사점임.

- 그러나 LLM들은 단순한 전략을 상대할 때에도 좌절게임(coordination games)에서는 비최적 행동을 보임.
- 대표적 게임인 죄수의 딜레마에서 GPT-4는 단호한 태도를 보이며 한 번이라도 부정적 상호작용이 발생하면 영구적으로 협력에서 배신으로 전환함. 이는 유한 반복 게임에서 평형 전략이지만, 두 에이전트의 공동 이익을 저해함.
- 성대결(Battle of the Sexes) 게임에서는 자신의 선호 안을 고집하는 비협력적 경향을 보이며, 죄수의 딜레마와 달리 개인 차원에서조차 비최적임.

- 현재 LLM들은 주로 인간에 대한 선의의 조수로 설계, 훈련됨에도 불구하고, 반복 게임에서 자기 이익만을 추구하는 자기중심적이고 비협조적 행동을 보여 LLM을 진정으로 사회적이고 정렬된 기계로 만들기에는 아직 발전이 필요함을 시사함.
- 이들의 좌절게임 관련 부적절한 반응은 LLM의 마음 이론(ToM) 관련 최근 논쟁에서 나타난 잠재적 실패 양상을 보여줌.
- 여러 실험을 통해 이러한 행동적 특성이 특정 프롬프트에 국한된 것이 아니며 LLM 전반의 패턴임을 확인함.

- 협력 상대의 실수 가능성을 LLM에 명시적으로 알려주는 개입은 협력을 증가시켜 LLM 사회적 행동의 가변성을 보여줌.
- GPT-4는 마지막 라운드에서 행동을 조정하지 않고, 미래 상호작용 가능성이 높아질 때 협력도를 높이는 인간과 달리 후방 추론(backward induction)과 장기 전략 계획 메커니즘이 부족함을 시사함.
- 결과적으로 GPT-4는 불확실한 상황에서 배신으로 기본 전환하는 경향이 있음.

- GPT-4에게 상대방의 선택을 먼저 예측하도록 하는 프롬프트는 단순 전략을 간과하는 결함을 줄이며, 이는 ToM 참여를 강제하는 명시적 사회 인지 유도 방식임.
- 이는 비사회적 추론 향상을 위한 chain-of-thought 프롬프트와 유사하게, 인간-LLM 상호작용 개선을 위한 사회 인지 프롬프트 적용 가능성을 제시함.

- 연구 한계:
  - 단순 2×2 게임에 한정되었으며, 보다 복잡한 게임 (예: 신뢰 게임, 다중 에이전트 게임 등) 연구가 필요함.
  - 본 연구는 주로 유한 반복 게임을 다루었으며, 추후 불명확하거나 무한 반복 게임에 대한 조사 필요.
  - 인간과 LLM 상호작용을 Battle of the Sexes, 죄수의 딜레마 두 게임에 한해 진행해 추가 게임에서의 인간 행동 동역학 연구가 요구됨.
  - LLM의 자기 보고 전략과 실제 행동 간 상관관계 분석도 추후 연구 과제임.

- 본 연구는 기계 행동 과학(Behavioural Science for Machines)의 중요성을 제기하며, 앞으로 복잡하고 다중모달, 물리적 환경에 내재된 LLM 인지 분석에 해당 방법론이 계속 활용될 것으로 기대함.

- 주요 수식 예시:
  - 게임에서의 선택 가능한 전략이 $$J \text{ 또는 } F$$ 임.
  - 불확실 상황에서 LLM이 협력할 확률 $$P(\text{협력})$$ 가 낮음.
  - 실험 결과에서 효과 크기 베이지안 팩터는 예: $$BF=80.6$$ 등으로 표기됨.

---

# Related work

- 알고리즘이 점점 더 정교해지고 그 의사결정 과정이 불투명해짐에 따라, 행동과학은 행동 관찰만으로 추론을 가능하게 하는 새로운 도구를 제공함[49,50].
- 행동 과제를 이용한 벤치마크가 여러 차례 활용되어 옴[10,53].
- 알고리즘이 다른 주체(사람, 기계 등)에 대해 추론하는 능력에 대한 연구는 행동과학에서 많은 영향을 받음[54–56].
- 특히 대부분의 대형 언어 모델(LLM)이 포함된 사회적 상호작용에서는 다른 주체의 신념, 욕망, 의도를 추론하는 능력, 즉 마음 이론(Theory of Mind, ToM)에 대한 관심이 높음[57].
- ToM은 선의의 가르침[58]부터 악의적 기만[56,59]에 이르기까지 다양한 상호작용 현상의 기반이며, 인간 사회적 상호작용의 핵심 요소로 여겨짐[60,61].
- LLM이 ToM을 갖추었는지에 대한 논쟁이 존재함.
  - 예: GPT-3.5가 여러 전형적인 ToM 과제에서 좋은 성과를 보임[39].
  - 반대 주장: 이는 특정 프롬프트에 의존한 결과일 뿐이라는 견해[37,38].
  - 사슬 추론(chain-of-thought)이 LLM의 ToM 능력을 향상시킨다는 연구도 있음[34].
- 현재 가장 큰 LLM인 GPT-4는, 이전에 GPT-3.5가 어려워하던 ToM 과제 변형에서도 좋은 성과를 보여 연구에 특히 관심 대상임[8].
- 게임 이론에서 가져온 게임들은 통제된 환경에서 상호작용 행태를 조사하기 위한 이상적인 테스트베드이며[62], LLM의 행동은 이런 과제들에서 점검되어 왔음[63].
  - 예: GPT-3이 독재 게임(dictator game)에 참여해 행태 변화가 관찰됨[40].
  - 동일한 접근법이 최후통첩 게임(ultimatum game)에도 적용됨[41].
  - 두 연구 모두 프롬프트에 따라 자기 이익적 행동이 조절되는 등 행태가 유동적임을 보여줌.
  - 그러나 이러한 연구는 모두 단회성(single-shot) 상호작용과 비교적 적은 횟수의 게임에 국한되고 반복 게임(iterated games)은 사용하지 않음.
- 본 연구는 최근 LLM 평가가 성능 평가에서 인간 행동과 비교하는 것으로 전환된 최근 동향을 기반으로 함.
- LLM 분석을 위해 인지심리학 도구[51,64]와 계산 정신의학적 관점[52]를 사용하는 시도가 있었음.
- 상호작용 주체 이론은 머신러닝 여러 응용 분야에 중요하며[65], 특히 한 주체가 다른 주체를 속이는 적대적 환경(adversarial setting)[66]에서 중요함.
- 다중 에이전트 시스템에서 친사회적 역학 이해[67]와 협력 촉진[68]은 복잡한 사회적 환경에서도 견고하고 신뢰할 수 있는 AI 시스템 개발에 필수적임[69].

---

# Methods

- 본 연구는 인간 참가자들이 LLM 에이전트와 게임을 할 때 어떻게 행동하는지 조사하기 위해, 두 가지 게임인 죄수의 딜레마(Prisoner’s Dilemma)와 성별 간 대결(Battle of the Sexes)에서 이들의 상호작용을 연구함.
- 참가자들이 서로 다른 에이전트를 인지하고 이에 따라 행동을 달리하는지 여부도 함께 분석함.

- **참가자 모집 및 인구통계**
  - 총 195명 참가자 (여성 89명, 평균 연령 26.72세, 표준편차 4.19)
  - Prolific70 온라인 플랫폼을 통해 모집, 다양한 배경의 신뢰할 수 있는 참가자 풀 확보
  - 통계적 방법으로 표본 크기를 미리 결정하지 않았으나, 기존 연구들[71–73]과 유사한 크기임
  - 참가자 조건:
    - 유창한 영어 구사
    - 승인률 최소 0.95 이상 및 1 이상
    - 이전 제출 횟수가 10회 이상이며 본 실험에 미참여자

- **윤리 및 보상**
  - 모든 참가자들은 연구 시작 전 사전 동의(informed consent) 제공
  - 튀빙겐 대학교 윤리 위원회에서 승인을 받음 (프로토콜 번호: 701/2020BO)
  - 보상: 기본 £3와 게임 점수에 따른 최대 £2의 보너스 (게임 내 획득 점수당 1센트)
  - 평균 시급은 £11.41
  - 실험 종료 후, 참가자들에게 실험 내용 완전 설명(디브리핑) 실시

- **데이터 처리**
  - 각 라운드에서 제한 시간 20초 내에 선택하지 않은 21명의 데이터는 제외함

- 이후 섹션에서 실험 환경 및 절차가 더 상세히 설명됨.

---

# LLM–LLM interactions

- 본 연구에서는 경제학 문헌에서 가져온 유한 반복 게임(finitely repeated games)을 대상으로 LLM의 행동을 분석함.
- 분석의 단순화를 위해 두 플레이어가 두 가지 선택지 중 하나를 선택하는 이산(discrete) 선택 게임에 집중.
- 두 LLM은 프롬프트 체이닝(prompt chaining)을 통해 상호작용하며, 모든 증거 통합과 과거 상호작용에 대한 학습은 컨텍스트 내 학습(in-context learning) 방식으로 이루어짐.
- 게임은 각 LLM에 프롬프로 입력되며, 이때 게임과 선택지 정보가 포함됨. 동일한 게임 프롬프트를 다른 LLM에도 제공함.
- 두 LLM 모두 생성된 토큰 $$t$$는 다음 식을 통해 샘플링됨:
  
  $$
  p_{\text{LLM}}(t \vert c(p)) = \prod_{k=1}^K p_{\text{LLM}}(t_k \vert c(p)_1, \ldots, c(p)_n, t_1, \ldots, t_{k-1})
  $$

- 프롬프트 입력 후 첫 토큰 예측 확률은 $$d = p_{\text{LLM}}(t_1 \vert c(p))$$이고, 선택 가능한 N개의 답변 토큰은 $$o = \{ o_i \}_{i=1}^N$$ (본 연구에서는 F와 J).
- 예측된 선택지는 확률이 최대인 옵션으로 결정:
  
  $$
  \hat{o} = \arg\max (\hat{c}_i), \quad \hat{c}_i = d[c_i], \quad i=1 \ldots N
  $$

- 양쪽 LLM이 선택을 완료하면, 이전 상호작용의 기록을 텍스트로 연결(concatenate)하여 다음 라운드를 위한 프롬프트로 업데이트.
- 각각의 게임은 총 10 라운드로 진행됨.
- 한 라운드에서 플레이어 1에 대한 보상은 $$\pi_1(x_1, x_2)$$로, 여기서 $$x_1$$, $$x_2$$는 각 플레이어가 선택한 전략.
- 반복 게임에서 보상은 각 단계의 보상을 할인율 $$\delta$$로 가중한 합으로 표현됨.
- $$n$$회 반복 시 플레이어 i의 누적 보상은 다음과 같음:

  $$
  U_i = \pi_i(x_1^0, x_2^0) + \delta \times \pi_i(x_1^1, x_2^1) + \delta^2 \times \pi_i(x_1^2, x_2^2) + \ldots + \delta^{n-1} \times \pi_i(x_1^{n-1}, x_2^{n-1})
  $$

- 본 실험에서는 할인 인자를 $$\delta=1$$로 설정함.
- 게임 시나리오의 특정 프레이밍 편향을 방지하기 위해 페이오프 행렬(pay-off matrix)에 대한 최소한의 설명만 제공.
- 선택지 이름과 프레이밍에 의한 오염 방지를 위해 F와 J라는 중립적인 옵션명을 사용함.

---

# Games considered

- 2×2 게임 144가지 조사, 각 플레이어는 두 가지 선택지 보유, 보상은 두 플레이어의 공동 결정에 의존
- 6가지 주요 게임 패밀리로 분류:
  - **윈-윈 (win–win)**: 비제로섬 게임의 특수한 경우, 양 플레이어가 최적 선택 시 상호 이익을 제공, 협력을 촉진하여 모두에게 이득이 되는 결과 도출
  - **죄수의 딜레마 (Prisoner’s Dilemma) 패밀리**: 두 플레이어가 협력(cooperate)하면 평균 이익, 배신(defect)하면 개인 이익 극대화, 그러나 일반적으로 나쉬 균형이 파레토 우월 결과보다 열등한 경우 발생
  - **불공정 게임 (unfair)**: 한 플레이어가 올바른 전략을 사용하면 항상 승리 가능, 매우 불평등한 결과 초래
  - **순환 게임 (cyclic)**: 지배 전략과 균형이 없으며, 플레이어들이 안정된 결과 없이 선택 패턴을 순환
  - **편향된 게임 (biased)**: 서로 다른 플레이어가 동시에 같은 옵션을 선택할 때 더 높은 점수를 받지만, 선호하는 옵션이 다름. 대표적 예로 성별 전쟁(Battle of the Sexes)가 있으며, 협조해 같은 옵션 선택 필요
  - **차선책 게임 (second-best)**: 양 플레이어가 공동으로 두 번째로 좋은 효용의 옵션을 선택할 때 더 좋은 결과 도출
- 게임 내 보상 구조를 전략적으로 변경하면 게임의 특성과 동역학이 변할 수 있음
- 추가로 죄수의 딜레마와 성별 전쟁 2가지 게임에서는 LLMs가 간단한 수작업 전략과 대결하여 행동을 세밀히 분석함

---

# LLMs considered

- 본 연구에서는 다섯 가지 LLM을 평가함.
- 사용한 모델 및 API:
  - OpenAI의 GPT-4, text-davinci-003, text-davinci-002 (completions endpoint를 통해 사용)
  - Meta AI의 Llama 2 70B 채팅 모델 (70억 개의 파라미터, 대화용으로 최적화)
  - Anthropic API의 Claude 2
- 추가로 MosaicPretrainedTransformer (MPT), Falcon, 그리고 Llama 2의 여러 버전 (MPT-7B, MPT-30B, Falcon-7b, Falcon-40b, Llama 2 7B, Llama 2 13B) 모델 실험 결과:
  - 주어진 작업에서 95% 이상 동일한 첫 번째 옵션만 선택하는 현상이 나타나 성능이 좋지 않음.
  - 따라서 주요 실험에 포함하지 않음.
- 모든 모델의 설정:
  - 온도(temperature) 파라미터는 $$0$$으로 설정
  - 각 질문에 대해 에이전트가 선택할 옵션을 나타내는 한 개의 토큰만 응답하도록 요청
  - 기타 모든 파라미터는 기본값 유지

---

# Playing 6 families of 2 × 2 games task design

- 2 × 2 게임은 단순해 보일 수 있지만, 순수 경쟁부터 혼합 동기와 협력에 이르기까지 다양한 상호작용을 탐구하는 데 매우 강력한 방법임.
- 이러한 상호작용은 ref. 22에서 우아하게 정리된 정형적 하위 가족들로 추가 분류 가능.
- 본 연구에서는 가능한 광범위한 상호작용을 포괄하기 위해, GPT-4, text-davinci-003, text-davinci-002, Claude 2, Llama 2의 행동을 이 정형적 게임 가족들에 대해 조사함.
- 다섯 개의 AI 엔진 모두 여섯 가족 내 모든 게임 변형들을 플레이하게 하여 행동을 관찰함.

---

# Cooperation and coordination task design

- 본 연구에서는 두 가지 게임인 죄수의 딜레마(Prisoner’s Dilemma)와 성 역할 전쟁(Battle of the Sexes)을 상세히 분석함.
- 이 두 게임은 LLM들이 매우 잘 수행한 경우와 상대적으로 잘 수행하지 못한 경우의 흥미로운 경계 사례임.
- 특히 GPT-4의 행동에 집중하는데, 이는 다른 행위자의 의도와 목표에 대한 신념을 보유할 수 있는지에 관한 최근의 ToM(마음 이론, Theory of Mind) 논쟁 때문임.
- ToM은 반복적인 상호작용을 성공적으로 진행하는 데 중요한 능력임.
- 두 추가 게임에서는 LLM들이 간단하고 수작업으로 코딩된 전략과 대결하도록 하여 LLM들의 행동을 보다 깊이 이해하고자 함.
- 이 간단한 전략들은 LLM이 보다 인간과 유사한 플레이어와 게임할 때 어떻게 반응하는지 평가할 수 있도록 설계됨.

---

# Statistical tests

- 모든 보고된 검정들은 양측 검정(two-sided test)입니다.  
- 대립가설(HA) 하에서 데이터의 우도와 귀무가설(H0) 하에서 데이터의 우도를 정량화하는 베이즈 인자(Bayes factors)를 함께 보고합니다.  
- 기본 양측 베이지안 t-검정은 Jeffreys–Zellner–Siow 사전분포를 사용하며, 그 척도(scale)를 $$\sqrt{2}/2$$로 설정하여 계산합니다.  
- 모수적 검정(parametric tests)의 경우 데이터 분포는 정규분포로 가정하지만, 이 가정은 공식적으로 검증되지 않았습니다.  
- 효과 크기(effect sizes)는 Cohen의 d 또는 표준화 회귀 계수(standardized regression estimates)로 보고하며, 95% 신뢰구간(95% CI)도 함께 제시합니다.

---

# Human–LLM interactions

- 인간 참가자 연구의 설계 및 진행에 관한 추가 세부사항 제공
- 보상, 인구통계, 프롬팅(prompting), 그리고 커버 스토리(cover stories)를 포함
- 연구 참여자에게 적절한 보상을 제공하여 참여 동기 부여
- 다양한 인구통계학적 특성을 반영하여 연구의 대표성 확보
- 프롬팅 기법을 통해 인간과 LLM 간의 상호작용 유도 및 조절
- 커버 스토리를 활용하여 연구 목적이 직접적으로 노출되지 않도록 설계

---

# Design

- 실험은 HTML, JavaScript, CSS 및 맞춤 코드를 결합하여 참가자에게 제시됨.
- 실제 게임플레이의 스크린샷을 포함한 설명이 제공된 후, 참가자는 이해도를 확인하는 설문지를 완료해야 함.
- 모든 질문에 올바르게 답한 경우에만 본 실험을 진행할 수 있음.
- 참가자는 Prisoner’s Dilemma와 Battle of the Sexes 두 게임을 플레이하며, 게임 순서는 피험자 간에 균형을 맞춤(순서 반전).
- 각 게임은 서로 다른 플레이어와 함께 10라운드씩 진행됨.
- 참가자 인터페이스(Supplementary Fig. 4)는 현재 게임에 대한 명확하고 실행 가능한 정보를 제공하도록 설계됨.
- 각 게임 종료 후, 참가자는 상대가 인간인지 인공지능 에이전트인지 판단하도록 요청받음.

---

# Prompts and human instructions

- LLM과 인간 참가자 간 상호작용에 사용된 커버 스토리는 내용적으로 동일함.
- 게임 규칙과 이전 상호작용의 역사 등을 포함하여 조건 간 일관된 프레이밍을 보장함 (자세한 프롬프트 진행 과정은 Supplementary A.1 참고).
- 다만, 각 대상에 맞게 표현 방식이 조정됨.
- 인간 참가자를 위해서는 시각적 신호와 간결한 텍스트를 우선시하여 보다 몰입감 있는 경험을 제공함 (Supplementary Fig. 4 참고).

---

# Ending and debriefing

- 참가자들은 상대가 다른 인간 참가자일 수도 있고 인공지능 에이전트일 수도 있다고 안내받음.
- 실제로 모든 참가자는 실험 전반(두 게임 모두) 동안 SCoT 프롬프트 버전 또는 프롬프트하지 않은 GPT-4 버전과 짝을 이루어 진행.
- 실험 완료 후, 참가자들은 연구 목적이 대형 언어 모델(LLM)을 인간과 더 유사하게 만드는 방법을 탐구하는 것임을 설명받음.
- 두 게임 모두 참가자들은 서로 다른 버전의 인공지능 에이전트와 경쟁했음을 밝혀줌.

- 추가 정보:
  - 연구 설계에 관한 자세한 내용은 Nature Portfolio Reporting Summary에서 확인 가능.
  
- 데이터 및 코드 공개:
  - 모든 참가자 및 모델 시뮬레이션 데이터는 GitHub에서 이용 가능:  
    https://github.com/eliaka/repeatedgames  
  - 연구에 사용된 코드, 다양한 프롬프트 버전 및 모델 시뮬레이션도 GitHub에서 제공:  
    https://github.com/eliaka/repeatedgames

---
