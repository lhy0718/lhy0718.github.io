---
title: "[논문리뷰] Aligning Medical Images with General Knowledge from Large Language Models (MICCAI 2024)"
date: 2025-03-23 00:00:00 +0900
categories:
  - Paper Review
tags:
  - MICCAI 2024
---

요약: ViP는 시각 증상 생성기와 이중 프롬프트 네트워크를 활용하여 대형 비전-언어 모델인 CLIP로부터 일반 지식을 전이하여 의료 영상 분석을 개선하는 새로운 프롬프트 학습 프레임워크를 제안하며, 실험 결과 두 개의 어려운 데이터셋에서 최첨단 방법을 능가함을 보여줍니다.

---

# 1 Introduction

- 의료 영상 분석은 비침습적 진단 및 치료를 가능하게 하여 헬스케어에서 중요한 역할을 한다.
- 딥러닝 기술의 발전으로, 컴퓨터 보조 의료 영상 분석은 여러 시나리오에서 큰 성공을 거두었다.
- 현재 방법들은 대개 감독 학습 패러다임을 채택하며, 이는 모델 훈련을 위한 많은 양의 라벨링된 데이터를 필요로 한다.
- 이러한 패러다임은 의료 영상의 수작업 주석에 의존하며, 이는 시간 소모적이고 노동 집약적이다.
- 대규모 비전 언어 모델(VLMs)의 등장은 한정된 데이터로 작업 특정 의료 영상 분석 모델에 지식을 전이할 수 있게 한다.
- CLIP은 4억 개의 이미지-텍스트 쌍에 대해 대조 학습으로 사전 훈련된 대표적인 예이다.
- CLIP은 비전 및 텍스트 인코더로 구성되고, 이미지와 해당 텍스트 부분을 시각 및 텍스트 임베딩으로 인코딩한다.
- 그러나 CLIP은 주로 웹에서 스크랩된 데이터를 기반으로 사전 훈련되며, 이는 자연 이미지-텍스트 쌍을 주로 포함하고 의료 데이터는 제외되어 의료 분야에 직접 응용하는 데 어려움이 있다.
- 이러한 문제를 해결하기 위해, 우리는 추상적 의료 용어를 자연 및 의료 도메인에서 공유되는 시각적 증상으로 번역함으로써 VLMs가 해석 가능하도록 돕는 것을 제안한다.
- 본 연구에서는 CLIP의 일반 지식 전이를 촉진하는 새로운 시각 증상 가이드 프롬프트 학습 프레임워크인 ViP를 제안한다.
- ViP는 시각 증상 생성기(VSG)와 듀얼 프롬프트 네트워크로 구성된다.
- VSG는 사전 훈련된 큰 언어 모델(LLMs)을 통해 시각적 증상을 생성하고, 이는 듀얼 프롬프트 네트워크의 텍스트 입력으로 사용된다.
- 듀얼 프롬프트 네트워크는 두 개의 학습 가능한 프롬프트 모듈: 문맥 프롬프트(CoP)와 병합 프롬프트(MeP)를 훈련시킴으로써 CLIP의 일반화 능력을 향상시킨다.
- 이 프레임워크는 두 개의 공개 데이터셋, Pneumonia와 Derm7pt에서 평가되었다.
- 실험 결과, ViP가 최첨단 방법들을 능가하며, 프레임워크 각 구성 요소의 효능을 강조한다.
- 주요 기여:
  1. 프롬프트 엔지니어링에 대한 LLMs의 중요한 영향을 드러내고, 해석 가능성과 성능 향상에 대한 영향을 강조한다.
  2. ViP는 LLMs를 활용하여 시각 증상을 확장 가능한 방식으로 생성하고, CLIP의 의료 도메인 지식 전이를 지원하는 두 개의 학습 가능한 프롬프트 모듈을 제안한다.
  3. 두 데이터셋에 대한 광범위한 실험을 실시하였고, 결과는 의료 영상 분석에 대한 ViP의 강력한 일반화 능력을 보여준다.

---

# 2.1 Overall Pipeline

- 입력 이미지 $$x$$와 질병 레이블 집합 $$C = \{c1, c2, ..., cn\}$$을 고려하며, 질병 카테고리의 총 수 $$N$$은 $$n$$입니다.
- 프로세스는 이중 프롬프트 네트워크의 사전 학습된 비전 인코더를 통해 $$x$$를 통과시켜 특징 벡터 $$f$$를 계산하는 것으로 시작됩니다.
- 동시에, 각 질병 카테고리에 대해 시각적 증상 생성기(VSG)가 몇 가지 시각적 증상을 생성합니다.
- 이러한 시각적 증상은 컨텍스트 프롬프트 모듈(CoP)에서 변형되어 이중 프롬프트 네트워크를 위한 텍스트 입력 임베딩을 생성합니다.
- 이러한 텍스트 임베딩은 사전 학습된 텍스트 인코더를 통해 처리되어 각 시각적 증상의 텍스트 특징을 계산합니다.
- 다음으로, 병합 프롬프트 모듈(MeP)은 텍스트 특징을 집계하여 질병 카테고리 $$c$$에 대한 대표 특징 $$sc$$를 얻습니다.
- 모든 카테고리 $$c ∈ C$$에 대해 집계된 시각적 설명 특징 집합 $$S = \{sc1 , sc2 , ..., scn \}$$을 얻습니다.
- 마지막으로, 가장 높은 코사인 유사도 점수 $$f \cdot sc, c∈ C$$를 가진 질병 카테고리를 예측합니다.
- 다음 섹션에서는 VSG와 이중 프롬프트 네트워크에 대해 자세히 설명합니다.

---

# 2.2 Visual Symptom Generator (VSG)

- VSG는 각 질병 카테고리마다 특정한 시각적 증상의 포괄적인 세트를 생성하는 것을 목표로 합니다.
- LLMs의 폭넓은 지식과 자연어로 쉽게 질의할 수 있는 특성을 활용하여, GPT-4와 같은 대형 언어 모델을 이용해 두 단계의 과정으로 시각적 증상 세트를 구축합니다.
- **첫 번째 단계**: 텍스트만으로 된 프롬프트를 사용하여 대략적인 시각적 증상을 얻음.
  - 프롬프트 예시:
    - Q: CLIP이라는 시각-언어 모델을 사용하여 {modality}에서 {category}를 탐지하려고 합니다. {category} 진단에 유용한 의료 시각적 특징은 무엇입니까? CLIP이 이해할 수 있게 간단한 단어로 설명하여 리스트를 작성해 주세요. {category}라는 단어는 사용하지 마세요.
  - 여기서 {category}는 주어진 카테고리 $$ c \in C $$로 대체되고, {modality}는 데이터셋의 이미징 모달리티로 대체됩니다 (예: 피부현미경 이미지).
  - 프롬프트는 GPT-4에 충분한 배경 지식을 제공하고, CLIP이 이해하기 쉬운 답변을 보장하는 것이 목표입니다.

- **두 번째 단계**: GPT-4의 시각적 질문-응답 기능을 활용하여 대략적인 세트를 다듬음.
  - 프롬프트 예시:
    - Q: 16개의 서브 이미지가 포함된 이 {category} 이미지의 색상, 모양 및 텍스처에 관한 시각적 특징을 제공해 주세요.
  - 여러 이미지의 응답을 통해 일반적으로 관찰되는 시각적 특징의 세트를 얻고, 처음의 대략적인 세트와 교집합을 취하여 정제된 세트를 얻습니다.
  - GPT-4 [1]를 사용하여 설계한 패이프라인을 통해 생성된 시각적 증상이 Fig. 2에 나타나 있습니다.
  - 생성된 시각적 증상은 일반적으로 병변의 색상과 모양에 대한 설명, 특정 구조물의 존재나 부재, 기타 관련 시각적 특징을 포함합니다.

---

# 2.3 Dual-Prompt Network

- Dual-Prompt Network는 CLIP을 기반으로 구축되었으며, CLIP의 이미지 인코더와 텍스트 인코더를 고정하여 대규모 사전 훈련 데이터로부터 일반적인 지식을 유지합니다.
- 기존의 CLIP 기반 접근 방식과 달리 텍스트 입력에 카테고리 이름을 사용하는 대신, VSG에서 생성된 시각적 증상을 활용하여 이미지 특징과 시각적 기술 특징의 정렬을 촉진합니다.
- 그러나, 우리의 프레임워크의 일반화 능력은 여전히 제한적입니다. 이 제한은 LLM의 응답에서 예상되는 CLIP 텍스트 입력 형식과의 잠재적인 편차와 명시적인 훈련 없이 질병 표현으로 시각적 증상을 효과적으로 통합하기 어려운 점에서 발생합니다.
- 이를 개선하기 위해 두 가지 학습 가능한 프롬프트 모듈: 컨텍스트 프롬프트(CoP)와 병합 프롬프트(MeP)를 제안합니다.

- **CoP(컨텍스트 프롬프트)**:
  - 카테고리 이름 외에도, 컨텍스트 단어는 이미지의 컨텍스트를 명시하는 완전한 문장을 형성하는 데 도움을 줍니다.
  - 예를 들어, CLIP는 카테고리 이름 앞에 {a photo of a}라는 컨텍스트를 추가합니다.
  - 의료 작업의 컨텍스트를 포착하기 위해 시각적 증상 앞에 맞춤형 템플릿을 삽입하는 것이 바람직하지만, 이러한 템플릿을 수작업으로 설계하는 것은 어려운 일입니다.
  - 따라서, $$\{p_i\}_{i=1}^M$$, $$p_i \in \mathbb{R}^d$$, $$i = 1, 2, ...M$$ 및 $$d$$는 텍스트 임베드 차원인 학습 가능한 토큰 세트를 도입하여 의료 작업의 컨텍스트를 데이터 주도로 학습합니다.
  - 최종 텍스트 입력 단어 임베딩 $$T$$는 학습 가능한 토큰과 $$e_d$$의 연결인 $$T = \text{Concat}(p_1, p_2...p_M, e_d)$$로 형성됩니다.

- **MeP(병합 프롬프트)**:
  - 텍스트 인코더를 통해 처리된 시각적 증상을 단일 표현으로 병합하는 구조입니다.
  - 이전 방법들은 평균 또는 최대 함수를 이용하여 모든 시각적 증상을 동등하게 취급하거나 가장 두드러진 특징에 의해 진단했습니다.
  - 하지만 모든 시각적 증상이 질병에 동등하게 기여하지 않으며, 가장 두드러진 시각적 증상만으로는 정확한 진단이 불가능한 경우가 많습니다.
  - 따라서, 각 질병 카테고리에 대해 학습 가능한 토큰을 도입하여 질병의 대표적 특징을 학습합니다.
  - 주어진 카테고리 $$c \in C$$와 텍스트 특징 행렬 $$T = [T^c_1, T^c_2, ..., T^c_k]^T$$ , 여기서 $$T \in \mathbb{R}^{k \times d}$$ 및 $$d$$는 텍스트 임베드 차원입니다.
  - 그리고 학습 가능한 그룹화 토큰 $$g \in \mathbb{R}^d$$ 를 도입하고, 다음과 같은 과정으로 $$Q$$와 $$K$$를 얻습니다:
    $$Q = gW_q, K = TW_k.$$
  - 질병 카테고리의 시각적 기술 특징을 집계하여 최종적으로 다음과 같이 표현됩니다:
    $$s^c = g + \text{Softmax}(QK^T \vert \sqrt{d})T.$$
- 모든 질병 카테고리의 집계된 시각적 기술 특징을 얻은 후, CoP와 MeP는 교차 엔트로피 손실로 최적화되며, 수식은 다음과 같습니다:
  $$L_{ce} = -\log \frac{\exp(f \cdot s_{c_y} / \gamma)}{\sum_{i=1}^{N} \exp(f \cdot s_{c_i} / \gamma)},$$
  여기서 $$c_y$$는 실제 질병 카테고리를 나타내며 $$\gamma$$는 학습된 온도입니다.

---

# 3.1 Dataset and Implementation Details

- **Dataset:**
  - **Pneumonia Dataset:**
    - 품목: 흉부 X선 이미지가 포함됨.
    - 분류: 정상 폐 또는 폐렴으로 분류.
    - 공식 데이터 분할: 5232개의 훈련 이미지와 624개의 테스트 이미지.
    - 추가적으로, 훈련 세트를 9:1 비율로 나누어 훈련 및 검증 세트 구성.
  - **Derm7pt Dataset:**
    - 품목: 2000개 이상의 임상 및 피부경 이미지.
    - 클래스: "멜라노마"와 "모반" 클래스의 827개 이미지를 사용.
    - 데이터 분할: 346개의 훈련 이미지, 161개의 검증 이미지, 320개의 테스트 이미지로 구성.
  - **평가 지표:**
    - 사용된 지표: 정확도(ACC)와 매크로 F1-점수(F1).
    - 매크로 F1-점수는 클래스별 F1 점수의 산술 평균을 계산하여 데이터 불균형 문제를 해결.

- **Implementation Details:**
  - 모델 학습: NVIDIA RTX 3090 GPU에서 제안된 ViP 모델을 학습.
  - 결과 평균: 세 가지 비전 백본(ViT-B/16, ViT-L/14, ResNet-50)에서 결과 평균화.
  - 설정:
    - 텍스트 임베딩 차원 $$d$$는 512로 설정 (CLIP [21]에 따라).
    - 컨텍스트 프롬프트(CoP)의 길이 $$M$$는 4로 설정 (CoOp [26]을 참조).
    - 학습 방법: SGD 사용 및 초기 학습률 0.001.
    - 학습 에포크: 50으로 설정.
    - 교차 엔트로피 손실의 온도 $$\gamma$$는 $$\frac{1}{100}$$로 설정 (CLIP [21]에 따라).

---

# 3.2 Comparisons with State-of-the-art Methods

- 설명 가능한 시각 증상의 효과:
  - 질병 진단을 위한 시각 증상의 효과를 평가하기 위해 제로샷 실험을 수행.
  - 우리의 접근 방식은 이미지를 시각적 설명 특징의 평균 임베딩과 비교하여 결정.
  - 제로샷 CLIP과 비교 시, Pneumonia에서는 0.44%, Derm7pt에서는 18.73% 정확성이 향상.
  - F1-score에서도 각각 1.58% 및 10.98% 향상.
  - 이는 LLMs가 의료 분야에 유익한 지식을 제공할 수 있음을 시사.

- CLIP이 실패하지만 우리의 방법이 질병 카테고리를 올바르게 예측한 사례 분석:
  - 이미지와 정확한 카테고리의 특성 간 유사성이 높아 진단 정확도가 향상.
  - 예: nevus로 진단된 이미지(사진 II(d))는 경계가 뚜렷하고 일관된 갈색, 병변 주위의 부기로 인해 특성이 많음에도 안쪽에 작은 검은 영역 존재.

- 진단 실패 사례:
  - 사진 4(I)에서는 폐렴의 특성이 표시되지만 전반적 유사성은 낮음.
  - 평균 함수를 통해 질병의 시각적 특징을 대표하는 데 한계.

- 관련 방법과의 비교:
  - ViP을 여러 최첨단 프롬프트 기반 모델과 비교하여 일반화 능력을 평가.
  - Table 1에서 ViP이 최고 정확도 86.69%, 81.11% 및 F1-score 84.94%, 77.3%로 Pneumonia 및 Derm7pt에서 두드러지는 성과를 보임.
  - 완전한 지도 학습 모드와 비교해 볼 때 Pneumonia에서는 경쟁력 있는 결과를 보이며, Derm7pt에서는 더 적은 훈련 데이터 상황에서 큰 격차로 우수한 성과를 나타냄.
  - 이는 낮은 리소스 환경에서 ViP의 강력한 일반화 능력을 입증.

---


# 3.3 Ablation Study

- 각 구성 요소의 효과를 평가하기 위해 ViP(Vision Prompt)에서의 구성 요소들의 효과를 탐구하기 위해 어블레이션 연구를 수행합니다. 이는 Table 2에 나타나 있습니다.
- 제로샷 베이스라인과 비교했을 때, CoP(Context of the Prompt)와 MeP(Message of the Prompt)의 통합은 상당한 개선을 보여주며, 이는 의료 작업 맥락 학습과 시각적 증상의 효과적인 집계의 중요성을 입증합니다.
- 평균 및 최대 함수 같은 비매개변수적 집계 방법과 비교했을 때, 우리의 제안된 MeP는 두 데이터셋 모두에서 우수한 성능을 보여줍니다.
- 이 결과는 우리 방법의 효과를 추가적으로 입증합니다.

- 추가 실험에서는 LLM(Large Language Models)에서 생성된 시각적 증상이 의료 도메인 일반화에 유용한 지식을 제공한다는 주장을 검증했습니다.
- Fig. 5에서 우리는 네부스(nevus)의 시각적 증상을 세 가지 종류의 지식으로 대체했습니다:
  1) 의료 도메인과 관련 없는 시각적 증상, 예를 들어 음식에 대한 설명과 같은 도메인 외 지식.
  2) 진단에 도움이 되지 않는 유용하지 않은 지식, 예를 들어 피부 구조에 관한 설명과 같이 목표 질병과 관련은 있지만 진단에 유용한 정보를 제공하지 않는 지식.
  3) 진단을 위한 잘못된 증상을 제공하는 부정확한 지식. 이 실험에서 네부스의 설명에서 특정 단어를 그 반댓말로 바꾸어 잘못된 설명을 생성했습니다.
- 다른 변형들과 비교했을 때, LLM에서 생성된 지식이 최상의 성능을 보여주며, 이는 정확한 시각적 증상이 의료 도메인 일반화에 기여함을 나타냅니다.


---

# 4 Conclusion

- 본 논문은 ViP라는 혁신적인 시각 증상 기반 프롬프트 학습 파이프라인을 제시하여 VLM의 지식을 의료 이미지 분석으로 효과적으로 전이함.
- 사전 학습된 LLM을 활용하여 유용한 시각 증상을 생성하고, 이를 통해 CLIP이 이미지 특징을 시각 증상과 맞추도록 함.
- ViP는 학습 가능한 두 개의 프롬프트 모듈(컨텍스트 프롬프트와 병합 프롬프트)을 포함하여 일반화 능력을 더욱 향상시킴.
- 실험 결과는 각 모듈의 효과성을 강조하였고, 우리 파이프라인이 최첨단 방법보다 우수하다는 것을 입증함.
- 향후 연구는 데이터와 주석이 부족하고 비용이 많이 드는 희귀 질병 및 기형 기관의 진단과 같은 다른 의료 이미지 분석 작업으로 프레임워크를 확장하는 데 중점을 둘 것임.
- 또한, 컨텍스트 프롬프트의 해석 가능성을 향상시키기 위한 기법을 조사할 계획임.
- 감사의 말씀: Yi Gu, Yibo Hu, Xiaoyu Fu와의 유익한 논의에 감사드리며 이 연구는 홍콩 혁신기술기금, 합동 과학 및 기술 혁신 협력구 프로젝트 및 홍콩 연구보조위원회의 지원을 받았음.
- 이해 충돌 고지: 저자들은 본 논문의 내용과 관련하여 선언할 이해 충돌이 없음.