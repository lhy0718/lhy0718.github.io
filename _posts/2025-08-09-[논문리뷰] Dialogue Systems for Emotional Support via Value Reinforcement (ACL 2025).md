---
title: "[논문리뷰] Dialogue Systems for Emotional Support via Value Reinforcement (ACL 2025)"
date: 2025-08-09 22:16:05 +0900
categories:
  - Paper Review
tags:
  - ACL 2025
  - empathetic dialogue systems
---

본 연구는 정서지원 대화 시스템에 가치 강화(value reinforcement)를 도입한 가치 주도 학습 방법을 제안한다. Reddit 데이터를 활용해 각 대화 턴에서 강화할 가치를 식별하고, 가치 강화를 통한 성능이 지원 기술, 감정 강도, 가치 강화 측면에서 우수함을 실험으로 입증했다.

---

# 1 Introduction

- 감정지원은 관계 갈등, 직장 스트레스 등 일상적 정서적 어려움을 다루도록, 위로, 수용, 격려를 제공하는 것을 목표로 한다(Atoum and Al-Shoboul, 2018; Burleson, 2003).
- 최근 대형 언어 모델의 발전은 대화 시스템(지원자 모델)의 개발을 가속화했다(Deng et al., 2024; Zhang et al., 2023; Chen et al., 2023).
- 하지만 많은 모델이 긍정적 감정의 강화에 집중하는 반면, 감정 변화만으로는 seekers의 더 깊은 내적 변화가 충분히 포착되지 않을 수 있어 장기적 효과가 제한될 수 있다. 예를 들어, 표면적인 감사 인사(“감사합니다”)가 감정 분류기에서 더 높은 긍정 점수를 받더라도, 더 나은 대응은 아니라고 평가되는 경우가 있다.
- 이러한 한계를 극복하기 위해 가치 강화(value reinforcement) 기반의 감정지원 접근이 제안된다. 인간의 가치(핵심 신념과 지향점)는 삶의 목적과 정체성에 깊게 연결되며, 자기지향성(self-direction), 자비(Benevolence) 등 다양한 가치가 내재적 변화에 기여한다.
- 가치의 중요성은 ACT(수용 및 헌신 치료) 및 가치 확인 개입(Values Affirmation Interventions)과 같은 현대 심리치료에서도 강조되며, seekers가 자신의 가치와 일치하는 목표를 추구하도록 돕는다.
- ESConv 등 널리 사용되는 감정지원 데이터셋에서 높은 효과를 보이는 seekers는 긍정적 가치가 더 뚜렷하게 나타나는 경향이 있다(다음 섹션 3 참조).
- 본 연구는 seeker 시뮬레이터를 활용한 시뮬레이션 기반 학습으로 supporter 모델을 훈련하는 프레임워크를 제시한다. 핵심 구성요소는 두 가지다:
  - (1) 목표 가치 탐지기(target value detector): 턴마다 강화해야 할 가치를 식별
  - (2) 참고 생성기(reference generator): 해당 가치를 강화하는 supporter 응답을 생성
- 이 둘의 출력을 통합해 seeker의 반응에서 가치 강화 보상을 극대화하도록 supporter 모델을 학습한다.
- 학습 과정은 두 단계로 구성된다:
  - 감독 미세조정(supervised fine-tuning): GPT-4o-mini의 시뮬레이션 능력을 더 작은 모델에 증류
  - 직접 정책 최적화(direct policy optimization, Rafailov et al., 2023): 모델의 가치 강화 효과를 향상
- 평가 범주는 크게 세 가지로 이뤄진다: supporter 능력, seeker의 최종 구제감(감정 완화), 가치 강화의 효과성.
- 실험 결과, 본 모델은 대부분의 기준에서 baselines를 상회하는 supporter 능력과 가치 강화 성과를 보였으며, seeker 구제감에서도 경쟁력을 유지하는 것으로 나타났다. 전문가 치료사들의 평가에서도 가치 강화의 강점이 두드러졌다.
- 본 연구의 주요 시사점은 다음과 같다:
  - 가치 강화로의 명시적 통합이 감정지원 시스템의 새로운 방향이 될 수 있음을 제시
  - 목표 가치 탐지기와 참고 생성기라는 두 가지 핵심 요소를 활용한 두 단계 접근 방식의 효과성 제시
  - Reddit 데이터를 활용한 실제 세계 지식 학습이 가치 강화와 감정 완화 성과를 높임을 보임
- 본 논문의 주요 기여
  - 가치 강화를 명시적으로 감정지원 시스템에 통합한 최초의 연구 중 하나
  - 목표 가치 탐지기와 참고 생성기의 두 단계 프레임워크를 제시하고, Reddit 데이터로 학습
  - 가치 강화를 통한 성과 향상으로, 감정지원 시스템에 가치 요소를 도입하는 길을 열어줌

---

# 2 Related Work

- 2.1 인간 가치와 정서 지원
  - 인간 가치는 삶에서 무엇이 중요한지 추구할 가치를 식별하도록 돕는 기본 신념이며, 이는 Searle(2003)에서 제시됨.
  - 가치에 맞춘 의사결정은 심리적 유연성(삶의 도전에 효과적으로 적응하는 능력)을 향상시키고, 학업 성취(Cohen et al., 2006, 2009)와 같은 장기적 결과를 지지함.
  - 가치 강화를 통해 구하는 사람(seeker)과 지지자(supporter) 간의 연결이 강화되고, 더 효과적이고 지지적인 대화의 기초가 형성됨(Wilson and Murrell, 2004).
  - 구하는 사람이 가치와 행동에 연결되도록 격려함으로써 장기적으로 긍정적 변화가 촉진되고 대인 관계의 역학이 풍부해져 대화가 더 의미 있고 영향력 있게 됨.

- 2.2 정서 지원을 위한 대화 시스템
  - 지지자 모델 향상을 위해 연구자들이 다양한 접근을 탐구함.
  - 한 방법은 대형 언어 모델(LLM)을 활용해 지지자 모델 학습용 다양한 대화를 생성(Zheng et al., 2024; Liu et al., 2023; Qiu et al., 2024)하는 것.
  - 또 다른 연구는 seekers의 미래 상태를 예측해 지지자 모델 학습을 개선(Zhou et al., 2023; Cheng et al., 2022; Shin et al., 2020)하는 데 초점을 둠.
  - 최근에는 seeker 시뮬레이터를 활용한 다중 턴 시뮬레이션으로 미래 반응을 예측(Deng et al., 2024)하는 노력이 증가.
  - 그러나 대부분의 연구는 인간 가치의 역할을 간과하는 경향이 있음.
  - 본 연구는 시뮬레이션 기반 학습에 기반하되, 주요 기여는 가치의 통합을 통해 정서 지원의 시스템적 효과성을 강화하는 데 있음.

---

# 3 Value Effects in Emotional Support

- 3.1 인간 가치의 분류(Taxonomy for Human Values)
  - Kiesel 등(2022)이 제시한 가치 분류 체계: Schwartz의 기본 가치 이론(Schwartz Theory of Basic Values)과 함께 Rokeach(1973), Brown & Crace(2002), Haerpfer 등(2020)의 세 가지 주요 가치 목록을 통합.
  - Schwartz 이론은 NLP 연구와 사회과학(예: 유럽 사회조사 ESS)에서 널리 활용되어 사람들의 태도, 신념, 행동의 변화를 추적하는 기반으로 사용되어 왔음.
  - 이 통합 분류는 인간 가치를 20개 가치 범주로 조직화한다는 특징이 있으며, 각 가치에 대한 자세한 내용은 표 38에 수록되어 있다고 요약됨.

- 3.2 가치가 감정 지원의 효과에 미치는 영향 탐구(Exploring the Impact of Values on Emotional Support Effectiveness)
  - 연구 동기: 가치가 감정 지원의 효과에 미치는 역할을 ESConv 데이터셋(Liu et al., 2021)을 통해 분석하고자 함. ESConv은 크라우드워커 간의 다회 대화에서 감정 지원의 맥락을 담고 있음.
  - 방법 요약
    - ESConv에서 대화 전후 부정 정서의 강도는 1(최저)에서 5(최고) 스케일로 평가됨.
    - 초기 강도 5인 대화를 두 그룹으로 나눌 때:
      - 고효과성 그룹: 최종 강도 1–2
      - 저효과성 그룹: 최종 강도 3–4
    - 대화의 마지막 네 턴에서의 긍정적 가치 표현을 자동 분류기로 분석. 이는 턴 길이의 차이를 보정하고 대화의 변화 효과를 포착하기 위함.
    - 실험 절차와 분석 상세 내용은 부록 A에 기재.
  - 수식 예시
    - 고효과성 그룹 및 저효과성 그룹의 구분은 다음과 같이 표현될 수 있음:
    $$ \text{high effectiveness}: finalIntensity \in \{1,2\} \\
       \text{low effectiveness}: finalIntensity \in \{3,4\} $$
  - 결과 요약
    - 고효과성 그룹은 저효과성 그룹에 비해 마지막 네 턴에서 긍정적 가치 표현의 평균 수가 더 많았음: 7.9 vs 6.5.
    - Figure 2는 고효과성 그룹에서 두드러진 가치 표현을 시각화해 보여줌.
    - Table 39는 seekers의 발화를 통해 나타난 가치 표현의 예시를 제시.
  - 연구 의의
    - 가치 강화가 감정 지원의 효과를 긍정적으로 향상시킨다는 경험적 근거를 제공.
    - seekers의 가치를 강화하도록 설계된 대화 시스템의 방향성을 제시하며, 가치 강화 전략을 갖춘 대화 시스템 개발의 토대를 마련함.

---

# 4 Emotional Support Dataset from Reddit

- 목적 및 핵심 질문
  - 가치 강화 및 효과적 발화 생성을 위한 데이터 필요성 강조
  - 두 가지 핵심 질문: (1) 각 턴에서 강화할 가치가 무엇인지, (2) 어떤 지원자 발화가 해당 가치를 가장 효과적으로 강화하는지
  - 이를 위해 대규모의 authentic 대화 데이터가 필요하다고 제시

- 데이터 소스와 구성
  - Reddit의 r/offmychest 서브레딧에서 감정지원 교환의 다양성 확보
  - 원 게시자(OP) = seekers(발신자), 댓글 작성자 = supporters(지지자)
  - 게시물과 댓글 스레드의 구조가 대화 흐름을 닮아 감정지원 상호작용의 역학 포착

- 데이터 수집 및 품질 관리
  - 2019년부터 2023년까지의 게시물 및 댓글 수집(Watchful 제공)
  - upvote 비율 및 점수 등의 메트릭으로 고품질 대화만 선별
  - 공개적으로 이용 가능한 콘텐츠만 사용, 비공개/삭제/개인식별정보 제외

- 목표 모델링 프레임워크
  - 목표: 각 턴에서 강화할 가치를 식별하는 타깃 밸류 디텍터(Target Value Detector)
  - 레퍼런스 제너레이터(Reference Generator): 타깃 가치를 촉진하는 레퍼런스 응답 생성
  - 지원자 모델(Supporter Model): 타깃 가치와 레퍼런스 응답을 바탕으로 지원자 응답 생성
  - 프레임워크의 구성은 그림 3에 제시된 세 가지 컴포넌트: 타깃 밸류 디텍터, 레퍼런스 제너레이터, 지원자 모델

- 라벨링 및 표현 가치
  - 데이터에 감정 강도(sentiment strength)와 표현된 가치(expressed values)를 포스트 수준과 댓글 수준에서 라벨링
  - 라벨링에 Liu et al. (2024)와 Schroter et al. (2023)의 모델 활용

- 가치 및 성공 여부의 해석
  - OP의 긍정적 코멘트에 담긴 가치를 해당 시점의 성공적인 타깃 가치로 간주
  - 해당 가치들을 촉진하는 효과적인 지원자 발화로 이전 코멘트의 발화자 코멘트가 간주

- 데이터 규모
  - 총 2만 샘플 이상을 포함

- 부록 및 추가 상세
  - 분류 모델과 생성 데이터세트의 상세 내용은 Appendix B에 수록

- 기타 특징
  - 데이터가 대화 흐름의 본질을 반영하도록 설계되어, 가치 강화 전략 학습 및 효과적인 지원 발화 생성 연구에 유용하게 활용 가능

---

# 5 Method

- 전체 프레임워크는 세 가지 핵심 구성요소로 구성됩니다.
  - (1) Target value detector: 각 턴에서 강화할 가치를 식별
  - (2) Reference generator: seekers가 해당 가치를 효과적으로 촉진하도록 발화를 생성
  - (3) Supporter model: 식별된 target values와 reference responses를 바탕으로 전략을 결정하고 응답 생성

- 5.1 Target Value Detector
  - Edu: Reddit의 감정적 지원 대화를 학습 데이터로 사용(섹션 4)
  - 입력: 대화 이력 o1, c1, o2, c2, ..., c_{t-1}, o_t
  - 출력: ct에서 타깃으로 삼을 가치들
  - Ground-truth vt+1: ot+1에서 관찰된 상위 3개의 가치(확률 기반)
  - 수식:
    $$ vt+1 = \text{LMTVD}(o_1,c_1,o_2,c_2,\dots,c_{t-1},o_t) $$
  - 상세 학습 방법 및 결과는 Appendix D.1 참조

- 5.2 Reference Generator
  - 학습 데이터: Reddit 데이터로 학습
  - 입력: 대화 이력(o1,c1,o2,c2,...,c_{t-1},ot)와 OP의 다음 발화 ot+1에 반영된 vt+1
  - 목표: ct를 생성하는 모델 학습
  - 학습 단계
    - SFT( supervised fine-tuning )
      - 목적: 대화 이력과 ot+1의 가치 표현을 조건으로 supporter의 코멘트 ct 생성
      - 수식:
        $$ ct = \text{LMRG}(o_1,c_1,o_2,c_2,\dots,c_{t-1},o_t; vt+1) $$
    - DPO( direct preference optimization )
      - 목적: SFT 모델의 생성 품질 향상
      - 선호 데이터셋 구성:
        - 기본 설정: 대화 이력에서 ct를 원래의 선호 응답으로 간주
        - 대체 응답: c′_t를 동일 대화 이력의 형제 코멘트 중에서 무작위로 샘플
        - vt+1과 v′_{t+1} 사이의 중복 값을 제거하여 원래 선호 응답에 고유한 최대 3개의 타깃 값을 남김
      - 보강 방법 및 세부 방법은 Appendix D.2 참조
  - 추가: Table 1에 제시된 시뮬레이션 기반 학습 데이터셋 크기
    - SFT: train 33,130; dev 2,367
    - DPO SFT: train 3,301; dev 628
  - 목표: Reddit 데이터의 다양성 분포를 everyday 대화와 정렬되도록 보정

  - 5.3에서의 참고: Reference generator는 두 단계(SFT + DPO)로 학습되며, 시뮬레이션 데이터를 사용해 Llama-3-8B-Instruct를 미세조정합니다.

- 5.3 Supporter Model
  - 역할: seeker와 상호 작용하며 target values에 맞춰 응답 생성
  - 입력 세 가지: (i) 대화 이력, (ii) 각 턴에서 식별된 target values, (iii) Reference generator가 만든 reference 응답
  - 한 턴에서의 사고 흐름( four aspects ):
    - (1) seeker의 문제점 및 현재 상태 식별
    - (2) reference 응답의 핵심 내용 분석
    - (3) reference 응답을 최종 출력에 포함할지 여부 판단
    - (4) 최적의 감정적 지원 전략 선택 및 최종 응답 생성
  - 출력 결정: (3)에서 “Yes”(reference 포함) 또는 “No”(참조 비적합)와 그 이유를 제시
  - 프롬프트: 전체 프롬프트는 Table 22에 수록
  - 배경: Reddit 기반 참조 응답의 분포가 일상 대화와 항상 일치하지 않으므로 선택적 참조 포함 필요
  - 학습 과정: 두 단계(SFT, DPO)로 진행, 시뮬레이션 데이터를 사용
  - 시뮬레이션 기반 데이터 생성
    - 시뮬레이션 도구: seeker 시뮬레이터와 함께 GPT-4o-mini를 사용
    - 데이터 구성: 상호작용은 대화 이력을 프롬프트로 삼아 차례로 생성
    - reference 사용 규칙: 시뮬레이션 중 대략 90%에서 reference 응답을 사용하지 않음
    - 편향 방지: 기준 응답 없이도 대체 응답(alternative responses)을 각 턴에 추가 시뮬레이션
    - 학습 대상: Llama-3-8B-Instruct 미세조정
  - 데이터 규모(표 1 참조): SFT와 DPO 시뮬레이션 데이터
  - DPO 스테이지에서의 보상 기반 데이터 구성
    - 각 대화마다 각 supporter 턴에 대해 두 개의 응답 후보를 가정(참조 응답 포함 여부)
    - 각 응답의 보상 계산 후 선호 응답과 거부 응답 결정
    - 보상 R(usup_t) 정의:
      $$ R(u^{\text{sup}}_t) = \sum_{k=1}^{h} \gamma^{k-1} N_{t+k} $$
      - N_{t+k}: 턴 t+k에서 seeker의 발화에 나타난 타깃 값의 빈도
      - h: 미래 예측의 허용 범위(look-ahead horizon)
      - γ: 즉시 보상과 미래 보상 간의 균형 조정 계수
    - 보상 차이가 임계값 Tdiff를 넘길 때만 DPO 데이터에 추가
  - 시뮬레이션 기반 대화는 seeker 시뮬레이터와의 상호작용으로 추가로 생성 가능
  - 시뮬레이션 데이터의 모델 학습 대상
    - 모델: Llama-3-8B-Instruct 미세조정
  - 시뮬레이션 기반 데이터의 취지: reference 응답의 효과를 보수적으로 평가하고, 다양하고 자연스러운 대화 흐름 확보

- 5.4 Seeker Simulator
  - 역할: 제공된 페르소나와 대화 이력을 바탕으로 seeker 발화 생성
  - 페르소나 생성: GPT-4o 및 GPT-4o-mini를 활용, 문제 유형, 감정, 상황 등 속성 정의
  - 생성 규모: 총 2,036개의 고유 페르소나
    - 학습용 1,796, 개발용 120, 테스트용 120
  - seeker 시뮬레이터: GPT-4o-mini 기반
  - 검증 및 품질 확인
    - 사람 평가: seeker 시뮬레이터의 발화가 인간의 발화만큼 자연스럽거나 더 자연스러움
    - 콘텐츠 측면: GPT-4o-mini의 seeker 시뮬레이터가 인간 seeker의 발화에 더 근접한 내용, 감정 톤, 가치 정렬
  - 추가 평가 및 근거: 인간 및 자동 평가를 통한 품질 검증(Figure 4 참조)

- 보충 정보
  - 전체 프롬프트의 구성 및 세부는 Table 22, Appendix C, D, E에 자세히 기재
  - 실험 설계에서 참조 응답의 사용 여부와 대체 응답의 시뮬레이션 등을 통해 편향을 줄이고 보다 일반화된 대화 흐름을 확보하려는 의도 포함

---

# 6 Experiments

- 6.1 평가 방법
  - seeker 시뮬레이터의 120개 보유 대상 페르소나로 대화를 통해 다양한 지원자 모델을 평가
  - 대화가 완료된 것으로 간주되는 조건: seeker 시뮬레이터가 “[END]”를 생성하거나 EmoLlama-Chat-7B의 감정 점수(|emotion score|)이 0.6 이상이고 감사 표현(예: “thank you”)이 포함될 때
  - 대화 턴 수 제한: 최대 20턴, ESConv 데이터셋의 평균 대화 길이(약 15턴) 기준
  - 이 제한 내에 종료된 대화만 평가에 포함

- 6.2 평가 지표
  - ES-Skills: 지원자의 능력을 3개 구성으로 평가
    - (1) 감정 지원 기술: Identification, Comforting, Suggestions, Experience, Informativeness
    - (2) 일반 대화 기술: Consistency, Role-Adherence, Expression, Humanness
    - (3) Overall
    - 각 기준은 5점 척도, GPT-4o-mini로 등급 산정
  - ES-Intensity: 대화 후 seeker의 부정적 감정 강도의 변화로 평가
    - 5점 척도, 낮은 점수가 더 좋음
    - 인간 시청자들의 평가를 근거로 한 GPT-4o-mini 기반 예측 모델 개발
    - 이 모델은 실제 평가와 0.345의 상관성을 보임
  - ES-Value: 대화 중 가치 탐색/강화의 경험과 지원자의 기여를 포함한 가치 강화 평가
    - 모델 간 쌍대 비교를 GPT-4o-mini가 심판으로 활용
    - 단일 대화에 대해 1–5 척도 평가의 편향 문제를 완화하기 위한 방법
    - 라이선스된 치료사들의 평가와의 상관 분석 수행
    - 모든 기준에서 양의 상관(0.198–0.778) 확인, 상당 부분이 통계적으로 유의

- 6.3 Baselines
  - Prompt-Based: GPT-4o-mini, Llama-38B-Instruct
  - ES 데이터셋 기반: Reddit(섹션 4), ESConv(Liu et al., 2021), ExTES(Zheng et al., 2024), Psych8k(Liu et al., 2023) 등
  - ES 방법론: Ask-an-Expert(Zhang et al., 2023), ESCoT(Zhang et al., 2024), PPDPP(Deng et al., 2024)
  - Emotion-Reinforced: 가치가 아닌 긍정적 감정 강화에 초점을 맞춘 버전도 실험에 포함(Appendix H)
  - 세부 내용은 Appendix G 및 기타 부록 참조

- 6.4 평가 결과
  - 6.4.1 가치 타깃팅 및 참조 응답의 효과
    - GPT-4o-mini를 대화 파트너로 한 절차적 분석에서 target values와 reference responses를 모두 사용할 때 ES-Skills가 크게 개선되고 ES-Intensity가 감소
    - 특히 Suggesting, Expression, Informativeness 등의 핵심 ES-Skills가 두 요소를 함께 사용했을 때 크게 향상
    - 가치 강화 역시 target values와 reference responses를 함께 활용할 때 크게 개선
    - Reddit의 현실 지식을 활용한 reference 응답의 효과 강조
    - 우리의 파인튜닝 모델은 GPT-시뮬레이션 데이터를 바탕으로 학습되었으며, 이후 실험에서도 동일한 구성(타깃 가치 및 참조 응답)을 사용
  - 6.4.2 Baselines와의 성능 비교
    - ES-Skills: DPO 모델이 대다수 지표에서 baselines를 능가, 특히 Suggestions, Experience, Informativeness에서 두드러진 개선
    - 표현력 관련 지표인 Expression, Humanness에서도 향상
    - Emotion-Reinforced 버전도 대부분의 baselines를 상회
    - ES-Intensity: DPO가 대다수 baselines보다 부정적 감정의 강도를 더 잘 감소
    - 다만 Llama-Psych8k는 ES-Intensity가 더 낮았는데, 이는 대답 길이(약 73단어)로 인한 영향으로 분석
    - ES-Value: 대다수 비교에서 우리의 모델이 우수하지만 Emotion-Reinforced DPO나 일부 파인튜닝 모델과는 비슷한 수준으로 나타난 경우도 있음
    - 40개 대화의 ES-Value 낮은 사례 분석 결과, 수용자의 고유한 강점/성과를 식별하고 감정 상태를 깊이 있게 다루는 능력 개선 필요
  - 6.4.3 Target Value Reinforcement의 성공성
    - ES-VR(DPO) 모델과 타 모델 간 타깃 값 재강화 효과 비교에서, 동일 입력(타깃 값 및 참조 응답)을 사용했을 때 1–3턴 이후에도 ES-VR이 항상 우수한 것으로 나타남
    - 다음 턴들에서 타깃 값이 관련성을 유지하는 비율이 더 높음(Two-Three Turn 분석)
    - 상세는 Appendix I 참조
  - 6.4.4 Reddit 데이터 외 일반화(Généralisation)
    - Reddit 데이터를 넘어 CBT 기반의 Cactus 데이터셋으로 학습한 ES-VR의 일반화 여부 평가
    - ES-Value에서 DPO(Cactus)가 모든 Baselines를 능가, 가치 신호 학습과 적용 능력 입증
    - ES-Skills에서도 DPO(Cactus)가 대체로 Baselines보다 우수하거나 비슷한 수준으로 나타남
    - 다만 Experience 메트릭은 Reddit 기반 DPO보다 낮았는데, 이는 Reddit의 공유 개인 서사가 참조 응답 생성에 더 큰 도움을 주는 것으로 분석
  - 6.4.5 전문가 평가
    - 두 명의 면허를 가진 임상 심리학자(임상 경력 3년 이상)가 ES-VR(DPO) 대화의 질적 분석 수행
    - 강점: seeker의 도전 과제를 효과적으로 확인하는 공감 표현, 신뢰 형성, 자기인식 및 가치 강화 촉진
      - 예: “which is completely understandable” 같은 문구를 통해 공감을 검증하고 탐색 촉진
      - seeker의 상황에서 긍정적 측면 강조 및 가치 강화 관련 목표 제시
    - GPT-4o의 반응과 비교 시 차이: GPT-4o는 더 많은 공감을 지속하고 부정적 분위기에 머무르는 경향이 있음
      - 이는 인간-기준의 선호 정렬로 인해 발생하는 경향으로 보이며, 우리의 방법은 seeker의 자각과 가치 강화에 더 초점을 둠
    - 개선점 제안
      - seeker의 관점과 상황에 대한 더 깊은 이해를 통해 맞춤형 지원 강화
      - 가치 추구와 관련한 잠재적 장애 요인에 대한 대처 제시 강화
      - 제안하는 가치에 대한 명확한 정의와 구체적 예시 제공, seeker's own 해석을 유도하여 lived experiences와의 연결 강화

- 부록 및 추가 분석 언급
  - 표 2, 표 3, 표 4 및 Appendix M/I/J/K 등에서 세부 수치 및 추가 방법론 확인 가능
  - ES-VR의 Target Value Reinforcement 및 일반화 실험의 구체적 절차와 파라미터 설정은 Appendix M, I, J, H 등 참조

- 요약 결론
  - Target value 설정과 참조 응답 활용을 병행하는 것이 ES-Skills 향상과 ES-Intensity 감소에 가장 큰 기여를 하며, 가치 강화 측면에서도 우수한 성능을 보임
  - Reddit 기반 학습이 참조 응답의 질에 큰 영향을 미치나, CBT 기반 데이터로의 일반화도 가능하나, 특정 메트릭에서 차이가 존재
  - 전문가 평가를 통해 가치 강화의 실제 효과와 개선점이 구체적으로 제시되었으며, 향후 개선 방향으로서는 개인적 맥락 이해 강화, 장애 요인 대응, 가치 정의의 구체화가 제시됨

---

# 7 Conclusion
- 본 논문은 현대 심리치료에서 강조되는 가치 강화(value reinforcement)에 기반한 최초의 감정 지원 프레임워크를 제시한다.
- 프레임워크는 목표 가치 탐지기(target value detector)와 참조 생성기(reference generator)를 포함하여, 지원자 모델이 가치에 정합적이고 효과적인 지원 응답을 생성하도록 능력을 향상시킨다.
- 평가 결과, 제안 프레임워크가 감정 지원의 품질과 가치 강화 측면에서 기존의 기준 모델들을 능가함을 보인다.
- 전문가 치료사 평가에서는 seeker의 도전과제를 검증하고 상황의 긍정적 측면을 강조하는 모델의 강점을 더욱 강조하며, 이는 효과적인 감정 지원의 핵심 요소로 확인된다.
- 이러한 결과는 가치 강화를 통한 지원 상호작용의 잠재력과 더 효과적인 감정 지원 시스템 개발의 기초를 제공한다.

---

# Limitations

- 장기적 평가의 부재: 본 프레임워크의 장기적 효과를 경험적으로 검증하지 못했다. 이전 연구가 상담 및 의사결정에서 가치 강화의 장기 이점을 시사하지만, 본 프레임워크의 장기적 결과는 아직 검증되지 않았다. 향후 연구에서 확장된 시간 프레임으로 지속적 영향을 평가해야 한다.
- 섹션 6.4.5의 개선 여지: seekers의 이슈와 사고를 심층적으로 탐구하고, 잠재적 장애물과 좌절에 대한 대응을 강화하는 방향이 필요하다.
- 데이터 셋 및 학습 방법론의 필요성: 더 포괄적인 데이터 셋 구축과 학습 방법론의 발전이 필요하며, 이를 통해 다양한 상황에서의 일반화와 견고성을 높여야 한다.
- DPO 훈련 시뮬레이션의 한계 및 추가 요소 필요성: 현재 시뮬레이션은 참조 응답의 사용을 기반으로 대화 경로를 다양화하고 보상을 가치 강화로 평가하는 데 초점을 맞췄다. 그러나 전략 선택 등 다른 요인도 가치 강화에 상당한 영향을 미칠 수 있어, 이러한 추가 요인을 포함한 시뮬레이션 및 훈련이 필요하다.
- 향후 연구 방향 요약: 장기적 효과의 검증, 심도 있는 문제 탐구 및 장애물 대응 강화, 데이터 및 학습 방법론 보강, 그리고 추가 요인을 반영한 시뮬레이션과 훈련의 도입이 필요하다.

---

# Ethical Considerations

- 자기 노출 공유 전략
  - 정서적 지지에서 친밀감을 형성하는 핵심 전략으로, 이전의 정서적 지지 시스템에서도 중요한 평가 기준으로 작용해 왔음(Zhang et al., 2023).
  - 다만 대화 시스템이 이러한 경험을 개인적인 것으로 제시하면 일부 사용자가 불편함을 느낄 수 있음.
  - 모델에서 자기 노출 전략을 제거하면 정서적 지지의 질에 영향을 미친다는 것을 Appendix L에서 확인했으며, 보다 정교한 경험 공유 방식에 대한 추가 연구가 필요하고 이는 향후 연구 방향으로 남김.

- 잠재적 위험 및 남용 방지
  - 시스템은 대인관계 갈등 및 학업 스트레스와 같은 일상적 도전에 대한 정서적 지지을 제공하되, 전문 심리 개입을 대체하지 않는다는 점을 명확히 제시.
  - 자동화 및 전문가 평가에서도 강력한 성능이 확인되었으나 특정 상황에서 응답이 의도치 않은 영향을 미칠 가능성 있음.
  - 이를 완화하기 위해 맥락에 민감한 응답 메커니즘을 구현하고 시스템을 전문가 치료의 보조 도구로 명확히 위치시킴.

- 편향성 및 과잉 일반화에 대한 대응
  - 온라인 플랫폼의 데이터에는 편향이 내재되어 있어 다양한 관점을 충분히 반영하지 못할 수 있음.
  - 이를 해결하기 위해 데이터 수집 대상과 기간을 신중히 선정해 정서적 지지 주제의 다양성을 확보하고, 참조 응답의 적합성을 평가하는 추가 필터링 절차를 지지자 모델에 도입함.
  - 균형 잡힌 시각을 촉진하여 공정하고 포용적인 지원을 제공하는 것을 목표로 함.
