---
title: "[논문리뷰] Evaluating the Ripple Effects of Knowledge Editing in Language Models (TACL 2024)"
date: 2025-03-01 00:00:00 +0900
categories:
  - Paper Review
tags:
  - LLM
  - NLP
  - Knowledge Editing
  - TACL
---

요약: 현대 언어 모델은 많은 사실적 지식을 포착하지만, 때때로 잘못된 정보가 생성될 수 있다. 본 연구에서는 이러한 사실 수정의 파급 효과를 평가하는 새로운 기준을 제안하고, 이를 통해 여러 수정 방법의 한계를 지적하였다.

---

# 1 Introduction

<img alt="image" src="https://github.com/user-attachments/assets/7adcf097-4d58-4f81-a862-f35630c51fd6" />

- 현대 언어 모델(LM)은 매개변수에 대량의 사실 지식을 포착.
- 이러한 지식은 후속 작업에서 효과적으로 활용 가능.
- 그러나 모델이 포착한 사실적 믿음은 부정확하거나 시간이 지남에 따라 구식이 될 수 있음.
- 이는 모델의 성능, 신뢰성 및 사용성에 영향을 미칠 수 있음.
- 이러한 한계는 지식 편집(Knowledge Editing, KE) 방법의 연구를 촉발.
  - KE는 LM의 사실 오류를 수정하는 것을 목표로 함.
  - 다양한 방법이 제안되어 LM의 매개변수에 사실을 주입.
- KE의 핵심 질문 중 하나는 이러한 편집 작업의 성공을 어떻게 평가할 것인지.
  - 기본적인 "정신 검증"은 모델이 수정된 사실을 올바르게 완성하는 것.
  - 그러나 다른 사실이 왜곡되지 않았는지 확인하는 것도 중요.
- 본 연구에서는 모형 수정 시 편집된 단일 사실을 넘어 서로 논리적으로 유도된 다른 사실들도 검증해야 한다고 주장.
- 이를 위해 "Ripple Effects"라고 불리는 편집에 따른 파생 효과를 고려한 평가 기준 제안.
- RIPPLE EDITS라는 새로운 벤치마크를 통해 KE의 포괄적인 평가 제공.
  - 5,000개의 엔트리를 포함하고, 각 편집에 대한 테스트 쿼리로 성공 여부 확인.
- 세 가지 인기 있는 편집 방법을 다섯 개의 강력한 LM에 대해 평가하며,
  - 현재 KE 방법이 해당 사실의 파생 효과를 잘 파악하지 못하는 경우 많음.
- 대형 모델이 ripple effects를 더 잘 처리하며, 빈번한 엔티티의 편집 시 논리적 오류가 더 발생함.
- 맺음말로 KE의 중요한 한계를 강조하고, comprehensive evaluation criteria 도입.
- RIPPLE EDITS와 코드 공개를 통해 KE에 대한 향후 연구 촉진.

---

# 2 Problem Setting

- 사실 지식 편집을 고려함.
- 사실은 주체(entity) e, 관계(r), 객체(o)로 표현되는 삼중항 형식으로 구성됨.
  - 예: (리오넬 메시, 팀, 인터 마이애미)
- 두 가지 편집 유형 구분:

  1. 기존에 인코딩된 사실 수정:

     - (e, r, o) → (e, r, $$o^*$$)
     - 주체 e와 관계 r에 대해 객체 o를 $$o^*$$로 업데이트.

  2. 새로운 사실 주입:
     - (e, r, $$o^*$$)
     - 모델에 의해 캡처되지 않은 정보.

- 추가 세부 사항:

  - 일대일 관계 (예: 생일)에서는 빈 객체를 채우는 것으로 간주될 수 있음:

    - (e, r, ∅) → (e, r, $$o^*$$)

  - 일대다 관계 (예: 형제, 직업)에서는 객체 집합을 확장:
    - (e, r, {o1, .., $$o_n$$}) → (e, r, {o1, .., on, $$o^*$$})

- 편집이 수정인지 주입인지 여부는 편집 전 모델에 정보가 캡처되었는지에 따라 달라짐.
- 특정 사실이 모델에 인코딩되었는지 평가하는 것은 다양한 입력 쿼리에 대해 객체 예측 테스트를 통해 수행됨.

---

# 3 Ripple Effects of Factual Edits

<img alt="image" src="https://github.com/user-attachments/assets/85f3e8a9-fb8a-47f4-8120-9db2baf2b05d" />

- **편집의 영향 평가**:

  - 주어진 편집 `(e, r, o)`가 새로운 상태 `(e, r, o')`로 전환될 때, 이와 관련된 사실 역시 변화할 것으로 예상됨.
  - 예시: 메시가 뛰는 팀이 변경되면 그가 속한 리그와 거주국에도 영향을 미칠 수 있음.
  - 지식 그래프 K에 대해, 편집 요청 δ에 따른 리플 이펙트를 정의: 모델이 K에서 암묵적으로 주입, 수정 또는 삭제해야 하는 트리플 집합 R(δ).

- **편집의 중대성**:
  - 다양한 수정이 서로 다른 크기의 리플 이펙트를 유발할 수 있음.
  - 예: 로마의 국가 변경은 많은 후속 변경을 유발하지만, 프린스의 형제 업데이트는 더 지역적인 영향을 미침.
  - 단일 편집 δ에 의해 영향을 받는 사실의 수를 그 심각도로 정의함.

### 3.1 평가 기준

- 리플 이펙트를 얼마나 잘 모델이 구현하는지를 평가하고자 함.
- 2-홉 거리 내 수정된 사실들에 집중하여 평가.
- 평가 기준:
  1. **논리적 일반화 (LG)**:
     - 지식 그래프에서의 관계가 특정 논리적 제약을 충족해야 함.
  2. **조합 가능성 I (CI)**:
     - 편집 후 대상 객체에 대한 정보와 함께 조합 가능성 확인.
  3. **조합 가능성 II (CII)**:
     - 편집된 사실을 다른 주제와 조합하여 확인.
  4. **주제 동의어 (SA)**:
     - 편집된 사실이 동의어로도 전파되는지 확인.
  5. **보존 (PV)**:
     - 다 대 일 관계에서 새로운 객체 추가가 다른 객체에 영향을 미치지 않도록 검증.
  6. **관계 특이성 (RS)**:
     - 편집으로 영향을 받지 않아야 할 다른 관계를 유지하는지 테스트.

### 3.2 관련 연구

- 여러 방법이 모델 내 사실 지식을 편집하기 위해 제안됨.
- 최신 연구에서는 후속 사실 삽입 방법이나 지식 보조 저장을 위한 어댑터 사용 등을 탐구함.
- **지식 편집 평가**의 필요성이 커지고 있으며, 기존 평가 기준(예: Zero-Shot Relation Extraction)과의 비교 연구가 진행 중임.

### 결론

- 편집의 리플 이펙트를 포괄적으로 평가하기 위한 RIPPLE EDITS 벤치마크를 제안함.
- 다른 편집 메소드들이 리플 이펙트를 일관되게 처리하는 데 어려움을 겪고 있지만, 간단한 인컨텍스트 편집 방법이 더 나은 성과를 보임을 보여줌.

---

# 4 The RIPPLE EDITS Benchmark

<img alt="image" src="https://github.com/user-attachments/assets/64bf8bcf-4060-4df6-944f-2c5a4bbf6059" />

- **데이터 생성 파이프라인 (§4.1)**

  - 사실적 편집 요청 및 평가를 위한 질의 생성 과정 설명
  - 템플릿 및 논리 규칙의 수동 작성을 포함하여 Nrel 기본 관계에 제한
  - **단계 1: 사실적 삼중항 수집**
    - WIKI DATA에서 최근에 수정된 사실(삼중항) 수집
    - 최근(RECENT), 무작위(RANDOM), 인기(POPULAR) 삼중항 유형으로 분류
  - **단계 2: 편집 생성**
    - RECENT: 새로운 사실을 삽입하기 위한 편집 요청 생성
    - RANDOM 및 POPULAR: 목표 객체를 샘플링하여 수정된 삼중항 생성
  - **단계 3: 평가 테스트 생성**
    - 편집 후 변화에 대한 평가 기준을 기반으로 테스트 질의 생성
    - 논리적 일반화, 관계적 특수성, 구성 가능성 등의 다양한 평가 기준 포함
  - **단계 4: 자연어 형식화**
    - 삼중항을 자연어 표현으로 변환하는 템플릿 사용

- **데이터 통계 (§4.2)**

  - RECENT, RANDOM, POPULAR 등 세 가지 범주에서 편집 요청 수집
  - RIPPLE EDITS 벤치마크로 공공 배포
  - 평균적으로 각 편집당 18-26개의 테스트 질의 생성

- **데이터 품질 (§4.3)**

  - 테스트 질의의 유효성을 검증하기 위해 200개 샘플을 수동 분석
  - 음성적 적합성(삼중항 의미적 정확성) 및 문법적 정확성을 검토
  - 100%의 쿼리가 의미적으로 적합하며 98.5%가 문법적으로 정확함을 확인

- **참고사항**
  - RIPPLE EDITS는 기존 벤치마크를 보완하는 목적으로 설계됨
  - 데이터 생성 프로세스의 반복적 편향을 감지할 수 있는 체계적인 접근 방식 강조

---

# 5 Experiments

- RIPPLE EDITS를 사용하여 최근의 지식 편집(Knowledge Editing, KE) 방법을 평가
  - 기존 벤치마크에서 상당한 진전을 보였으나, 현재 방법들은 편집 후 모델 지식의 일관된 변화를 도입하는 데 어려움이 있음
  - 편집된 사실에 조건을 둔 단순한 인-컨텍스트 편집 기준선이 더 나은 결과를 얻음

## 5.1 평가 설정

<img alt="image" src="https://github.com/user-attachments/assets/28b48801-0434-443d-9987-7acf948117ef" />

- **데이터**

  - 성공적인 편집 사례만 평가하고, 편집 전 모델이 올바르게 응답한 테스트 쿼리만 사용
  - 편집 방법 F와 모델 M에 대해, 특정 조건을 충족하는 편집 요청만 포함

- **편집 방법**

  - MEND, ROME, MEMIT 세 가지 KE 방법을 평가
    - MEND: 그라디언트를 수정하여 로컬 편집 생성
    - ROME: 트랜스포머 MLP 층의 가중치에 랭크-원 업데이트 수행
    - MEMIT: 여러 사실을 한 번에 편집할 수 있도록 조정된 ROME의 확장

- **기준선**

  - 인-컨텍스트 편집(ICE) 기준선 실험
    - 모델 매개변수를 변경하지 않고, 새로운 사실에 기반하여 생성

- **모델**

  - GPT-2 XL, GPT-J, LLaMA, GPT-NeoX 등 다양한 크기의 최근 오토 회귀 디코더 모델 사용

- **평가**
  - 각 모델-방법 조합을 별도로 평가하고, 성공적인 생성으로 판단 기준을 설정

## 5.2 결과

- 실험 결과:
  - 기존 KE 방법들은 편집으로 인한 파급 효과를 처리하는 데 어려움이 존재
  - ICE 기준선이 전반적으로 가장 좋은 결과를 보였으며, GPT-3에서 평균 성능이 가장 높았으나 LLaMA가 경쟁력 있는 성과 기록
- **모델 크기에 따른 성능**

  - 모델 크기가 클수록 편집 성능 향상

- **방법 간 성능 비교**
  - MEND가 Logical Generalization에서 outperform, Compositionality에서는 열세를 보임
- **데이터 분할에 따른 결과**
  - 편집된 사실의 인기도에 따라 성능이 달라짐
  - 인기 있는 엔티티가 더 큰 파급 효과를 나타냄

## 5.3 오류 분석

- ROME과 ICE의 효과 비교
  - 편집 후 실패한 쿼리를 세 가지 카테고리로 분류하여 분석
  - 대부분의 경우 사실 편집이 잘못된 변경을 초래함
  - GPT-3는 더 자주 응답을 거부하며, 이는 성능 저하의 원인으로 작용

이 실험들은 KE 방법들이 모델의 지식 수정에 있어 여전히 개선의 여지가 많다는 것을 시사하며, 향후 연구 방향에 대한 통찰을 제공함.

---

# 6 Conclusion and Discussion

- 지식 편집에서의 리플 이펙트 개념을 도입
  - 특정 사실을 편집하면 관련 사실도 추가로 업데이트해야 함
- 리플 이펙트를 평가하기 위한 기준 제안
- RIPPLE EDITS라는 진단 벤치마크 생성
  - KE 방법들이 리플 이펙트 처리를 얼마나 잘하는지 평가
- 주요 KE 방법들을 평가한 결과
  - 일관된 편집을 도입하지 못하는 경우가 많음
  - 향후 KE 방법 개발 시 리플 이펙트를 더욱 신중히 고려해야 함
- 간단한 인컨텍스트 편집 방법이 RIPPLE EDITS에서 가장 우수한 결과를 보임
  - 이러한 편집 접근 방식의 잠재력을 강조
- 현재 벤치마크는 가능성 있는 리플 편집의 일부만 다룸
  - 두 개 이상의 홉을 포함하는 리플 이펙트 및 다양한 편집의 그래프 구조 탐색 가능
- 단일 편집의 리플 이펙트에 초점을 맞추었으나,
  - 미래 작업에서는 여러 사실을 동시에 편집하는 경우 고려할 수 있음
- 모델이 리플 이펙트를 포착하는 사례 분석 및 변환기 아키텍처에서의 구현 방법론 논의 필요

## 한계

- 데이터 생성 파이프라인이 기존 지식 기반(WIKI DATA)에 의존
  - 불완전하거나 오래된 정보일 수 있음
- RIPPLE EDITS는 모든 가능한 리플 편집을 다루지 않음

  - 포괄적 평가 시 문제 발생 가능성
  - 도메인 특화 지식 기반의 경우 보완이 더욱 중요함
  - 해결책으로 LMs의 내부 지식 사용 가능성 제안

- 리플 이펙트에 초점을 맞추어 편집의 의도나 주제 특이성을 점검하지 않음
  - 기존 벤치마크에서 다루어져야 함
- 리플 편집 후에 다소 관련이 먼 기타 사실(예: 프랑스의 수도 편집이 폴란드 인구에 미치는 영향)에 대한 검증 부재
- 이러한 평가 구축이 어려운 이유는 고려해야 할 많은 사실이 존재하고, 특정 편집이 영향을 미쳐야 할 triplet을 자동으로 판단하기 어려움
