---
title: "[논문리뷰] You Don’t Know My Favorite Color- Preventing Dialogue Representations from Revealing Speakers’ Private Personas (NAACL 2022)"
date: 2025-03-20 00:00:00 +0900
categories:
  - Paper Review
tags:
  - NAACL 2022
  - Empathetic Dialogue Systems
---

요약: 대규모 사전 학습 언어 모델을 사용하는 소셜 챗봇에서 사용자 정보 유출 문제가 발생하며, 이를 해결하기 위해 챗봇의 숨겨진 상태로부터 사용자 인격 유출을 방지할 수 있는 효과적인 방어 목표를 제안하고, 실험을 통해 공격 정확도를 크게 감소시킬 수 있음을 입증하였다.

---

# 1 Introduction

- 소셜 챗봇은 사실적 질문에 답하기부터 감정적 교감을 제공하기까지 다양한 응용 분야에서 널리 활용되고 있음.
- 최근 대규모 사전훈련된 언어 모델의 진보로, 대규모 생성형 언어 모델(LM)에 기반한 챗봇 구축 시도가 있었음.
- 이러한 LM 기반 챗봇을 학습시키기 위해서는 개인 대화가 수집되지만, 대형 언어 모델은 학습 데이터를 기억하는 경향이 있으며 개인 데이터가 모델로부터 복구될 수 있음.
- 간단한 학습 목표에 대한 "과잉학습"은 학습 과제와 간접적으로 관련된 민감한 속성을 드러낼 수 있음.
- LM 기반 소셜 챗봇은 본질적으로 일반 LM의 개인정보 문제와 과잉학습 문제를 상속받음.
  
- 예를 들어, GPT-2를 인코더 및 디코더로 사용하는 LM 기반 소셜 챗봇의 경우, 각 발화의 학습된 표현이 상대방에게 노출되면, 상대방은 표현을 기반으로 페르소나 정보를 예측하는 분류기를 구축할 수 있음.
- 예제로부터, 14개의 발화 중 5개의 경우에서 공격자가 페르소나를 성공적으로 예측할 수 있었으며 이는 사용자가 페르소나 정보를 공개하지 않으려는 경우 문제가 될 수 있음.
- 실제 응용 프로그램에서 이러한 챗봇을 배포할 때, 모델이 개인정보를 누설하지 않도록 해야 함.

- LM 기반 소셜 챗봇의 개인정보 문제를 체계적으로 연구할 때 여러 가지 도전이 존재함.
  1. LM에 의해 얼마나 많은 개인정보가 노출되는지를 정량화할 수 있는 기존 데이터가 없음.
  2. 발화 수준의 표현을 공격하여 민감한 정보를 얻는 방법을 보여주는 기존 연구가 없음.
  3. 페르소나 추론 공격에 방어할 수 있는 LM 기반 챗봇이 없으며, 알려진 및 알려지지 않은 페르소나 속성을 보호하는 방법을 보여주는 연구가 없음.

- 이러한 도전을 해결하기 위해, 우리는 GPT-2를 챗봇으로 사용하고, PersonaChat 데이터셋에서 페르소나와 대응하는 발화를 정렬하여 데이터셋을 수집함.
- "과잉학습"이 LM 기반 챗봇에서 스피커의 페르소나를 드러낼 수 있음을 보여줌.
- 외부 다층 인식기(MLP) 공격자 모델을 구축하여 발화 수준의 임베딩에 대한 블랙박스 페르소나 추론 공격을 수행함.
- 챗봇의 매개 변수에 접근하지 않고 공격자 모델이 4,332개의 페르소나 중 37.59%의 정확도로 스피커의 페르소나를 추론할 수 있음.
- 이는 발화 수준의 임베딩이 스피커의 개인정보 속성을 드러낼 수 있는 잠재적 취약성을 가지고 있음을 시사함.
  
- 이러한 과잉학습 문제를 해결하기 위해, 방어 학습 전략을 GPT-2에 적용하여 블랙박스 공격을 방지함.
- KL 발산 손실(KL 손실)과 상호 정보 손실(MI 손실)을 추가 방어 목표로 GPT-2를 훈련하여 공격자의 페르소나 추론 정확도를 0.53%로 낮춤.
- 우리의 기여는 다음과 같이 요약될 수 있음:
  1. 우리가 아는 한 LM 기반 챗봇에 대한 페르소나 추론 공격을 최초로 공개 및 분석하고 이를 개인정보 문제로 간주함.
  2. 대화 표현이 해당 스피커의 페르소나를 누설하지 않도록 효과적인 방어 훈련 알고리즘 제안.
  3. 제안된 방어 메커니즘의 개인정보 및 유용성을 정량화하기 위한 광범위한 실험을 수행. 페르소나 유출 문제를 해결하면서도 제안된 훈련 알고리즘은 유용성에 거의 부정적인 영향을 미치지 않음.

---

# 2 Related Work

- 사적인 데이터를 학습한 언어 모델은 민감한 정보를 노출할 수 있는 프라이버시 위험에 직면한다.
- 이전 연구들은 주로 언어 모델의 입력과 출력에만 접근 가능한 블랙박스 공격 방식을 고려했다.
  - Carlini et al. (2021)은 GPT-2에 대한 블랙박스 모델 인버전 공격을 빔 검색을 사용한 설명적 프롬프트로 수행했다.
  - Lehman et al. (2021)은 전자 건강 기록을 사전 학습한 BERT를 빈 칸 채우기와 모델 탐색을 통해 개인 건강 정보를 복원했다.
  - Zanella-Béguelin et al. (2020)은 모델의 사전학습 및 파인튜닝 단계에 대한 블랙박스 접근을 통해 파인튜닝 데이터셋의 민감한 시퀀스를 추출할 수 있음을 보여주었다.
  - Malekzadeh et al. (2021)은 클라이언트-서버 분산 설정에서 정직하지만 호기심 많은(HBC) 분류기를 사용해 서버 측에서 민감한 속성 유출을 고려했다.

- 대화형 AI에서, 훈련 대화는 BooksCorpus나 Wikipedia 등 일반적인 언어모델링에서 사용되는 코퍼스보다 더 많은 개인적 속성을 포함할 가능성이 있다.
  - Tigunova et al. (2019)은 Hidden Attribute Model (HAM)을 제안하여 다양한 대화 데이터셋에서 발화자의 직업과 성별을 추출했다.
  - Wu et al. (2020)은 Attribute Extractor를 적용하여 발화자의 속성 삼중항을 유연하게 생성하고, 삼중항 기반으로 후속 작업을 제안했다.
  - Pan et al. (2020)은 언어 모델의 임베딩을 활용하여 입력의 숫자와 키워드를 복원했다. 이들의 작업은 단순한 데이터 복원 사례만 다루며 최적의 방어 성능을 얻기 위해 유틸리티 저하를 동반한다는 단점이 있다.
  
- 우리의 연구는 모델 입력에 고정된 패턴이나 규칙이 없다. 키워드 찾기나 숫자 복원이 아닌, 이러한 임베딩에서 더 복잡한 개인 속성을 추론하는 것을 목표로 한다. 게다가, 제안된 방어는 유틸리티에 거의 영향을 미치지 않는다.

---

# 3 Attacking on Language Models

- 블랙박스 방식으로 GPT-2에 대한 퍼소나 추론 공격과 방어 전략을 설명.
- **3.1 문제 정의**: 
  - GPT-2 기반 챗봇 $$f$$이 사적 대화 $$D$$를 학습.
  - 챗봇은 언어 모델링을 사용하여 학습됨:
    $$ Lf(u; θ_f) = - \sum_{i=1}^{|u|} \log(Pr(w_i | c, w_0, w_1, ..., w_{i-1})), $$
  - 대화 데이터셋 $$D_a$$을 가진 적대자는 특정 대화 $$U_i$$의 발화 목록 $$\{u_{i1}, u_{i2}, ..., u_{in_i}\}$$와 해당 발화의 민감한 퍼소나 목록 $$s_i$$를 보유.
  - 퍼소나 $$s_{kj}$$는 사전 정의된 사전과 일치 가능한 정수로, $$0 \leq s_{kj} \leq C - 1$$인 다수의 속성을 가짐.
  - 적대자의 목표는 발화의 임베딩 $$f(u)$$을 통해 발화의 퍼소나 $$s$$를 추론하는 것.
  
- **3.2 블랙박스 퍼소나 추론 공격**:
  - 이 공격은 지도 학습 분류 문제로 볼 수 있음.
  - 블랙박스 공격 설정에서, 적대자는 모델의 매개변수 $$\theta_f$$에 접근하거나 수정하지 않고 접근용 임베딩만 쿼리 가능.
  - 적대자는 외부 데이터를 이용해 공격 모델 $$A$$를 구축하고, 블랙박스 접근으로 챗봇에서 얻은 임베딩을 사용.
  - 퍼소나 예측기의 출력 $$A(f(u))$$는 퍼소나 속성에 대한 확률 분포로, 다음과 같은 손실 함수 $$L_A$$를 가짐:
    $$ L_A(u_{kj}, s_{kj}; \theta_A) = \text{CE}(A(f(u_{kj})), s_{kj}), $$
  - 잘 작동하는 퍼소나 예측기는 개인정보 위협을 야기할 수 있으며, 머신러닝 서비스(MLaaS)에서는 중간자 공격을 수행하는 데 사용될 수 있음.
  - 심지어 원시 데이터가 보호되고 전송 채널이 보안되더라도, 호기심 많은 서비스 제공자는 사용자 퍼소나 수집을 위해 공격자를 학습시킬 수 있음.

---

# 4 Defense Learning Strategies

- LM 훈련 목표는 챗봇의 유용성에만 집중하며, 이는 과학습 문제를 초래할 수 있음.
- 개인정보 추론 공격에 대한 최적의 프라이버시 보존 챗봇을 구현하려면, 공격자 모델의 확률 분포가 균등 분포에 가까워야 함.
- 이상적인 상태에서 공격자는 후향 추정 $$A(f(u))$$에서의 정확도를 랜덤 추측 이상의 수준으로 개선할 수 없어야 함.
- 프라이버시 제약은 챗봇의 생성 능력에 대한 저하가 최소화 되어야 함.
- **Section 4.1**: KL 손실은 퍼소나 예측기의 추정 분포를 평평하게 하도록 설계됨.
  - Kullback–Leibler 발산을 최소화하여 공격자에게 유용한 정보를 주지 않음.
  - 공격자 모델의 분포를 평평하게 하여 과학습 방지.
- **Section 4.2**: MI 손실은 숨겨진 상태 $$f(u)$$와 개인 퍼소나 속성 $$s$$ 사이의 상호 정보를 최소화.
  - 이러한 상호 정보의 상한은 KL 발산으로 표현하고, 이를 통해 방어적 훈련 목표 설정.
  - $$\min_{\theta_f} \max_{\Psi} E_{q(f(u))}[\log p_{\Psi}(s|f(u))]$$ 처럼 표현됩니다.
- **Section 4.3**: 전체 훈련 목표는 KL 손실, MI 손실, LM 손실로 구성됨.
  - MI 손실의 가짜 적대자 목표가 KL 손실과 상충하므로 더 많은 가중치를 KL 손실에 부여.
  - $$L = Lf + \lambda_1L_{kl} + \lambda_2L_{mi}$$로, $$\lambda_1 \geq 10\lambda_2$$.

이 훈련 전략은 블랙박스 공격 상황에서도 개인정보 추론 공격에서 챗봇의 퍼소나 과학습 문제를 완화하고자 함.

---

# 5 Experiments

- 이 섹션에서는 제안된 방어 학습 전략의 개인정보 보호와 유틸리티를 평가하기 위한 실험을 수행.
- **5.1 실험 설정**
  - 사용 데이터셋: GPT-2 기반 채팅봇 훈련을 위해 다이얼로GPT와 PersonaChat 데이터셋 사용.
  - 애노테이트된 데이터셋 Da는 Dialogue NLI 데이터셋을 기반으로 정렬된 (발화, 페르소나) 쌍으로 구성.
  - 데이터는 학습/검증/테스트 데이터셋 비율로 8:1:1로 재배치.
  - 공격자 모델: 2-layer 신경망과 교차 엔트로피 손실을 사용.
  - 평가 기준: 개인정보 보호는 페르소나 추론 정확도와 가중 F1 점수로 평가. 유틸리티는 BERTScore, Distinct, BLEU, perplexity로 평가.

- **5.2 개인정보 보호**
  - 방어 없는 공격: 다양한 시나리오에서의 공격 성능을 평가.
  - 방어된 LM에 대한 공격: KL 손실과 MI 손실을 사용하여 공격자의 성능 크게 감소.

- **5.3 정상 연구**
  - KL 손실과 MI 손실의 효과를 시험.
  - LM+KL과 LM+MI는 공격의 정확도를 감소시킴.
  - 높은 Max-Ratio는 개인정보 유출로 이어질 수 있음.

- **5.4 유틸리티 평가**
  - BERTScore, Distinct, BLEU, PPL을 사용하여 생성 성능 평가.
  - KL 손실과 MI 손실이 퍼플렉시티를 증가시키지만, 생성 품질에 큰 영향을 미치지 않음.

- **5.5 공격의 다양한 설정**
  - 불균형 데이터 분포에 대한 공격: 제안된 학습 손실이 보이지 않는 페르소나에 대한 공격도 방어 가능.
  - 적은 페르소나 레이블에 대한 공격: 작은 레이블 공간에서도 방어 성능 유지.

- **5.6 사례 연구**
  - 실제 대화 예시를 통해 방어된 모델이 효과적으로 개인정보 추론 공격을 방어함을 보여줌.

---

# 6 Conclusion

- LM 기반 챗봇이 화자의 페르소나를 드러내는 경향이 있음을 보여주고, 이를 방지하기 위한 효과적인 방어 목표를 제안하였다.
- 다른 연구들과 달리, 우리의 방어 학습 전략은 LM 기반 챗봇의 강력한 생성 능력을 저하시키지 않는다.
- 광범위한 실험을 통해 프라이버시와 유용성을 평가하였다.
- 다양한 설정 하에서 블랙박스 페르소나 추론 공격을 수행하여 제안된 방어 학습 전략의 견고함을 입증하였다.
- 자동화된 메트릭을 사용하여 제안된 방어 학습 전략이 유용성을 유지함을 보여주었다.
- 미래 연구에서는 공격자 모델의 분포를 평탄화하는 작업을 제안한다.

---

# 7 Ethical Considerations

- 모든 저자들은 ACM 윤리 강령을 준수하며 행동 강령을 존중합니다.
- 본 연구는 본질적으로 프라이버시 관련 속성에 대한 블랙박스 공격을 고려하며, 챗봇이 개인 정보를 과도하게 학습하지 않도록 하는 효과적인 학습 전략을 제안합니다.
  
### 데이터셋
- 데이터셋 수집 과정에서, 모든 대화와 개인 정보는 PersonaChat 및 DNLI와 같은 공개된 데이터셋에서 수집되었습니다.
- 모든 대화 참여자는 익명 처리되었으며, 식별 가능한 개인 정보는 포함되지 않았습니다.

### 모델
- LM(언어 모델) 기반 챗봇의 학습을 위해 표준 방법을 따릅니다.
- 현재 언어 모델 내의 편향 문제를 인지하고 있으며, 향후 더 공정한 언어 모델이 등장하면 방어 전략을 확장할 계획입니다.