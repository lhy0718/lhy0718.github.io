---
title: "[논문리뷰] Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis (AAAI 2024)"
date: 2025-05-25 19:58:08 +0900
categories:
  - Paper Review
tags:
  - AAAI 2024
  - LLM in Game Theory
---

본 연구는 게임 이론의 핵심 원리인 합리성을 기준으로 대형 언어 모델(LLM)의 행동을 분석한 결과, 최신 LLM도 인간과 상당한 차이를 보여 사회과학 게임 실험에 LLM을 도입할 때 신중한 접근이 필요함을 제시한다.

---

# Introduction

- 게임 이론(Game theory)은 인간 행동을 평가하기 위한 수학적 이론으로, 현실 상황을 매우 추상적으로 표현하여 사회과학(경제학, 심리학, 사회학 등) 분야에서 표준 분석 도구로 자리 잡음 (Roughgarden 2010; Dufwenberg 2011; Osborne and Rubinstein 1995; Charness and Rabin 2002).

- 대규모 언어 모델(LLMs)의 급속한 발전으로 인해 LLMs의 행동이 인간과 높은 정합성을 보임 (Ouyang et al. 2022; OpenAI 2023; Bai et al. 2022).

- 따라서 많은 연구자들이 LLMs를 인간과 유사한 연구 대상으로 간주하고, 게임 실험을 통해 사회과학 내 LLMs의 전문성을 분석하고 있음 (Dillion et al. 2023; Chen et al. 2023; Akata et al. 2023).

- 기존 연구들은 주로 경험적 관점에서 LLMs와 게임 이론을 도구로 활용할 뿐, 게임 이론 맥락에서 LLMs를 체계적으로 분석하지 않아 LLMs의 근본적인 역할과 한계가 불명확함.

- 중요한 질문들:
  - 어떤 연구 대상 역할을 LLMs가 수행하지 못하는가?
  - 어떤 유형의 게임에서 LLMs의 성능이 떨어지는가?
  - 어떤 게임 과정에 LLMs가 더 적합한가?

- 게임 이론에서 플레이어(연구 대상)의 행동은, 그림 1의 개요와 같이 선호도 $$P$$와 지각된 게임 정보 $$I$$ (게임 규칙과 과거 기록 등)에 근거해 행동 $$a \in A$$를 선택하여 게임에서 승리하는 과정임.

- 게임 이론의 근본 원리인 합리성(rationality)은 플레이어 행동의 평가 기준이며, 합리적인 플레이어는 세 가지 특징을 가짐(Zagare 1984; Osborne and Rubinstein 1995):
  - 게임에 대한 명확한 욕구(desire)를 구축한다.
  - 게임 내 불확실성에 관한 믿음(belief)을 다듬는다.
  - 욕구와 믿음을 바탕으로 최적의 행동을 취한다.

- 욕구 $$D(\cdot)$$는 플레이어의 추상적 선호도 $$P$$에 따라 게임 결과에 대한 구체적 의견을 나타냄.

- 믿음 $$\Omega_I$$는 게임 정보 $$I$$로부터 다듬어진 주관적 불확실성 판단(예: 상대 행위)임.

- 최적 행동 $$a \in A$$는 욕구 $$D(\cdot)$$와 믿음 $$\Omega_I$$를 조합하여 추론해야 함.

- 본 연구는 이 세 가지 합리적 플레이어 특성을 기준으로 LLMs를 체계적으로 분석하는 관점을 제안함.

- 선택한 세 가지 고전 게임:
  - 독재자 게임(dictator game): LLMs가 명확한 욕구 구축 능력 보유, 하지만 비정형 선호도 부여 시 수학적 능력 저하 및 선호 이해 실패 관찰.
  - 가위바위보(Rock-Paper-Scissors): 단순 패턴으로부터 믿음을 다듬는 능력 부족, 복잡한 믿음 추론 게임 수행에 부정적 시각 존재. 하지만 GPT-4는 특정 패턴에서 인간과 유사한 뛰어난 성능을 보이며, 게임 정보가 늘어남에 따라 믿음 다듬기에 자신감 증가.
  - 링-네트워크 게임(ring-network game): LLMs가 플레이어 행동을 자율적으로 따르지 못함. 게임 과정에서 행위를 명시적으로 분해하면 최적 행동 능력은 향상되나, 믿음 다듬기 간과 혹은 수정 현상은 여전히 존재.

- 요약: 본 연구는 게임 이론 맥락에서 LLMs 능력의 한계를 세 가지 관점에서 체계적으로 탐구하며, 이를 통해 LLMs가 사회과학 분야에 원활히 도입될 기반을 마련하고자 함.

---

# Related Work

- **사회과학에서의 LLM 활용**
  - LLM은 인간 행동과 높은 정렬도를 보이는 것이 큰 장점임 (Bai et al. 2022; Ouyang et al. 2022).
  - 비용과 효율성 측면에서 많은 사회과학 연구에서 LLM을 인간 연구 대상 대체로 활용하기 시작함 (Aher, Arriaga, and Kalai 2022; Argyle et al. 2023; Bybee 2023; Park et al. 2022).
  - 예시:
    - 사회학의 공정성 및 프레이밍 효과 탐구를 위해 고전 게임 실험에 LLM 도입 (Horton 2023).
    - 소비자 행동 연구에서 LLM의 행동이 경제 이론과 일치 (하향 수요곡선, 소득의 한계 효용 감소, 상태 의존성) (Brand, Israeli, and Ngwe 2023).
    - 금융 연구에서 예산 배분 시나리오에서 LLM의 결정은 인간보다 높은 합리성 점수를 받음 (Chen et al. 2023).
    - 심리학 실험에서 LLM 행동이 주류 사회적 가치와 일치 (Dillion et al. 2023).
  - 다만, 사회과학 분야에서 LLM의 능력 한계에 대한 체계적인 분석은 아직 부족함.

- **게임 이론**
  - 게임 이론은 불확실성 하에서 합리적 플레이어 행동을 분석하고 예측하는 수학적 이론임 (Roughgarden 2010; Dufwenberg 2011).
  - 원래 경제학에서 개발되었으며, 시장 경쟁, 경매 메커니즘, 가격 전략 등 다양한 경제 행동을 게임 실험으로 모델링함 (Ichiishi 2014; Samuelson 2016).
  - 과학 이론 간 융합으로 정치학, 사회학, 심리학 등 사회과학 다른 분야에도 적용됨 (Shubik 1982; Larson 2021; Dillion et al. 2023).
  - LLM의 게임 이론 수행 연구의 강점:
    - 강한 조작 가능성: 게임 이론 실험 설계가 비교적 단순함.
    - 강한 분석 가능성: 게임 이론은 실험 결과에 대해 포괄적 이론적 지원 제공.
    - 강한 일반화 가능성: 사회과학 분야 많은 현상을 고차원적으로 추상화함.

---

# Preliminaries of Game Theory

- 게임 이론의 핵심은 불확실성 조건 하에서 플레이어가 최적의 행동을 취하도록 유도하는 것임 (Roughgarden 2010).
- 일반적으로 게임은 다섯 가지 요소로 모델링됨:
  - 게임 정보 $$ I $$: 게임 규칙, 이력 기록 등
  - 플레이어가 취할 수 있는 행동 집합 $$ A $$
  - 행동의 가능한 결과 집합 $$ C $$
  - 결과 함수 $$ g: A \to C $$: 각 행동에 결과를 대응
  - 욕구 함수 $$ D_c: C \to \mathbb{R} $$: 플레이어의 선호도 $$ P $$에 의해 결정됨  
    - 임의의 $$ c_1, c_2 \in C $$에 대해, 플레이어는 $$ D_c(c_1) > D_c(c_2) $$일 때 $$ c_1 $$을 선호함

- 게임 과정의 불확실성을 제거하기 위해 대부분의 게임 연구에서 신념 이론을 사용함 (Morgenstern 1945; Lindley and Savage 1955).
  - 합리적인 플레이어는 게임 정보 $$ I $$를 바탕으로 모든 불확실성에 대해 주관적 확률 분포를 추정하며, 이를 플레이어의 신념이라 함 (Osborne and Rubinstein 1995).
  - 구체적으로, 플레이어는 신념 $$ \Omega_I $$, 신념에 대한 확률 분포 $$ p(\Omega_I) $$, 결과 함수 $$ g: A \times \Omega_I \to C $$를 가짐.

- 플레이어는 다음 식을 최대화하여 최적 전략 $$ \pi^*(a|I) $$를 찾음:
  $$
  \pi^*(a|I) = \arg\max_{a \in A} \mathbb{E}_{\omega \sim p(\Omega_I)} [D(a, \omega)],
  $$
  여기서 $$ D(\cdot) $$는 $$ D_c \circ g(\cdot) $$의 간단한 표현임.

- 위 식은 합리적 플레이어의 세 가지 특징을 명시적으로 나타냄:
  - 명확한 욕구를 가짐: 욕구 함수 $$ D(\cdot) $$ 구성
  - 불확실성에 대한 신념 갈고 닦음: 신념 확률 분포에서 샘플링 $$ \omega \sim p(\Omega_I) $$
  - 욕구를 최대화하는 최적 행동 선택: $$ \arg\max_{a \in A} D(a) $$

- 본 연구는 불확실성이 오직 상대방의 행동에서만 발생한다고 가정함.

---

# LLMs in Game Theory

- 본 절에서는 게임 이론 내에서 LLMs가 얼마나 합리적 플레이어의 세 가지 특성(명확한 욕구 형성, 신념 정제, 최적 행동 취하기)을 달성할 수 있는지 고전적인 세 게임(독재자 게임, 가위바위보, 링 네트워크 게임)을 통해 체계적으로 분석함.
- 분석 대상 LLMs: OpenAI의 text-davinci-003(GPT-3), gpt-3.5-turbo(GPT-3.5), gpt-4(GPT-4)

---

## 1. LLMs가 명확한 욕구(Build Clear Desire)를 형성할 수 있는가?

- 게임 이론의 전제: 플레이어는 결과 집합 $$C$$에 대해 추상적 선호 $$P$$를 갖고, 합리적 플레이어는 이를 바탕으로 욕구 함수 $$D(\cdot)$$를 생성하여 각 결과 $$c \in C$$에 대해 욕구를 측정해야 함.
- LLM은 텍스트 프롬프트를 통해 선호를 부여받기 때문에 욕구를 합리적으로 구축하는지 확인 필요.

### 독재자 게임 (Dictator Game)

- 두 명의 플레이어: 독재자와 수령자
- 독재자는 두 분배 옵션 중 하나 선택, 수령자는 독재자 선택 수락
- 대표적 선호 유형:
  - 평등(EQ): 불평등 혐오
  - 공통 이익(CI): 공동 이익 극대화
  - 자기 이익(SI): 자신의 이익 극대화
  - 이타주의(AL): 상대 이익 극대화

- 분배 옵션 설정 (독재자 수령자 순):
  - EQ: (300, 300)
  - CI: (400, 300)
  - AL: (100, 500)
  - SI: (500, 100)

- 독재자의 최적 전략:
  $$
  \pi^*(a|I) = \arg\max_{a \in \{X,Y\}} \{D(X, \omega_I), D(Y, \omega_I)\}
  $$

- 실험 결과 (정확도):

| LLM | EQ | CI | SI | AL |
|-----|----|----|----|----|
| GPT-3 | 1.0 | 0.4 | 1.0 | 0.0 |
| GPT-3.5 | 1.0 | 1.0 | 1.0 | 1.0 (0.6) |
| GPT-4 | 1.0 | 1.0 | 1.0 | 1.0 |

- 주요 발견
  - 일반 선호(EQ, SI)에는 모두 정확한 욕구 형성.
  - 비일반 선호(CI, AL)에서는 오류 다수 발생, 특히 GPT-3은 AL 선호 심각하게 오인.
  - GPT-3의 오류는 숫자 혼동, GPT-3.5는 선호 혼동에서 기인.
  - GPT-4는 거의 완벽한 인간 수준 수행.

- **인사이트:** LLM은 텍스트 프롬프트로부터 기본적인 욕구를 형성 가능하지만, 비일반 선호에 대한 욕구 구축은 취약함. 비일반 선호에 대한 명확하고 상세한 설명이 도움될 것.

---

## 2. LLMs가 신념(Belief)을 정제(Refine)할 수 있는가?

- 합리적 플레이어는 불확실성에 대해 신념 $$\Omega_I$$를 정보 $$I$$로부터 정제해야 함.
- 정제는 표면 정보에서 심층 통찰 추출 과정을 의미.
- 신념 정제는 인간에게도 어려운 과제임.

### 가위바위보(Rock-Paper-Scissors, R-P-S)

- 두 명의 플레이어가 동시에 선택하는 제로섬 게임, 규칙은 단순함.
- 신념 정제를 잘 수행하면 상대 비무작위 행동 패턴을 학습해 유리함 획득 가능.

- 라운드 $$i$$에서 내 행동과 상대 행동은 각각 $$a_i^m, a_i^o$$
- 과거 기록 $$\{a^o_{<t}, a^m_{<t}\}$$이 신념 정제에 사용됨.
- 최적 전략:
  $$
  \pi^*(a_t^m|I) = \arg\max_{a_t^m \in A} \mathbb{E}_{a_t^o \sim p(\Omega^{\{s<t\}}_{o,a^m_{<t}})}[D(a_t^o, a_t^m)]
  $$

- 상대 행동 패턴 4가지 설정(Table 2 참조):
  - $$a_t^o = C$$: 상대 행동 고정
  - $$a_t^o = f(a_{<t}^o)$$: 상대 행동 과거 패턴에 따른 루프 (loop-2, loop-3)
  - $$a_t^o = f(a_{<t}^m)$$: 상대가 내 과거 행동을 복사 또는 카운터
  - $$a_t^o \sim p(P)$$: 상대가 선호 확률에 따라 행동 표본 추출

- 보상 설정: 승리 2점, 무승부 1점, 패배 0점
- 10라운드 후 평균 점수 측정, 각 실험 10회 반복

- 주요 결과 (평균 점수):
  - GPT-3: 고정 패턴에서 무작위 결과와 유사 → 신념 정제 능력 부족
  - GPT-3.5: 지속적 향상, 어느 정도 패턴 캡처 가능
  - GPT-4: 약 3라운드 후 완벽 근접 점수, 특히 루프-3 패턴에서 뛰어남
  - 복사/카운터 패턴과 표본 추출 패턴에서는 모든 LLM 성능 낮음

- GPT-3.5는 상대가 루프를 따른다고 분석하면서도 확신 못 했음
- GPT-4는 점차 확신을 가지며 정확히 패턴 인지

- **인사이트:** 현 시점 LLM의 신념 정제 능력은 한계가 크고 복잡한 패턴인식에는 미흡함. GPT-4 정도는 일부 반복 패턴에서 가능성을 보임. 복잡 신념이 필요한 게임에서 LLM 도입은 신중해야 함.

---

## 3. LLMs가 최적 행동(Optimal Action)을 취할 수 있는가?

- 합리적 플레이어의 궁극 목표는 욕구 $$D(\cdot)$$와 신념 $$\Omega_I$$를 기반으로 최적 행동 선택.
- LLM이 욕구와 신념을 결합하는 방식 다양, 어떤 형태가 더 적합한지 불분명.

### 링 네트워크 게임 (Ring-Network Game)

- 2인 2행동 게임
- 각 플레이어는 상대 행동 정보에 따라 행동 $$a_m \in \{U,V\}$$와 $$a_o \in \{X,Y\}$$ 선택
- 페이오프 행렬은 Fig. 5(a)와 같음
- 상대는 언제나 행동 $$Y$$를 선택하는 것으로 가정(최적 신념 정제 필요)

- 최적 전략:
  $$
  \pi^*(a_m|I) = \arg\max_{a_m \in \{U,V\}} [p(a_o|M) \cdot D_m(a_m|a_o, M)]
  $$

- 세 가지 행동 양식 (신념과 행동 연결 형태) 실험:
  - **암묵적 신념 → 행동 (Implicit Belief → Take Action)**: 페이오프만 주고 LLM이 내부적으로 신념/욕구 결합해 행동 선택
  - **명시적 신념 → 행동 (Explicit Belief → Take Action)**: 우선 신념(상대 행동) 명시적 분석 → 이후 행동 결정
  - **주어진 신념 → 행동 (Given Belief → Take Action)**: 상대 행동 신념을 명시적으로 제공받고 행동 결정만 수행

- 페이오프 함수 변형 4가지 (Fig.6)로 난이도 조절, 행동 이름 편향 위해 페이오프 스왑 후 10회 반복 실험

- 결과 (행동 정확도 및 신념 정확도, Table 3):
  - GPT-3 전반적으로 저조
  - GPT-3.5, GPT-4 모두 암묵적 신념→행동 형태에서 거의 실패 (GPT-4 심각)
  - 명시적 신념→행동에서는 신념 정제 정확도는 매우 높으나 행동 취하는 성능은 다소 저조(GPT-4~70%)
  - 주어진 신념→행동에서는 GPT-4가 거의 완벽(100%), GPT-3.5도 80% 이상

- 실패 원인 사례 분석 (Fig.7):
  - 신념 무시: LLM이 예상 페이오프와 혼동하여 $$p(a_m|a_o, M) \to p(a_m|M)$$로 판단 (주로 GPT-3.5)
  - 신념 수정: 신념에 확신을 못하고 변경 $$p(a_m|a_o, M) \to p(a_m|\hat{a}_o, M)$$ (GPT-4에서 주로 발생)

- **인사이트:**
  - LLM은 자율적으로 인간과 같은 게임 행동 과정을 모방하지 못함.
  - 명시적 사고 과정 분리가 LLM 최적 행동 수행에 도움됨.
  - 그러나 명시적 게임 과정 내에서도 신념을 무시하거나 변경하는 오류 빈발.
  - 해결책으로 명시적 신념을 대화 내에서 완전한 '주어진 신념'으로 변환해 제공하는 방법 권장.

---

# 요약

- LLMs는 텍스트 프롬프트를 바탕으로 기본적 욕구 형성이 가능하나, 비일반적 선호에 약함.
- 신념 정제 능력은 아직 불완전하며, 반복적 단순 패턴 정도만 효과적으로 학습 가능.
- 최적 행동은 명시적으로 신념과 욕구를 단계별 분리할 때 성능이 향상됨.
- LLMs가 인간과 동일한 합리적 게임 과정을 따르지 못하는 한계를 인지하고, 명확한 신념 전달이 중요하다.

---

# Conclusion

- LLMs의 급속한 발전으로 인해 LLMs가 궁극적으로 인간 사회의 모든 영역에 통합될 것으로 예상되므로, 다양한 분야에서 LLMs의 능력 한계를 체계적으로 분석하는 것이 시급함.
- 본 연구에서는 사회과학의 중요한 분야인 게임 이론에서 LLMs를 체계적으로 분석하고자 하였음.
- 실험을 통해 LLMs가 합리적인 플레이어로서 어느 정도 역할을 수행할 수 있는지 세 가지 측면에서 평가하였고, 게임 이론에서 LLMs의 약점도 발견함.
- 본 연구는 게임 이론 맥락에서 LLMs를 분석하려는 초기 시도로서 다음과 같은 한계가 존재함:
  - 선택한 게임의 난이도가 상대적으로 낮아 실제 게임 시나리오와 충분히 근접하지 않음.
  - LLMs의 능력을 분석하는 관점이 한정적이며, 합리성 원리만 고려함.
  - 게임 실험 분석 과정이 다소 거칠고, 더 많은 비교 및 소거 실험이 부족함.
- 결론적으로, 게임 이론 맥락에서 LLMs에 관한 연구는 아직 매우 초기 단계이며, 많은 탐색적 연구가 필요함.