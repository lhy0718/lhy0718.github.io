---
title: "[논문리뷰] LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments (ACL 2024)"
date: 2025-04-07 11:00:00 +0900
categories:
  - Paper Review
tags:
  - ACL 2024
  - Multi-agent
  - Benchmark
---

대규모 언어 모델(LLM)의 새로운 벤치마크인 LLMArena를 소개하며, 이는 다중 에이전트 동적 환경에서 LLM의 다양한 능력을 평가할 수 있도록 설계되었다. 연구 결과, LLM은 완전한 자율 에이전트로 발전하는 데 있어 여전히 많은 개선이 필요하다고 강조한다.

---

# 1 Introduction

- 최근 대규모 언어 모델(LLMs)의 발전이 자연어 처리(NLP) 분야의 진전을 크게 촉진
  - LLM은 다양한 다운스트림 작업에서 우수한 제로샷 능력을 발휘하며 특히 복잡한 텍스트 이해 및 생성에 강함
  - 특정 작업에 대한 훈련 없이도 훌륭한 적응성 보유

- LLM은 최소한의 유도 프롬프트를 사용하여 생소한 상황을 효과적으로 분석하고 대응 가능
  - 이러한 특성으로 인해 LLM을 독립적인 에이전트로 간주하고, 소프트웨어 개발 및 지식 통합과 같은 복잡한 실제 작업에서 자동화된 지원 제공 가능

- LLM의 에이전트로서의 능력을 파악하기 위해 연구자들은 특정 환경에서 LLM의 성능을 평가하기 위해 다양한 시나리오 개발 및 활용에 집중
  - 예: AgentBench는 운영체제와 데이터베이스 등의 8개의 환경을 소개하여 코드 생성 및 측면 사고 능력을 평가

- 기존 벤치마크의 한계:
  - AgentBench와 같은 정적 데이터셋 사용으로 인한 데이터 유출 및 오버피팅 문제 발생 가능
  - 단일 에이전트에만 초점을 맞춘 평가 시스템의 한계

- 이를 보완하기 위해 LLMArena 제안
  - 동적 다중 에이전트 상호작용을 위한 특별한 평가 기준
  - 7종의 다중 에이전트 게임 환경을 포함

- LLMArena의 사용 예:
  - 각 게임 환경에서 에이전트의 공간적 이해, 전략적 계획, 수리적 추론, 위험 평가, 의사소통, 상대 모델링 및 팀 협력 능력 종합 평가

- LLM의 성능을 보다 정확하게 평가하기 위해 TrueSkill™ 점수 시스템 적용
  - 승률 외에도 에이전트 간의 상대적 기술 수준 평가 가능

- 14종의 다양한 크기와 유형의 LLM에 대한 광범위한 실험 및 인간 평가 수행
  - 다중 에이전트 환경 내에서 LLM의 팀워크 및 상대 모델링 능력에서 유의미한 개선 가능성 확인

- LLMArena가 향후 연구에 영감을 줄 것으로 기대
  - LLM의 핵심 능력을 강화하여 실제 응용에서의 널리 사용 촉진 목표

---

# 2 Benchmark Detail

---

# 2.1 Benchmark Overview

- LLMArena 내의 7개의 독특한 환경을 나타내는 그림 1
- 각 환경에서 LLM은 특정 능력 조합을 활용하여 과제를 효과적으로 해결해야 함
- 예시: 
  - Undercover 환경에서는 다음과 같은 다면적인 기술 세트가 필요함:
    - 상대 모델링
    - 효과적인 커뮤니케이션
    - 팀 협업
- 이러한 능력 중 하나라도 결여될 경우, 해당 환경에서 궁극적인 실패를 초래할 수 있음

---

# 2.2 Benchmark Construction

- LLMArena의 사용자 친화성과 강력한 확장성을 보장하기 위해 PettingZoo를 기반으로 환경 개발
- 이 접근 방식으로 후속 연구자들이 새로운 환경을 쉽게 통합 가능
  - PettingZoo 인터페이스 사양에 맞춘 환경 생성
  - LLMArena에 새로운 환경 추가하여 LLM의 다양한 능력 평가 가능
- LLMArena의 모든 환경은 시스템 프롬프트 제공
  - 게임 규칙 및 LLM 에이전트를 위한 플레이 가이드 포함
- 프롬프트 템플릿에는 다음 정보 포함:
  - 현재 게임 상태
  - 역사적 데이터
  - 다양한 선택적 행동
- 이를 통해 LLM 에이전트에게 보다 구조적이고 정보가 풍부한 게임 경험 제공
- 모든 프롬프트는 부록 A에 수록됨

---

# 2.3 Metrics

- LLMArena에서 사용되는 평가 지표를 설명.
- 이전 연구와의 비교 분석.
  
- **TrueSkill™**
  - Herbrich et al. (2006)에 의해 개발된 점수 시스템.
  - 경쟁 게임에서 여러 에이전트의 기술 수준 평가.
  - 이전 연구에서 사용된 정적이거나 상대방과 무관한 지표(승률, 완료율)와 비교하여 더 세밀한 평가 제공.
  - 단순 승패를 넘어서 게임의 질 및 플레이어 간 기술 차이 고려.
  - 높은 기술의 상대를 이길 경우 더 많은 점수 획득.
  - 단순 승률 계산과 달리, 상대방의 기술 수준을 반영하여 에이전트의 진정한 역량을 평가.

- **보상**
  - 현재 많은 연구에서 전문가 LLM을 활용해 다른 LLM의 능력을 평가.
  - 그러나 이 방법은 종종 해석 가능성이 부족함.
  - LLMArena는 강화 학습의 보상 개념을 활용하여 에이전트의 행동 품질을 평가.
  - 이 방법은 LLM의 능력을 정량적이고 해석 가능하게 분석할 수 있는 격차를 해소.

---

# 2.4 Environment Settings

- **TicTacToe**
  - 3×3 격자에서 두 개의 LLM 에이전트가 번갈아가며 표시를 놓음.
  - 세 개의 마크가 수평/수직/대각선으로 정렬되면 승리.
  - 모든 셀이 채워지면 무승부.
  - **주요 도전 과제**: 전략적 계획 및 공간적 추론.
  - **평가 기준**: TrueSkill™ 등급.

- **ConnectFour**
  - 6×7 체스판에서 두 개의 LLM 에이전트가 번갈아 가며 선택.
  - 한 열의 가장 낮은 빈 공간에 토큰을 놓음.
  - 네 개의 마크가 정렬되면 승리.
  - **주요 도전 과제**: 전략적 사고 및 공간적 추론.
  - **평가 기준**: TrueSkill™ 등급.

- **Texas Hold’em**
  - 두 개의 LLM 에이전트가 플레이하는 카드 게임.
  - 각 플레이어에게 2장의 개인 카드와 5장의 커뮤니티 카드가 배포됨.
  - 4개의 베팅 라운드에서 협상 진행.
  - **주요 도전 과제**: 수치적 추론, 상대 모델링, 위험 평가.
  - **평가 기준**: TrueSkill™ 등급.

- **Undercover**
  - 두 그룹의 플레이어가 유사하지만 다른 단어를 받는 파티 게임.
  - 커뮤니케이션 및 투표 단계로 나눠짐.
  - 5개의 LLM 에이전트가 플레이; 감시자를 식별해야 함.
  - **주요 도전 과제**: 커뮤니케이션, 상대 모델링, 팀 협력.
  - **평가 기준**: 감시자로 플레이할 때의 승률.

- **Bargain**
  - 두 개의 LLM 에이전트가 아이템 분배 전략을 세워야 하는 반협력 게임.
  - 각 아이템은 두 에이전트에게 고유한 가치 부여.
  - 합의에 도달하면 최대 총 가치를 가진 에이전트가 승리.
  - **주요 도전 과제**: 수치적 추론, 커뮤니케이션, 상대 모델링.
  - **평가 기준**: TrueSkill™ 등급.

- **Bid**
  - 첫 번째 가격 봉인 입찰 경매에 기반한 환경.
  - 두 개의 LLM 에이전트가 각기 다른 내적 가치를 가진 아이템에 대해 입찰함.
  - **주요 도전 과제**: 수치적 추론, 상대 모델링.
  - **평가 기준**: 평균 보상.

- **Hanabi**
  - 협력적인 카드 게임으로, 두 개의 LLM 에이전트가 서로의 카드만 볼 수 있음.
  - 정보 토큰을 사용해 카드를 드러내고, 버리고, 플레이하여 불꽃을 형성.
  - **주요 도전 과제**: 팀 협력, 전략적 계획, 수치적 추론.
  - **평가 기준**: 다양한 LLM 팀의 평균 보상.

---

# 3.1 Experiments Setup

- 실험은 폐쇄형 소스 GPT 계열 LLM의 API를 직접 호출하여 수행
- 오픈 소스 LLM은 로컬에 배포 후 OpenAI API 호출 형태로 캡슐화
- 재현 가능성을 보장하기 위해 모든 LLM의 온도를 0으로 설정
- Undercover를 제외한 모든 환경에서 LLMArena 실행:
  - 모델 간 TrueSkill™ 등급 수렴 시점까지 진행
  - 각 모델 및 환경에서 50게임 이상 플레이
- Undercover 환경에서:
  - 각 LLM을 잠입 요원으로 하여 100게임 플레이
  - 4명의 GPT-3.5-turbo 비잠입 요원과 함께 플레이
- 초기 설정:
  - 모든 LLM 에이전트의 TrueSkill™ 점수: 25.0
  - 분산(variance): 8.33

---

# 3.2 Main Result

- 표 1에서는 LLMArena의 7개 환경에서 14개의 다양한 LLM의 상대 정규화 점수를 제공합니다.
- 정규화 이전의 원본 데이터는 부록 C에서 확인할 수 있습니다.
- 주요 결론:
  - 모델 파라미터의 크기가 증가함에 따라 LLM의 능력이 개선됨.
    - 예: 약 70B 파라미터 모델들의 평균 성능은 82.87.
    - 약 30B와 10B 파라미터 모델들은 각각 80.68과 71.05의 평균 성능을 기록.
    - 10B에서 30B로의 모델 크기 증가에 따른 성능 향상 (+9.63)은 30B에서 70B로의 증가 (+2.19)보다 더 두드러짐.
  
  - LLM의 크기가 큰 모델들이 같은 계열의 작은 모델들보다 높은 성능을 보이는 경향이 있으나, 특정 환경에서는 예외가 존재.
    - 예: Undercover 환경에서 Qwen-72B는 Qwen-14B보다 25.93 낮은 점수를 기록, 이 경향에서 벗어남.
  
  - Bid와 Hanabi 환경에서의 LLM 성능은 모든 환경의 평균에 비해 상당히 낮음.
    - Bid에서 17.72, Hanabi에서 10.88 점수 부족.
    - 규모가 작은 LLM(∼10B)는 이 환경들에서 평균보다 각각 31.71 및 15.45 더 낮은 점수.
    - 이는 숫자적 추론, 상대 모델링 및 팀 협업이 LLM에게 여전히 큰 도전 과제임을 강조.

  - GPT-4와 다른 모델들 사이의 성능 격차가 상당함.
    - GPT-4는 평가된 모든 작업에서 SOTA 성능을 달성하며, 다른 모델들보다 평균 16.76의 차이로 우수함.

---

# 4 Analysis

- **전반적인 실험 목표**
  - LLM을 공간 이해, 전략 계획, 수치 추론, 위험 평가, 커뮤니케이션, 상대 모델링, 팀 협력 등 7가지 주요 관점에서 평가.
  
- **공간 이해**
  - 100회의 자기 플레이 게임을 통해 LLM의 승률과 환상 발생 빈도 기록.
  - 위치 정보 없이 게임 진행 시 환상 발생 확률 59.5% 증가, 승률은 평균 38.3% 감소.
  - 2D 체스판의 수직 차원 이해에 제한이 있으며, 1D 문자열로 변환 과정에서 정보 손실 발생.
  - TicTacToe에서 성능 저하가 더 두드러짐.

- **전략 계획**
  - ConnectFour 환경의 보드 가치 함수 정의:
    $$V(s) = 10 \times (My4(s) - Oppo4(s)) + 5 \times (My3(s) - Oppo3(s)) + 2 \times (My2(s) - Oppo2(s))$$
  - 매개변수가 많은 LLM이 전략적 계획에서 우수함을 보임.
  - 인간 플레이어가 모든 게임에서 승리하며, GPT-4는 보상 극대화를 우선시하지 않음.
  
- **커뮤니케이션**
  - LLM의 게임 내 힌트 분석, 평균적으로 95.30%의 성공률 기록.
  - 다른 LLM의 정보를 해석할 때 의사소통 오류 발생.

- **상대 모델링**
  - LLM이 상대의 비밀 단어를 추론하는 능력 검토.
  - GPT-4가 가장 뛰어난 추론 및 상대 모델링 능력을 발휘하였으며, 성공적으로 적절한 힌트를 조정.

- **수치 추론**
  - 경매 상황에서 LLM의 입찰 행동 분석.
  - 대부분의 LLM이 내기 균형보다 높은 입찰을 시도함.

- **위험 평가**
  - Texas Hold’em 카드 게임에서 LLM의 위험 평가 능력 검토.
  - LLM이 보수적인 접근 방식을 선호하며, GPT-4는 높은 승률일 때 공격적인 전략을 선택함.

- **팀 협력**
  - Hanabi 환경에서 팀 협력의 효율성 분석.
  - 대부분의 LLM이 카드 공개 행동을 많이 사용하지만, 정보를 공유하기 위한 카드 버리기는 소홀히 함.

---

# 5 Related Works

- **LLM 평가**
  - LLM의 빠른 발전으로 기존 NLP 작업의 평가 방법이 성능을 제한함 (Hu et al., 2023b 등).
  - 사람에 의해 작성된 시험 질문 사용이 주류 평가 패러다임으로 발전, 세계 지식과 추론 능력 테스트에 집중 (Hu et al., 2023a 등).
  - 다양한 차원에서 LLM을 평가하는 새로운 벤치마크 및 방법 제안:
    - 전문 분야에 대한 적응성 (Xiang et al., 2023).
    - 현실 세계 응용 (Li et al., 2023b; Liu et al., 2023a, 2024b,a).
    - 강건성 (Zhu et al., 2023; Hu et al., 2024).
    - 다중 모달 능력 (Fu et al., 2023).

- **LLM을 에이전트로 간주하는 추세**
  - 학계에서 LLM을 에이전트로 취급하는 연구 증가 (Liu et al., 2023b; Wu et al., 2023).
  - 게임에서 에이전트로 작동할 때 LLM 성능 평가 시도.
  - 하지만 대부분의 연구는 단일 에이전트에 한정, 그룹 행동과 협력 시나리오의 특성을 충분히 포착하지 못함.

- **LLM 기반 에이전트**
  - 강화 학습이 자율 에이전트 훈련에 광범위하게 사용됨 (Ribeiro, 2002 등).
  - LLM을 인지 개체 및 에이전트의 컨트롤러로 활용하는 것이 인정받고 있음 (Huang et al., 2022 등).
  - CoT 기술 소개, LLM 기반 에이전트의 추론 및 계획 능력 강화 (Wei et al., 2022).
  - 협력적 및 적대적 프레임워크가 단일 에이전트보다 여러 작업에서 일반적으로 우수함 (Park et al., 2023).

---

# 6 Conclusion

- LLMArena라는 벤치마크를 도입하여 LLM 에이전트의 다양한 능력을 평가 
- 7개의 게임 환경 분석 결과, LLM의 약점 드러남:
  - 공간 추론
  - 상대 모델링
  - 팀 협력
- LLM 에이전트의 성능 향상이 동적 다중 에이전트 환경에서의 주요 도전 과제로 남아 있음
- LLMArena의 활용 기대:
  - 향후 연구자들이 다양한 시나리오에서 평가를 진행할 것으로 예상
- 한계점:
  - LLM 에이전트의 능력은 텍스트 분석을 넘어서야 함
    - 비디오와 오디오 등 다양한 모달 입력과의 상호작용 필요
  - 외부 도구를 활용하는 LLM의 가능성 탐구 부족
- 윤리적 고려사항:
  - LLM 에이전트의 책임 있는 사용과 악용 가능성 문제 제기
  - 결정권에 LLM을 배치할 경우 책임 소재와 윤리적 프레임워크 필요
  - 가이드라인 및 모니터링의 필요성 강조

---

# Limitations

- LLM 에이전트를 둘러싼 윤리적 문제는 다음과 같은 여러 도전 과제가 있음:
  - 책임 있는 사용의 필요성
  - 남용 가능성

- 자율 LLM 에이전트를 의사 결정 역할에 배치할 경우:
  - 책임 문제 제기
  - 비윤리적 사용을 방지하기 위한 강력한 프레임워크 필요

- 남용 가능성:
  - 상황 조작 또는 시스템 악용 가능성 존재
  - 엄격한 지침 및 모니터링 필요