<img width="811" alt="image" src="https://github.com/user-attachments/assets/9a690b26-f562-4f50-b420-c49ca1574f9e" />---
title: "[논문리뷰] Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation- A Reinforcement Learning Approach (ACL 2023)"
date: 2025-03-14 15:00:00 +0900
categories:
  - Paper Review
tags:
  - ACL 2023
  - Empathetic Dialogue Systems
---

요약: 본 연구는 다중 대화에서 긍정적인 감정 유도를 형식화하고, 대화 일관성을 유지하면서 감정 지원을 제공하는 새로운 패러다임과 이를 위한 모델 SUPPORTER를 제안한다.

---

# 1 Introduction

<img width="399" alt="image" src="https://github.com/user-attachments/assets/a374f1c7-6345-42ef-886e-88fa6013362f" />

- 감정 지원(ES)은 정서적 고통에서 회복하고 정신 상태를 개선하기 위한 목적임 (Burleson, 2003).
- 이는 사회적 상호작용에서 감정 지능의 한 형태로 나타남 (Heaney and Israel, 2008; Atoum and Al-Shoboul, 2018).
- ES를 사회 대화 시스템에 적용하여 신뢰할 수 있는 도움을 주는 에이전트를 만드는 것이 최근의 트렌드임 (Huang et al., 2020; Rains et al., 2020).

- 일반적인 접근법은 공감을 모델링하여 타인의 상황과 감정을 이해하려고 함 (Keskin, 2014).
- 그러나, 공감 대화(Rashkin et al., 2019)는 ES 제공에 있어 다음과 같은 두 가지 결함이 존재함:
  1. 다중 턴 대화에 대한 고려 부족:
     - 각 단일 대화 턴에서 공감 응답을 생성하는 것은 사용자 피드백 및 정신 상태 변화를 무시하게 됨.
  2. 감정 유도에 대한 인식 부족:
     - 단순히 감정적 공명을 발산하는 것은 사용자가 부정적인 정신 상태에서 벗어나는 데 실패함.

- Liu et al. (2021)은 이러한 결함을 보완하기 위해 감정 지원 대화(ESC) 작업을 설계하였으나, 기존 연구는 효과를 무시하며 기반 응답 및 대응 전략에만 집중함 (Tu et al., 2022; Cheng et al., 2022; Peng et al., 2022).

- 그러므로 전체 ESC 프로세스를 수립하기에 여전히 부족함.
- 이에 따라 다중 턴 ESC와 긍정적 감정 유도를 도입하고, 사용자와의 대화를 통해 긍정적인 정신 상태에 이르도록 점진적으로 공감하고 유도하도록 설계됨.

- ESC의 어려운 점:
  1. 사용자 감정은 종종 부정적에서 긍정적으로 변화하며, 이러한 변화를 잘 처리해야 함 (예: “학교가 닫혔다” → “이제 기분이 좋아졌다”).
  2. 감정 지원 응답은 공감과 유도 간의 미세한 균형을 유지해야 함.
  3. 언어 표현의 측면에서, 긍정적 감정을 유도해야 하지만 대화의 일관성을 해치지 않아야 함.

- 본 논문에서는 다중 턴 감정 지원 대화를 촉진하기 위해 SUPPORTER를 제안함. 
  - SUPPORTER는 전문가 혼합(MoE) 기반 강화 학습(RL)을 이용하여 발전된 모델임. 

- 주요 기여:
  1. 긍정적 감정 유도로 다중 턴 ESC의 도전 과제를 공식화하는 새로운 패러다임을 소개함.
  2. 모호한 SL과 대화 일관성을 유지하면서 긍정적 감정을 유도하는 SUPPORTER 모델을 제안함.
  3. 자동, 대화형 인간 및 새로운 감정 지원과 대화 일관성 평가를 통해 SUPPORTER의 우수성을 입증함.

---

# 2 Related Work

- **공감 기반 대화**
  - 따뜻한 대화 시스템 구축의 핵심은 공감 능력을 부여하는 것 (Rashkin et al., 2019).
  - **정서적 공감** (Lin et al., 2019; Majumder et al., 2020; Li et al., 2020, 2022): 사용자의 감정을 인식.
  - **인지적 공감** (Zheng et al., 2021; Sabour et al., 2022; Zhou et al., 2022): 사용자의 상황 이해.
  - 단일 턴 공감에 초점을 두고 감정 유도 부족으로 인해 사용자의 정신 상태 개선이라는 목표에 도달하기 어려움.

- **감정 지원 대화 (ESC)**
  - Liu et al. (2021)의 ESC 설계는 상호작용에서 감정 지원 제공.
  - 본 연구는 ESC 관련 작업과 연결되지만, 긍정적 감정 유도 효과 증진에 중점.
  - 지식 융합 (Tu et al., 2022; Peng et al., 2022) 및 계획 전략 (Cheng et al., 2022)은 단어 겹침 지표에 유익하지만, 이를 감정 지원에 주는 가치가 불투명하고 설득력이 떨어진다고 주장.

- **긍정적 감정 유도 대화**
  - 사용자가 정서적 고통에서 벗어나 낙관적인 상태로 대화를 이끄는 직관적 솔루션 (Mishara et al., 2007; Jiang et al., 2021).
  - 이전 작업들 (Hasegawa et al., 2013; Lubis et al., 2018, 2019a,b)은 정서 유도 과정을 이상적인 단일 턴 대화로 설정.
  - 실제 시나리오는 복잡한 정서적 변동을 포함하는 다중 턴 상호작용으로 발생하므로, 긍정적 감정 유도를 ESC에 확장하여 실제 애플리케이션으로 정의.

---

# 3 Preliminaries

- 대화의 t번째 턴에서 주어진 대화 맥락 $$C_t = \{x_1, y_1, \ldots, x_{t-1}, y_{t-1}, x_t\}$$에 따라 사용자 정신 상태를 개선하는 응답 $$y_t$$를 생성하는 것이 목표.
  
- 긍정적인 감정을 유도하기 위한 감정 유도(ES):
  - 다중 턴 감정 유도에서 두 가지 문제 발생:
    1. 긍정적인 감정을 유도하는 강도가 대화 진행에 따라 점진적으로 조정되어야 함.
    2. 사용자 다음 턴의 발언 피드백을 통해 긍정적 감정 유도의 효과가 간접적으로 검증되어야 함.
  - 이를 위해 대화 수준 및 턴 수준의 ES 보상 체계를 구축하여 유도 정책 학습을 안내하고, 자동 및 상호작용적 인간 평가를 통해 ES 성능을 측정.

- 대화의 일관성을 위한 언어 표현:
  - 유도를 강화하는 생성 과정에서의 두 가지 문제:
    1. 적절한 통제 없이 감정 유도 목표를 탐욕스럽게 추구할 경우 맥락 일관성이 결여될 수 있음.
    2. 응답이 사용자의 기대에 부합하는지 여부는 사용자 다음 발언의 피드백 필요.
  - 이를 해결하기 위해 맥락적 및 미래 대화 일관성 보상을 구축하여 이중 일관된 표현의 학습을 안내하고, 대화 목표의 일관성을 평가하기 위한 자동 및 상호작용적 인간 평가를 수행.

---

# 4 Methodology

<img width="811" alt="image" src="https://github.com/user-attachments/assets/788bb05d-5dac-4aaa-a50b-ca6b0fbf0db1" />

- **모델 개요**
  - ourSUPPORTER는 대화 맥락을 입력으로 받아 상태 시퀀스를 구성.
  - 대화 인코더를 사용하여 대화의 의미를 인코딩하고, 감정 및 키워드 예측 작업과 관련된 전문가 조합이 상태 의미를 특성화하여 전문가 선택 정책의 행동 후보를 생성.
  - 업데이트된 상태를 이용해 응답 생성 및 정책 최적화.

- **4.1 다중 작업 전문가 혼합 (Multi-task Mixture-of-Expert)**
  - **대화 인코더**
    - BlenderBot을 활용. 입력 시퀀스 X를 [CLS] 토큰과 함께 결합하여 대화의 숨겨진 상태 HX를 획득.
  - **감정 전문가**
    - 사용자 감정 상태의 가능성 있는 전환을 추적하기 위해 감정 전문가를 개발.
    - M개의 세밀한 감정 반응 추출. 감정 단어를 ID하기 위해 VAD 사용.
    - 긍정적 및 부정적 감정 전문가 생성:
      $$H_{X,pos} = MLP_{pos}(H_X)$$
      $$H_{X,neg} = MLP_{neg}(H_X)$$
    - 감정 예측을 위한 손실함수 정의:
      $$L_{ctx-emo,pos} = -\frac{1}{|e^*_{pos}|}\sum_{i=1}^{|e^*_{pos}|} \log P_{pos}(e^*_i)$$
      $$L_{ctx-emo,neg} = -\frac{1}{|e^*_{neg}|}\sum_{i=1}^{|e^*_{neg}|} \log P_{neg}(e^*_i)$$
  - **키워드 전문가**
    - 대화의 일관성을 유지하기 위한 키워드 예측을 산출.
    - Bidirectional Emotion Keyword Graph G 생성.
    - 각 발화에서 두드러진 키워드를 추출하고 VAD로 감정 극성을 식별.
    - PMI를 사용하여 키워드 간의 관계 설정.
    - 키워드 손실 함수 설정:
      $$L_{kws} = L_{ctx-kws,pos} + L_{ctx-kws,neg} + L_{ftr-kws,pos} + L_{ftr-kws,neg}$$
  - **다중 작업 학습**
    - 감정 및 키워드 전문가의 표현을 평균화하여 원래 시맨틱을 유지하면서 다양성을 보존.
    - 손실 함수는 MSE를 통해 최적화:
      $$L_{mse} = \alpha \sum_{i=1}^{dh}(h_X[i]-h_{exp}[i])^2$$
    - 최종 다중 작업 손실 함수 정의:
      $$L_{exp} = L_{emo} + L_{kws} + L_{mse}$$

- **4.2 MoE 기반 강화 학습 (MoE-based Reinforcement Learning)**
  - **상태 정의**
    - 대화 맥락과 추출된 키워드를 연결하여 초기 상태 s1를 정의.
  - **행동 정의**
    - 다중 작업 전문가를 변환하여 각 단계에서 행동 공간 Ak을 정의.
  - **정책 네트워크**
    - REINFORCE와 기준선 기반 정책 선택 네트워크 설계.
    - actor 네트워크가 현재 상태 및 행동 공간을 기반으로 행동 선택 확률 분포를 방출.
  - **보상 구조**
    - 대화의 감정 유도 및 일관성을 유지하기 위해 보상을 정의.
      - 대화 수준 ES 보상 정의:
        $$PED_{cES} = f_{ES}(y) - f_{ES}(ct)$$
      - 턴 수준 ES 보상 정의:
        $$PED_{tES} = |f_{ES}(y) - f_{ES}(cf)|$$
      - 일관성 보상 정의:
        $$r_{cDC} = f_{cDC}(C \oplus C_{kws}, y \oplus y_{kws})$$
      - 미래 일관성 보상 정의:
        $$r_{fDC} = f_{fDC}(cf \oplus cf_{kws}, y \oplus y_{kws})$$
      - 총 보상 계산:
        $$r = w_{cES} \cdot r_{cES} + w_{tES} \cdot r_{tES} + w_{cDC} \cdot r_{cDC} + w_{fDC} \cdot r_{fDC}$$

- **4.3 최적화 (Optimization)**
  - 에이전트 학습 목표: 예상 누적 보상 최대화. 
  - 에이전트 손실 정의:
    $$\nabla_{\theta}J_{\theta} = E_{\pi}[\nabla_{\theta}\log \pi_{\phi}(a_k,s_k,A_k)(G-Q_{\delta}(s_k))]$$
  - 응답 생성을 위해 마지막 상태 HS,K+1에서 디코더 최적화:
    $$L_{gen} = -\sum_{m=1}^{M}\log P(y_m \mid H_{S,K+1},y_{<m})$$
  - 예비 학습을 위해 사전 학습된 BenderBot 모델 초기화 및 조정:
    $$L_{warm} = L_{exp} + L_{gen}$$
  - 최종 조인트 학습 손실 정의:
    $$L_{joint} = L_{agent} + L_{gen} + \frac{1}{K+1}\sum_{k=1}^{K+1}L_{exp,k}$$

---

# 5 Experiments

- **실험 설정**
  - **데이터셋**: ESConv 데이터셋 사용, 다중 대화에서 사용자가 개인적인 부정적인 상황을 털어놓고 지지자가 위로함.
  - **모델 선정**:
    - MoEL, MIME, BlenderBot-Joint, MISC, GLHG, Bart-Joint, SUPPORTER 등 다양한 모델 사용.
  
- **자동 평가**:

  <img width="811" alt="image" src="https://github.com/user-attachments/assets/7a2f98e0-b34e-4da8-9463-a2988f0a9919" />

  - 사용된 척도: Perplexity (PPL), Bleu (B-n), Distinct (D-n).
  - 효과 측정:
    - ES 점수: 대화 수준(cES), 턴 수준(tES) 측정.
    - 대화 일관성 점수: 맥락적(cDC), 미래(fDC) 일관성 측정.
  - **성과**: SUPPORTER는 가장 높은 다양성과 ES 점수를 달성, 경쟁력 있는 대화 품질 유지.

- **소거 연구**:
  - 각 구성 요소의 삭제 실험 수행 (예: EmoExperts, KwsExperts, Multi-Task 등).
  - Emotion 전문가 및 키워드 전문가가 성과에 중요한 영향을 미친다는 결과 도출.
  - Warm Start와 Joint Training을 각각 실시한 결과, 두 방식의 효과를 분석.

- **상호작용 인간 평가**:

  <img width="811" alt="image" src="https://github.com/user-attachments/assets/f1765434-0b7b-41a4-8a84-d5995b429107" />

  - 100개의 부정적인 상황을 대상으로 3명의 작업자들이 모델들과 다회 대화 시뮬레이션.
  - 평가 항목: 유창성, 정보성, 일관성, 지지성, 전반적 선호도.

- **정성적 분석**:
  - 전문가의 구체성 분석: t-SNE를 사용하여 전문가의 잠재 공간을 시각화.
  - 감정 유도를 조정하는 능력 분석: 대화 턴에 따른 긍정적 감정 거리 변화 추적.

- **파라미터 분석**:
  - 반복 단계를 변화시켜 성능에 미치는 영향을 분석.
  - 지표의 변화 과정 및 SUPPORTER의 성과 비교. 

이 실험들은 SUPPORTER 모델의 유효성을 입증하고, 다양한 요소들이 성과에 미치는 영향을 심도 있게 분석하였습니다.

---

# 6 Conclusions

- 본 논문에서는 다중 대화형 정서 지원 대화(ESC)를 긍정적 감정 유발 과정으로 형식화하는 새로운 패러다임을 소개하고, MoE 기반의 강화 학습 모델인 SUPPORTER를 제안하였습니다.
- 잘 설계된 정서 지원(ES) 및 대화 일관성 보상을 통해 긍정적 감정 유발을 효과적으로 수행하는 모델의 우수성을 실험을 통해 검증하였습니다.
- 본 연구는 사용자들의 정신 상태 개선을 위한 긍정적 감정 유발을 포함한 ESC 개발을 촉진하는 데 기여할 것입니다.
  
## Limitations
- 강건하지 못한 강화 학습의 요구: 긍정적 감정 기반 ESC를 위한 유연한 표현 모델링이 가능하지만, 안정성 문제가 있어 추가적인 지식이나 전략이 필요합니다.
- 심리학 이론에 대한 추가 참고 필요: 대화 맥락과 향후 피드백을 통합하여 후행 패턴을 학습하는 이점이 있지만, 인지 행동 치료(CBT)와 같은 심리학 연구에서 더 많은 지식을 참조할 필요가 있습니다.
- 보상 설계 최적화 필요: 인간 피드백 레이블을 갖춘 고품질 데이터셋을 통해 보상 모델을 학습시킬 수 있으며, 이에 따른 최적화 및 비용 간의 균형 필요합니다.

## Ethical Considerations
- 본 연구에서 사용한 ESConv 데이터셋은 공개 벤치마크로, 민감한 개인 정보나 비윤리적인 언어를 포함하지 않습니다.
- 우리는 이 데이터셋을 기반으로 사용자들의 정신 상태 개선을 위한 대화 시스템을 구축하려고 하며, 전문 심리적 상담이나 치료를 대체하지 않습니다.
- 자해나 자살 관련 대화와 같은 위험한 비일상적 상황에 대해서는 치료 효과를 주장하지 않으며, 인간 평가의 익명성을 보장합니다. 이 연구는 ACL 윤리 강령을 충족한다고 믿습니다.
