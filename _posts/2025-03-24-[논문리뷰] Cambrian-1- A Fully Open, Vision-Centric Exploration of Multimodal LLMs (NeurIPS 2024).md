---
title: "[논문리뷰] Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs (NeurIPS 2024)"
date: 2025-03-24 00:00:00 +0900
categories:
  - Paper Review
tags:
  - NeurIPS 2024
---

요약: Cambrian-1은 비전 중심 접근 방식을 통해 설계된 멀티모달 LLMs (MLLMs)의 가족으로, 시각 표현 학습 연구와의 연결이 부족한 비전 요소 설계 문제를 개선하며, 실험 및 평가를 통해 다양한 모델과 아키텍처에 대한 새로운 통찰을 제공합니다. 이 논문은 고품질의 시각 명령어 조정 데이터의 중요성을 강조하며, Cambrian-1이 최신 성능을 달성하는 동시에 멀티모달 시스템과 시각 표현 학습의 발전을 촉진하는 포괄적인 지침서로서 기능하도록 만듭니다.

---

# 1 Introduction

<img width="717" alt="image" src="https://github.com/user-attachments/assets/ef05b8a3-1c56-42a7-a7a7-d25f5ac0bb50" />

- 언어의 이해와 의미가 감각적 토대를 필요로 하는지에 대한 철학적 논쟁이 오랫동안 지속되어 왔습니다.
- 아리스토텔레스는 감각 경험과 경험적 관찰을 통한 지식 획득의 중요성을 강조했으며, 이는 그의 Peripatetic 학파에 중심적이었고 오늘날까지도 영향을 미치고 있습니다.
- 아퀴나스는 13세기에 "감각에 먼저 존재하지 않는 것은 지성에도 존재하지 않는다"라는 Peripatetic 공리를 통해 이러한 아이디어를 공식화했습니다.
- 감각적 토대가 없더라도, 이를 가지는 것이 유익하다는 것은 확실합니다.
- 캄브리아기 폭발 동안 시각의 등장이 초기 동물들에게 음식을 찾고 포식자를 피하며 진화하는 데 중요했다고 믿어집니다.
- 대부분의 인간 지식과 동물 지식은 시각, 청각, 촉각, 미각, 후각과 같은 감각 경험을 통해 물리적 세계와 상호작용하며 습득됩니다.
- 이러한 감각 경험은 세상을 이해하는 데 기본적이며 실제 세계에서의 행동과 의사 결정에 중요합니다.

- 철학적 논쟁을 넘어, 최근 멀티모달 대형 언어 모델 (MLLM)의 발전은 시각적 표현 학습과 언어 이해에 대한 실질적인 초점을 맞추게 했습니다.
- 언어 모델들은 강력한 확장 행동을 보였으며, 최근 멀티모달 학습의 발전은 더 크고 나은 LLMs의 개발에 의해 크게 좌우됩니다.
- 한편, 시각 구성 요소에 대한 설계 선택은 종종 충분히 탐색되지 않고 시각적 표현 학습 연구와 연결되지 않았습니다.
- 예를 들어, LLaVA와 같은 많은 선구적 프레임워크는 언어에 의해 강력히 지도된 시각 변환기 기반 CLIP 모델을 시각적 특징 추출기로 사용합니다.
- 다른 시각적 표현, 예를 들어 자가 지도 DINO와 같은 것들이 탐색되고는 있지만, 이 분야에 대한 포괄적이고 체계적인 연구가 부족합니다.
- 이러한 격차는 주로 연구가 도전적이기 때문에 존재합니다. MLLMs는 복잡한 훈련과 평가 파이프라인을 가지며 고려해야 할 많은 설계 결정이 있습니다.

- 이 연구에서는 MLLMs를 시각 중심 관점에서 탐험하여 격차를 해소하고자 합니다. 구체적으로, MLLM 지시 조정을 다양한 시각적 표현에 대한 평가 프로토콜로 사용합니다.
  
- 현재 멀티모달 학습 연구의 두 가지 잠재적 우려도 이 연구에 대한 동기가 됩니다:
  1. 언어에 너무 일찍 지나치게 의존하면 효과적인 시각적 표현 학습의 결핍을 보완하는 지름길이 될 수 있습니다.
  2. 기존 벤치마크들은 실제 시나리오에 적절한 지침을 제공하지 못할 수 있습니다—여기서 시각 적 토대가 강력한 멀티모달 이해에 필수적입니다.
  
- 전통적인 시각적 표현 학습의 평가 프로토콜 (예: ImageNet-1K, COCO, ADE20K)들은 점점 더 포화 상태에 이르고 있으며 실제 세계 분포에서 발견되는 다양한 인식 도전을 반영하지 못합니다.
  
- 본 연구는 새로운 프로토콜 설계를 탐색하고, 미래의 더 나은 시각적 표현 개발에 지침을 제공할 데이터를 수집하고자 합니다.
  
- 또한, 이 통합 설정에서 시각적 표현을 더 잘 평가하기 위해, 우리는 전통적인 비전 벤치마크를 VQA 형식으로 변환하여 비전 중심 MLLM 벤치마크, CV-Bench를 개발합니다.

- Cambrian-1은 MLLM의 설계 공간에 중요한 통찰력을 제공하는 다섯 가지 주요 기둥으로 구성되어 있습니다:
  - 시각적 표현: 다양한 비전 인코더와 조합을 탐구합니다.
  - 커넥터 디자인: 비전 특징을 LLMs와 통합하면서 토큰 수를 줄이는 새로운 동적이고 공간 인식 커넥터를 설계합니다.
  - 지시 조정 데이터: 공공 소스로부터의 고품질의 시각적 지시 조정 데이터를 큐레이션합니다.
  - 지시 조정 레시피: 지시 조정 전략과 관행을 논의합니다.
  - 벤치마킹: 기존 MLLM 벤치마크를 분석하고, 새로운 비전 중심 벤치마크 "CV-Bench"를 도입합니다.

---

# 2 Evaluating Visual Representations through MLLMs

<img width="717" alt="image" src="https://github.com/user-attachments/assets/c197048a-db59-45b2-8afa-5d9da8f01982" />

- **CLIP 기반 MLLM**
  - 대부분의 MLLM은 시각 인코더로 CLIP [109]를 사용
  - 언어와 사전 정렬이 되어 있고, 언어 모델(LLM) 토큰 공간에 적응하기 쉬움
  - 강력한 언어 선행 지식은 양날의 검, 시각 표현 학습의 결핍을 보완

- **시각 인코더 평가**
  - 여러 시각 인코더 선택이 MLLM의 다중모달 기능에 미치는 영향 평가
  - 전통적 프로토콜을 넘어서는 MLLM 평가 프레임워크 제안
  - Linear probing, end-to-end fine-tuning를 대체할 수 있는 방법 제안

## 2.1 Analyzing the Benchmarks

<img width="717" alt="image" src="https://github.com/user-attachments/assets/dd2ef94b-8194-441c-b451-3c6563d6b4cc" />

- **벤치마크 선택**
  - 멀티모달 기능을 정확하게 평가할 벤치마크 선택 필요
  - 연구에서 사용된 공통 벤치마크 집합 사용

- **모델 트레이닝 과정**
  - 23개의 다른 비전 백본(모델계열 참조)
  - ShareGPT-4V [27]로부터 1.2M 어댑터 데이터로 연결자 학습, 737K 명령어 튜닝 데이터로 미세 조정

- **시각 입력의 필요성**
  - MLLM 성능을 비주얼 입력 여부에 따라 비교
  - 무작위 추측을 통한 예상 점수 계산
  - 일부 벤치마크는 시각 정보에 크게 의존하지 않음

- **벤치마크 클러스터링**
  - MLLM 성능 측면에서 벤치마크 간의 상관 관계 분석
  - 주요 카테고리: "General", "Knowledge", "Chart & OCR", "Vision-Centric"

## 2.2 Cambrian Vision-Centric Benchmark (CV-Bench)

- **기존 벤치마크의 한계**
  - 대부분의 벤치마크가 비전 중심 기능을 제대로 측정 못함
  - 충분한 샘플을 제공하는 비전 중심 벤치마크 부족

- **CV-Bench 도입**
  - 총 2638개의 수작업 검사 예시 제공
  - 기존 비전 벤치마크 재사용을 통해 멀티모달 컨텍스트 내에서 모델 평가

## 2.3 Instruction Tuning Recipes

<img width="717" alt="image" src="https://github.com/user-attachments/assets/a9d13989-633e-45b4-92d6-99d5ad4c6696" />

- **MLLM 학습 과정**
  - 두 단계의 프리트레인 및 미세 조정 과정
  - 프리트레인: 어댑터 데이터를 통해 연결자 학습
  - 미세 조정: 명령어 튜닝 데이터로 연결자 및 LLM 학습

- **학습 전략 재탐색**
  - 언어- 및 비언어-지도 인코더를 비교
  - 다양한 시각 인코더 선택의 영향과 튜닝 전략 비교

## 2.4 MLLMs as a Visual Representation Evaluator

<img width="717" alt="image" src="https://github.com/user-attachments/assets/f6073f2d-5bfe-4d6c-995d-ba98cb062f8d" />

<img width="717" alt="image" src="https://github.com/user-attachments/assets/08295eda-ada2-4a1f-b5a9-02ce2bb227be" />

- **다양한 평가 방법**
  - 전통적인 ImageNet-1k linear probing를 넘어선 새로운 시각 모델 평가 인터페이스 제공

- **모델 성능 비교**
  - 언어-지도 모델이 비언어-지도 모델보다 대부분의 벤치마크에서 우수
  - 고해상도 모델이 Chart & Vision-Centric 벤치마크에 특히 효과적

- **성능 갭 좁히기**
  - SSL(자기 감독 학습) 모델의 지속적인 파인튜닝을 통해 언어-지도 모델과 유사한 성능 달성 가능성 탐색

## 2.5 Combining Multiple Vision Encoders

- **다중 시각 인코더 결합**
  - 서로 다른 시각 인코더 결합의 잠재력 탐색
  - SSL 모델 추가가 비전 중심 과제에서 성능 개선, OCR 벤치마크에서도 혜택

- **한계 및 나아갈 방향**
  - 단순한 인터폴레이션 및 단순 연결의 한계 지적
  - 정보 손실을 줄이고 모델 결합 효율성을 높이는 전략 추구

- **결론**
  - 여러 시각 인코더 조합이 다중 벤치마크에서 MLLM 성능 향상을 가져올 수 있음

---

# 3 Spatial Vision Aggregator (SV A): A New Connector Design

<img width="717" alt="image" src="https://github.com/user-attachments/assets/8d1b41a8-b31f-42df-a5a6-b69fa7fef538" />

- SV A는 다중 비전 인코더로부터 정보를 효과적으로 집계하고 보간에 의해 발생할 수 있는 정보 손실을 방지하기 위해 학습 가능한 잠재 쿼리 세트를 사용합니다.
- 두 가지 새로운 비전 중심의 설계 원칙을 도입합니다:
  1. 쿼리의 각 토큰에 대해 집계 공간을 명시적으로 정의하여 공간 귀납적 편향을 도입합니다.
  2. LLM 계층 전반에 걸쳐 비전 피처를 여러 번 집계하여 필요할 때마다 시각 정보를 통합할 수 있도록 합니다.
- 크로스 주의 메커니즘을 통해 정보를 집계하기 위해 C차원의 학습 가능한 잠재 토큰 $$x \in \mathbb{R}^C$$을 만들어 L × L번 반복하여 2D 격자 $$X \in \mathbb{R}^{L^2 \times C}$$로 형성, 쿼리로 사용합니다.
- N개의 비전 인코더에서 얻은 시각적 피처 집합 F는 컨텍스트 (키 및 값)로 사용됩니다.
- 각 비전 인코더의 출력 해상도는 L의 배수로 설정되어 있으며, k번째 비전 인코더의 피처 맵은 $$m_kL \times m_kL \times C$$의 해상도를 가집니다.
- 공간 귀납적 편향: 크로스 주의 동안 공간 구조를 유지하기 위해 각 쿼리 토큰을 모든 비전 인코더의 피처 맵의 특정 부분 영역에 맞춥니다. 

$$
F_k[m_k \cdot i : m_k \cdot (i+1), m_k \cdot j : m_k \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}
$$

- 쿼리 벡터 $$\mathbf{q}^*_{i,j} \in \mathbb{R}^{1 \times C}$$가 (i, j) 위치에서 계산되고, 수식은 다음과 같은 형식입니다:

$$
\mathbf{q}^*_{i,j} = \text{softmax} \left( \frac{\mathbf{q}_{i,j} \cdot [\mathbf{k}_{i,j,1}, \mathbf{k}_{i,j,2}, \ldots, \mathbf{k}_{i,j,N}]^\top}{\sqrt{C}} \right) [\mathbf{v}_{i,j,1}, \mathbf{v}_{i,j,2}, \ldots, \mathbf{v}_{i,j,N}]
$$

여기서

- $$\mathbf{q}_{i,j} = W_Q \mathbf{x}_{i,j} \in \mathbb{R}^{1 \times C}$$은 쿼리 벡터, $$\mathbf{k}_{i,j,k} = W_K F_k[m_k \cdot i : m_k \cdot (i+1), m_k \cdot j : m_k \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}$$는 키 벡터, $$\mathbf{v}_{i,j,k} = W_V F_k[m_k \cdot i : m_k \cdot (i+1), m_k \cdot j : m_k \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}$$는 각 인코더 k에 대한 값 벡터입니다.
- 여러 레이어의 비전 집계: 다중 비전 인코더로부터 피처를 집계함에도, 고해상도 입력 (큰 $$m_k$$) 또는 다중 인코더 (큰 N)로 인해 잠재적 정보 손실이 발생할 수 있습니다. 이 문제를 방지하기 위해, LLM 안에 여러 번 크로스 주의를 삽입하여 일관된 비압축 시각 정보를 접근할 수 있도록 합니다.
- 하이퍼파라미터: D와 G, 즉 크로스 주의 레이어 수와 학습 가능한 쿼리 그룹 수를 도입하여 용량을 조절합니다. LLM 레이어 내의 크로스 주의 레이어에서 D와 G는 항상 1로 설정됩니다.
- SV A는 다른 집계 방법들보다 consistently outperform하며 고해상도 시각 정보를 잘 집계합니다.
- D, G를 통해 하이퍼파라미터 설정의 이점을 실험적으로 분석하였고, SVA는 기존의 방법들보다 성능에서 뛰어난 개선을 보였습니다.
- 결론적으로, 공간 귀납적 편향과 LLM과 비전 피처간의 깊은 상호작용이 비전 피처를 더 효과적으로 집계하고 압축하는 데 도움을 줍니다.

---

# 4 Instruction Tuning Data for Training MLLMs

- **데이터 수집**

  <img width="717" alt="image" src="https://github.com/user-attachments/assets/9713db2f-de95-4202-90ef-512aed9e5c98" />

  - 언어 데이터에 비해 다중모달(시각적) 인스트럭션 튜닝 데이터는 매우 드물고 수집이 어려움.
  - Visual Question Answering (VQA), OCR 데이터와 같은 시각적 상호 작용 데이터를 포함하는 여러 기존 벤치마크 및 데이터셋을 활용하여 데이터 수집.
  - 대화 능력을 유지하기 위해 고품질의 언어 전용 인스트럭션-팔로잉 데이터를 소량 수집.
  - 데이터를 일반 대화, OCR, 계산, 코드, 수학, 과학, 언어 전용 데이터로 카테고리화.
  - 데이터 출처는 Fig. 8에, 데이터 준비의 세부 사항은 부록 G에 설명됨.
  - **인터넷 데이터 수집 엔진**: 데이터 분포의 불균형 해소를 위해 대규모 고품질의 과학 데이터를 생성할 수 있는 데이터 엔진 도입.
    - 이전보다 400% 증가한 161,000개의 과학 관련 데이터 포인트 생성.
  - **Cambrian-10M**: 약 9,784,000개의 데이터 포인트를 포함하는 대규모 인스트럭션 튜닝 데이터 풀 생성.

- **데이터 큐레이션**

  <img width="717" alt="image" src="https://github.com/user-attachments/assets/8769b794-fe39-439b-89c8-709c3070157e" />

  - Cambrian-10M은 다양한 출처로부터 수집한 대규모 인스트럭션 튜닝 데이터 풀.
  - 데이터 카테고리 간의 비율 불균형을 해결하기 위한 첫 단계로 데이터 큐레이션 진행.
  - **데이터 균형 조정**
    - 특정 데이터 소스의 데이터 포인트 수에 대한 임계값 설정 ($t = 150k, 250k, 350k, 450k$) 및 효과적인 임계값 탐색.
    - '250k'와 '350k' 사이의 임계값이 Cambrian-10M에 가장 효과적임을 발견.
  - **데이터 비율**: 다양한 데이터 유형의 비율 균형이 중요.
    - 실험을 통해 일반, OCR 및 언어 데이터의 균형이 중요하다는 결론.
    - 지식 집약적 작업의 성능은 OCR, 차트, 추론 및 일반 인식의 조합에 따라 달라짐.
  - **Cambrian-7M**: 데이터 필터링을 통해 Cambrian-10M에서 파생된 더 작은 고품질 데이터셋. 
    - 표 3에 따르면, 샘플 수가 적음에도 불구하고 성능 향상을 보여줌.
    - "answer machine 현상"을 방지하기 위해 시스템 프롬프트 적용. 추가 정보는 부록 G.2에 제공.

---

# 5 State of the Art Performance

- 우리는 이전 연구의 통찰력을 활용하여 Cambrian-1이라고 부르는 MLLMs(다중 모드 대형 언어 모델) 계열을 훈련함.
- 다양한 크기의 LLM 백본을 사용한 모델을 훈련: LLaMA-3-Instruct-8B, Vicuna-1.5-13B, Hermes-2-Yi-34B.
- Vision 구성 요소는 OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, OpenCLIP ConvNeXt-XXL@1024, DINOv2 ViT-L/14@518 모델을 결합하여 Spatial Vision Aggregator 사용.
- 2.5M 어댑터 데이터를 사용하여 커넥터를 사전 훈련하고 Cambrian-7M 데이터 믹스로 지시를 튜닝.
- 모델은 Section 2.1에서 분류된 벤치마크에서 평가되었고, 결과는 Table 4에 제시.
- Cambrian-1은 LLaVA-NeXT와 Mini-Gemini 같은 오픈 소스 모델을 초월하며 특히 고해상도 이미지 처리가 필요한 작업에서 뛰어난 성능 발휘.
- Cambrian-1은 일부 벤치마크에서 GPT-4V, Gemini-Pro, MM-1과 같은 최고 독점 모델과 비슷한 수준의 성능 달성.
- 모델의 가중치, 오픈 소스 코드, 데이터셋, 모델 훈련 및 평가를 위한 자세한 레시피 제공.
- 본 연구가 오픈 연구 커뮤니티를 강화하고, 시각적 표현 학습 및 다중 모드 시스템 연구를 가속화하기를 희망.

(추가적인 노트)
- GPT-4의 GQA 테스트 세트 성능이 낮은 이유는 다른 모델은 GQA 훈련 세트로 훈련된 반면, GPT-4의 훈련 세트는 불확실하기 때문일 수 있음.
