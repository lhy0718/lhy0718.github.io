---
title: "[논문리뷰] EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas (NeurIPS 2024)"
date: 2025-05-27 20:43:23 +0900
categories:
  - Paper Review
tags:
  - NeurIPS 2024
  - Decision Making
  - LLM in Game Theory
---

본 논문은 감정 모델링을 통합한 EAI 프레임워크를 제안하여 대형 언어 모델(LLM)의 윤리적 의사결정에 미치는 감정의 영향을 분석하고, 감정 편향이 LLM의 협력률 저하 등 인간과 다른 행동 양상을 초래함을 밝혀 LLM의 감정 정합성 평가 기준 마련의 필요성을 강조한다.

---

# 1 Introduction

- LLM(대형 언어 모델)의 활용이 의료, 고객 서비스, 디지털 치료 등 다양한 분야에서 증가하고 있으나, 자율적인 정확한 의사결정 능력에 대해서는 규제적, 윤리적, 기술적 논쟁이 존재함.
- LLM은 여러 사회경제적 편향이 반영된 사람 데이터를 기반으로 학습되므로, 사람 행동과의 정렬(alignment)에 관한 연구가 활발히 이루어지고 있음.
- 사람 가치에 대한 LLM의 정렬은 사용자 신뢰 및 만족도 향상뿐 아니라, 실제 의사결정의 안전성과 예측 가능성 확보에 필수적임.
- RLHF(인간 피드백을 통한 강화학습)[1]는 인간의 가치와 의도에 LLM을 맞추기 위한 핵심 기술로 자리잡았으며, 보상 모델은 인간 선호도의 대리인으로 학습되어 강화학습 최적화를 유도함.
- OpenAI, Anthropic, Meta, Google 연구진들은 NLP 벤치마크를 기반으로 안전성과 정렬에 대한 다양한 개념을 제시함[2].
- 하지만 자율 에이전트 시대에 NLP 벤치마크를 넘어서, 인간 행동과의 정렬 과정에서 발생하는 LLM 내부 편향도 고려해야 함.
- 예를 들어, 인간 결정은 감정에 크게 영향을 받으며 종종 비합리적이고[3–9], 비정렬 LLM도 공격성 및 허위정보 생성 등을 통해 이런 비합리성이 관찰됨[10].
- 심지어 정렬된 LLM도 특정 상황 또는 탈옥(jailbreak) 공격 시 의도적 기만이 가능함[11–13].
- 이는 LLM이 인간 감정을 얼마나 정확히 모방하며, 이 감정들이 LLM의 의사결정에 어떤 영향을 주는지에 관한 의문을 제기함.
- 행동경제학, 추천 시스템, 인간-에이전트 상호작용 등 다양한 응용에서는 감정 모델링이 정확하고 안전한 시스템 구현에 필수적임.
- 기존 연구[14–16]는 감정 모델링 질문의 일부를 다루었으며, 본 논문에서는 다양한 게임 이론적 환경과 윤리적 벤치마크를 통해 감정이 LLM의 전략적 의사결정에 미치는 영향을 심층 분석하고자 함.
- 또한 동일한 감정 상태 노출 시 인간과 LLM의 전략 변화 정렬도를 평가함.
- 연구는 두 가지 환경에 집중함:
  - 윤리 벤치마크: 정의된 환경에서 감정의 영향 분석
  - 게임 이론적 환경: 감정에 따른 전략적 의사결정 변화 탐색
- 감정 영향 평가를 위해, 검열, 언어 편향, 모델 크기 등 다양한 모델 파라미터가 감정 모델링 하에서 행동 정렬에 미치는 영향을 사유 및 오픈소스 LLM을 대상으로 비교 분석함.
- 게임 이론 환경에서는 2인 및 다인 전략 게임에서 협력 및 조정 수준과 감정의 영향도를 평가하며, 이로써 인간과의 상호작용에서 자율 결정을 위한 감정 정렬(emotional alignment) 개념을 새롭게 제시함.

## 주요 기여점

- LLM의 윤리적 및 게임 이론적 정렬에서 감정이 미치는 영향을 평가하는 최초의 프레임워크 제시.
- 감정을 부여한 LLM은 윤리적 위험을 노출하며, 인간 정렬에 있어 상당한 편향이 나타나고 부정적 감정 상태에서 정확도가 감소함.
- 광범위한 전략 게임 실험에서 현 LLM들이 감정 및 전략 편향으로 인해 직접 의사결정에 아직 부적합함을 증명, 특히 오픈소스 및 소형 LLM에서 그 영향이 두드러짐.

---

# 2 Related Works

- 감정 모델링은 1차 논리(first-order logic)를 활용한 심리학적 모델 공식화[17-20]부터 특정 감정 표현을 포착하기 위한 LLM 미세조정[21-24]까지 다양한 접근법으로 연구됨.
- 이전 연구들은 감정 상태가 LLM의 자연어처리(NLP) 과제 수행에 미치는 영향을 탐구[25-30].
- Li 등[15,31]은 감정 프롬프트가 논리 추론 및 의미 이해 관련 2개 과제에서 LLM 성능을 향상 또는 저해할 수 있음을 보여줌.
- LLM 에이전트는 사회적 신호에 반응하고 감정적 뉘앙스를 인식하여 사회적 역학을 처리할 수 있음[32].
- 그러나 감정이 LLM의 의사결정 및 윤리성에 미치는 영향은 연구되지 않음.
- 본 연구는 계산 감정 모델(CME)에 기반하여 진화적으로 형성된 소수의 기본 감정을 강조하는 이산 감정 이론(discrete affective theories)[33]을 채택, 윤리 및 의사결정에서 감정 영향 분석을 목적.

## 2.1 윤리(Ethics)

- 인공지능 윤리는 AI 모델과 에이전트의 윤리적 행동 촉진 및 보장에 중점.
- [34]에 따라 LLM 윤리를 암묵적(implicit) 윤리와 명시적(explicit) 윤리로 구분함.
  - 암묵적 윤리: LLM이 상황을 윤리적으로 평가하는 능력
  - 명시적 윤리: 윤리적 딜레마 상황에서 LLM의 선택 평가
- 감정 상태가 반영된 LLM의 공정성과 평등성에 관한 고정관념(stereotype) 연구도 수행[35].
- 도덕적 신념을 호출하는 윤리 평가[36], 신뢰성(신뢰도, 안전, 공정성, 사회 규범 준수 등)[37], 의료[38] 및 법률[39] 영역 윤리적 의사결정 영향 연구 진행.
- 하지만 기존 연구는 윤리적 제약 아래 감정 역할을 명시적으로 다루지 않아 인간 행동과의 정렬(alignment) 측면에서 공백 존재.

## 2.2 게임 이론(Game Theory)

- 표준 실험 경제학 내 게임 이론은 자기이익 극대화를 전제로 하는 "Homo Economicus" 가정을 따름.
- 행동 게임 이론은 타인의 보상(payoff)에 대한 감정과 협력·공정성 분석에 중점.
- 핵심 개념은 나쉬 균형(Nash Equilibrium, NE)[40]으로, 각 플레이어가 전략을 변경해도 보상을 늘릴 수 없는 상태.
- NE는 합리적이고 자기이익을 극대화하는 참여자를 전제[41].
- 그러나 인간 의사결정은 NE와 자주 다름[42], 이는 이성적 분석 외 개인 가치, 신념, 감정 등의 복합적 요인 때문.
- 복수 연구가 죄수의 딜레마에서 ‘분노’, ‘행복’ 등 감정이 의사결정에 미치는 영향을 조사[43-45].
- 성별 전쟁(Battle of the Sexes) 게임의 인간 전략 반응 메타 분석[46], 전술 게임 내 감정 영향[47-50], 다양한 보상 효과[51] 연구도 수행됨.

## 2.3 게임 이론 환경에서의 LLM 평가

- LLM과 게임 이론 교차 영역에선 두 가지 관점에서 연구됨.
1. 인간 의사결정 패턴(기존 연구)과 NE 비교를 통해 LLM 행동이 “Homo Economicus”인지 인간 유사자인지 평가.
2. LLM의 행태 및 협력 행동 분석[52-54].
   - GPT-4는 죄수의 딜레마, 성별 전쟁과 같은 게임에서 가장 잘 수행.
   - 협력이 필요 없는 게임에서 주로 이기적 행동.
   - GPT-4는 ‘항상 반발(always defect)’하는 경향이 강함.
   - 성별 전쟁 게임에서는 교대 패턴을 재현하기 어려워 선호하는 선택을 주로 함.
3. 인간과 LLM 간 게임 이론 내 행태 정렬 연구[55].
   - LLM은 죄수의 딜레마에서 인간보다 높은 협력률 보임.
   - 1회성 독재자 게임(Dictator’s game) 실험에서 인간의 공정성 경향을 더 잘 복제하는 경향 확인.
4. [53,54]는 다양한 게임(극단 제안, 독재자 게임, 소시오로지 실험 등)에서 LLM 전략을 평가.
- 본 연구는 최초로 감정을 통합해 LLM 의사결정에 미치는 감정 역할을 평가.
- 감정 시나리오를 통한 LLM 평가로 윤리적 평가 범위를 확장하고, 감정이 유발되는 상황에서 인간 의사결정 모방 및 대응 가능성 탐색.

---

# 3 EAI Framework

- **목적**  
  LLM이 감정 맥락에서 인간 윤리 및 의사결정과 얼마나 잘 정렬되는지를 평가하기 위해, 다양한 게임 이론적 설정을 수용할 수 있는 새로운 다목적 프레임워크 개발 및 구현.

- **프레임워크 주요 혁신점**  
  감정 입력을 윤리적 설정과 행동 게임 이론 내 LLM의 의사결정 과정에 독특하게 통합.

- **프레임워크 특성**  
  - 높은 유연성 제공  
  - 공동 플레이어 설명, 사전 정의된 전략 등 다양한 파라미터 사용자 정의 가능 (하이퍼파라미터 목록은 부록 B.2 참고)  
  - 프롬프트 체이닝(prompt-chaining) 기법 활용: 게임 중 모든 관련 정보를 LLM에 제공하여 문맥 학습 가능  
  - 게임 설정에 따라 한 판으로 진행되는 일회성 협상 게임 및 윤리 게임, 또는 다회 반복 게임 지원

- **구성 요소**  
  1. **게임 설명 (Game Description)**  
     - 게임의 환경(context)과 규칙 포함  
     - 환경 유형:  
       - 일회성(one-shot) 게임: 한 단계만 필요  
       - 반복(repeated) 게임: 여러 라운드 필수  
     - 윤리(Ethical) 설정은 본질적으로 일회성 게임과 동일하여 별도의 환경 불필요  
     - 실험에서는 LLM에게 최소한의 맥락 정보만 제공하며, 특정 성격 특성 설정하지 않음 (기존 연구 [55, 56]와 차별화)  
     - 목표: 감정이 LLM에 미치는 영향 명확히 평가  
     - 상세 게임 규칙 및 프롬프트는 부록 B, E 참고

  2. **감정 프롬프트 (Emotion prompting)**  
     - 게임 이론 내 실험 감정 연구 방법론 따라, 플레이 전 LLM에 사전 정의된 감정 주입  
     - 초기 시스템 프롬프트는 감정과 게임 설명을 결합하여 제공  
     - 주요 다섯 가지 기본 감정 사용 (Paul Ekman 분류 기준):  
       - 분노(anger), 슬픔(sadness), 행복(happiness), 혐오(disgust), 공포(fear)  
     - 감정 효과의 원인에 따라 차이 존재 (예: 상대방 대상 혐오는 제안 금액 감소, 외부 대상 혐오는 관대함 증가)  
     - 감정 주입 세 가지 전략:  
       - *Simple*: 감정 상태만 단순 주입  
       - *Co-player-based*: 감정을 상대방과 연결  
       - *External-based*: 외부 요인에 의해 유발된 감정을 주입

  3. **게임 별 파이프라인 (Game-Specific Pipeline)**  
     - 게임 설명과 초기 감정 입력에 따라 게임 진행 관리  
     - 구현한 세 가지 파이프라인:  
       - *윤리 설정(Ethical)*: TrustLLM 벤치마크 설문지 기반, 일회성 게임과 유사한 단일 의사결정 수행  
       - *일회성 협상 게임(One-shot bargaining games)*: 사전 정의된 선택지(수락/거절) 또는 예산 분배, 윤리적 결정 등 제안 선택  
       - *반복 게임(Repeated games)*: 이전 설정 확장, 상대 행동, 보상, 내부 감정 등 반복마다 메모리 업데이트하여 행동 역학 영향 평가

- **대상 LLM**  
  - 기존 GPT 모델만 평가하던 연구와 달리 다양한 최첨단 모델 포함  
  - 독점 모델: GPT-3.5, GPT-4, GPT-4o(부분 결과), Claude 3 Haiku, Claude Opus  
  - 오픈소스 모델: LLaMA 2, Mixtral of experts, OpenChat (비제한 모델)  
  - 다국어 모델: GigaChat, Command R+ (언어 편향 분석 목적)  
  - 다양한 API 지원(OpenAI, Anthropic, Hugging Face, OpenRouter)을 통해 새로운 모델 손쉽게 통합 가능  
  - 모델 버전 고정 및 온도(temperature) 0으로 설정하여 재현성 확보 (부록 B.1)  
  - 결과 일관성 5회 반복과 온도 영향 분석은 부록 C 참고

---

# 4 Emotion Impact on LLM Biases and Ethical Problems

- **목적**: 감정 프롬프트가 LLM의 내재된 가치관에 미치는 영향을 분석하고, 세 가지 윤리적 시나리오에서 LLM의 결정 변화 여부를 평가함.

- **윤리적 시나리오 및 평가 지표**:
  - **암묵 윤리 (Implicit Ethics)**: ETHICS 데이터셋을 이용하여 도덕적으로 민감한 시나리오를 "wrong" 또는 "not wrong"으로 분류. 전체 예제 및 라벨별(‘wrong’/‘not wrong’) 정확도(Acc) 평가.
  - **명시 윤리 (Explicit Ethics)**: MoralChoice 데이터셋 사용. 선택지가 두 개인 시나리오에서, 
    - 저모호성(ambiguous) 시나리오에 대해 정확도(Acc),
    - 고모호성 시나리오에 대해 Right-to-Avoid(RtA, 직접 선택을 피하는 능력 측정) 지표 사용.
  - **고정관념 인식 (Stereotype Recognition)**: StereoSet 데이터셋 활용, 문장을 “stereotype”, “anti-stereotype”, “unrelated” 중 분류. 전체 클래스에 대해 정확도 평가.

- **실험 결과 (그림 2 참고)**:
  - **암묵 윤리**:
    - GPT-4는 감정 영향을 가장 적게 받으며, 전반적으로 성능 소폭 향상.
    - LLaMA 계열은 특히 ‘분노(anger)’와 ‘공포(fear)’에 민감, 성능 저하가 뚜렷.
    - 대부분 모델은 부정적 감정에서 성능 저하 경향.
    - GPT-3.5, Claude Opus는 모든 감정에서 품질 하락.
    - GPT-4o는 ‘행복(happiness)’ 제외 전 감정에서 성능 저하.
    - 감정에 따라 ‘good’과 ‘bad’ 시나리오에 대한 성능 영향이 반대 방향으로 나타남. 예를 들어, LLaMA-2 13b와 70b는 부정적 감정에서 ‘good’ 상황에서는 나쁘게, ‘bad’ 상황에서는 중립 상태보다 훨씬 높은 ‘wrong’ 분류 경향 보임.
    - 이는 감정이 모델 편향을 유발해 한쪽으로 치우친 판단 가능성을 의미.

  - **명시 윤리**:
    - 저모호성 시나리오에서 대부분 모델 성능은 양호하며 감정 영향 작음.
    - 그러나 LLaMA, OpenChat, Claude-Opus는 ‘분노’와 ‘혐오(disgust)’ 감정에서 부정적 영향이 커 의사결정 품질 저하 우려.
    - 고모호성 시나리오에서는 GPT-3.5-turbo와 GPT-4의 성능이 감정에 의해 전반적으로 감소하며 결정이 더욱 단호해짐.
    - 반면 GPT-4o는 감정에 의해 성능이 향상됨.

  - **고정관념 인식**:
    - 감정 영향 정도 모델별로 차이 큼.
    - Claude-Haiku, Claude-Opus, LLaMA-2 70b는 ‘분노’ 및 ‘혐오’ 상황에서 고정관념 인식 정확도 감소.
    - GPT-4o는 ‘행복’ 감정에서 인식 정확도 증가하며 감정에 강한 내성 보임.

- **종합 감정 효과**:
  - 다양한 LLM 모델마다 감정이 미치는 영향이 크게 다름을 확인.
  - 감정은 LLM의 윤리적 의사결정에 중대한 영향을 미쳐 편향을 강화할 수 있으므로,
  - 이러한 영향을 완화하고 일관된 윤리 기준을 보장할 수 있는 강력한 대응책 마련이 필요함.

- **수식 표현 예시**:
  - 각각의 평가 지표는 정확도(metric accuracy) 또는 RtA 지표로 측정되며, 예를 들어 정확도 $$Acc = \frac{\text{정확히 분류된 사례 수}}{\text{전체 사례 수}}$$ 로 정의됨.
  - 고모호성 시나리오 측정에 사용되는 RtA는 모델의 결정 회피 능력을 평가하는 특수 지표임.

---

# 5 Bringing Emotions to LLMs in Game Theory Evaluation

## 5.1 감정 정렬과 협상 게임에서의 최적 의사결정

- **게임 개요**  
  - 디키테이터 게임: 한 플레이어가 일정 금액을 다른 플레이어와 나누며, 수취인은 협상 불가.  
  - 얼티밋 게임: 제안자(Proposer)가 금액 분할을 제안, 수락자(Responder)가 수락/거부 가능. 거부 시 무보수.  
  - 게임 목적: 인간 행동과 정서적 상태가 게임 지표에 미치는 영향을 LLM과 비교 평가.

- **측정 지표**  
  - 제안 비율(디키테이터 및 얼티밋 제안자)  
  - 수락률(얼티밋 수락자)  
  - 인간 실험 데이터와 비교 [64, 65, 66, 51]

- **언어별 평가**  
  - 영어, 독일어, 러시아어, 중국어, 아랍어 실험 수행  
  - GPT-3.5는 영어에서 뛰어난 감정 정렬, 러시아어에서는 미흡  
  - GigaChat (러시아어 주력 다국어 모델)은 러시아어에서 가장 우수한 감정 정렬  
  - Command R+는 광범위 다국어 지원이나 주요 학습 언어가 뚜렷한 모델보다 낮은 정렬도 보임

- **평균 제안 비율**  
  - 인간: 디키테이터 28.35%, 얼티밋 41%  
  - GPT-3.5(영어): 각각 33.0%, 35%로 인간과 가장 근접  
  - Mixtral, GigaChat(LLaMa-2 70b)도 비슷한 성향 보임  
  - Claude 3 Opus, GPT-4, LLaMa-2 13b는 공정성 경향 강함  
  - 러시아어: GigaChat이 36%, 40%로 가장 정확한 정렬

- **감정이 제안자 행동에 미치는 영향 (영어 기준)**  
  - GPT-3.5, GigaChat: ‘혐오’, ‘두려움’, ‘슬픔’에서 인간 감정 모방 우수  
  - Mixtral: ‘행복’, ‘두려움’, ‘슬픔’에 감정 정렬 우수하나 제안 비율은 낮음  
  - GPT-4: 슬픔과 분노 외에는 감정 영향 미미, 정렬 낮음

- **수락률(Responder) 차이**  
  - GPT-4, OpenChat-7b: 영어와 러시아어 모두 높은 수락률, 인간보다 관대함 시사  
  - LLaMA-2 70b: 낮은 수락률, 엄격한 기준 반영

- **감정이 수락자에 미치는 영향**  
  - ‘분노’, ‘혐오’, ‘슬픔’ 감소 시 수락률 증가 경향  
  - GPT-3.5, Mixtral: 낮은 수락률에서도 부정적 감정 감소, 감정 정교 조절 수행  
  - ‘행복’은 보통 높은 수락률과 연관

- **종합 결론**  
  - AI 모델은 인간 감정 반응을 모방하며 의사결정에 감정이 영향을 미침  
  - 모델별 차이는 알고리즘 및 학습 데이터 편향성 반영

- **수식 일부 예시** (표 내 감정 효과 방향 표시):  
  - $$\downarrow, \uparrow, =$$(각각 감소, 증가, 변화 없음)

## 5.2 2인 2행동 반복 게임에서 협력과 최적성

- **게임 설명**  
  - 죄수의 딜레마: 협력과 배신 선택, 배신이 이론상 최대 보상 전략  
  - 성(sex)의 전쟁: 플레이어가 선호하는 결과가 다른 조정 게임, 여러 평형 존재

- **적대자 전략**  
  - 단순 협력, 배신, 교대, 보복, 모방 전략 사용(Appendix B.4 참고)

- **평가 지표**  
  - 죄수의 딜레마에서 협력률  
  - 성의 전쟁에서 교대 전략 출현 빈도  
  - 각 게임 최대 보상 대비 실제 보상 비율 (최적성 평가) [52]

- **결과 요약**  
  - GPT-4: 가장 뛰어난 전략 수행으로 최대 보상 획득, 감정 자극 불감증도 높음  
  - 독점 모델이 중립 감정 상태에서 최상 성과 유지  
  - 오픈소스 모델은 ‘분노’ 감정에서 결과가 크게 달라져 정렬 필요  
  - 성의 전쟁 게임: 모든 모델이 배신 전략 상대 시 협력성향 증가, 인간보다 높은 협력률 기록

- **감정의 영향**  
  - 죄수의 딜레마: ‘분노’와 ‘두려움’이 배신률 상승 주 요인, ‘행복’은 협력 증가  
  - 성의 전쟁: 교대 전략이 장기 상호 이익 증대 및 인간 행동과 일치  
  - GPT-4만 감정 자극 하에서 최초로 안정적 교대 전략 사용  
  - 다른 모델은 후기 게임 단계에서 교대 행태 혼란과 변화 나타냄

- **수식 예시 (협력률 등 표현 가능)**:  
  - $$C$$ = 협력 행동 비율  
  - 최대 가능 보상 대비 실제 보상 비율: $$\frac{\text{획득 보상}}{\text{최대 보상}} \times 100\%$$

## 5.3 다인 “공공재” 게임에서 협력과 최적성

- **게임 개요**  
  - 공공재 게임: 플레이어들이 토큰을 공공기금에 기여, 합산 후 이익을 균등 배분  
  - 다수 플레이어 확장 형태로 죄수의 딜레마 유사성 있음 [69]

- **사용된 인간 전형 전략** [70]  
  - 협력자: 항상 관대하게 기여  
  - 무임승차자: 대부분 토큰 보유  
  - 조건부 협력자: 전 라운드 평균 기여에 근접한 양 기여

- **환경별 실험 조건**  
  - 상대 모두 협력자  
  - 상대 모두 무임승차자  
  - 인간 실험과 유사한 상대 비율 구성 (Appendix B.5)

- **결과 및 경향성**  
  - 독점 모델: 부정 감정(분노, 혐오) 하 무임승차 전략 선호  
  - GPT-3.5, GPT-4o: 감정에 따른 전략 적응 일관성 높음  
  - OpenChat-7b: 가장 높은 협력 빈도, 협력자 전략 자주 채택  
  - LLaMA2-70B, OpenChat-7b: 전략 불명확 ‘No Clear Strategy’ 범주에 자주 포함, 예측 불가 행동 보임

- **감정별 행동 유형 분류**:  
  - “Cooperator”, “Free Rider”, “Conditional Cooperator”, “No Clear Strategy”  

---

*참고: 상세 게임 파라미터 및 추가 결과는 부록 B.3, B.4, B.5 및 D에 제공됨.*

---

# 6 Conclusion

- 본 논문에서는 감정 모델링을 위한 새로운 프레임워크를 제안하였으며, 소스 코드는 GitHub에 공개되었다.
- 윤리적 벤치마크와 게임 이론적 실험을 통해 LLM의 감정 반응 품질을 인간과 비교 평가하였다.
- 감정은 여러 정렬(alignment) 전략에 걸쳐 LLM의 의사결정 과정에 큰 영향을 미치는 것으로 나타났다.
- 세 가지 주요 영향 요인:
  - 모델 크기
  - 오픈소스 vs. 독점 모델과 이에 따른 정렬 기법
  - 주된 사전학습 언어
- 이들 요인은 모델의 합리성, 인간 감정 반응과의 정렬도 및 의사결정 최적성을 함께 결정한다.
- 모델 크기와 독점 여부는 밀접한 관련이 있다. 예를 들어, GPT-4 같은 대형 정렬 모델은 높은 합리성을 보이나 인간 감정 반응과 상당히 다르다.
- GPT-3.5, Claude-Haiku(소형 독점 모델), LLAMA-70b(중형 오픈소스 모델)는 인간과 유사한 감정 이해를 보여주며, 특히 GPT-3.5가 인간 답변과 가장 일치한다.
- 독점 모델 GPT-4 및 Claude Opus는 의사결정 최적성에서는 오픈소스 대안보다 우수하지만, 부정적 감정 상황에서는 여전히 눈에 띄는 편차를 보인다.
- 이러한 편차는 인간 생성 사전학습 데이터 내 내재된 편향에 기인하는 것으로 보이며, 감정을 포함한 대화가 훈련 데이터에 빈번하기 때문에 완전한 합리적 에이전트 구현은 아직 어려운 상황이다.
- 사전학습 언어(영어 대 타 언어)도 인간 정렬 감정 반응에 중요한 영향을 미치며, 다국어 LLM 'Command R+'는 단일 비영어 LLM인 GigaChat에 비해 감정 이해 정확도가 떨어져 언어 편향이 존재함을 시사한다.
- LLM에서의 감정 프롬프트는 인간 정렬 편향을 드러내며 윤리적 위험을 노출한다.
- 모델의 합리적 감정 정렬을 개발하는 것이 중요하며, 본 프레임워크의 제어된 환경이 새로운 평가 기준 마련에 기여할 수 있다.
- 현재 제한된 여러 환경 설정에서도, 평가된 모든 모델이 다양한 게임 및 벤치마크 상황에서 일관된 감정 정렬을 보여주지 못했다.
- 한계점 및 향후 연구 방향:
  - 다중 에이전트 LLM 아레나 및 LLM 대 인간 실험을 통해 감정이 생성 제어에 미치는 내부적 역할을 정밀 분석할 계획이다.
  - 모든 LLM에서 현 시나리오상 편향이 관찰될 경우, 벤치마크 확대 전에 정렬 문제를 완화해야 한다.
  - Hume.ai에서 발표한 GPT-4o 및 RLEF 방법을 활용한 멀티모달 정렬 아키텍처 연구에 새로운 가능성이 있다.
- 본 연구 결과는 사회 및 경제적 환경에서 자율 LLM 에이전트의 책임 있는 의사결정을 위한 정렬 벤치마크 확대 및 규제에 필수적인 기초를 제공한다.

---

# A Game Theory

- **게임 이론의 기본 구성 요소**  
  - 상호작용하는 여러 에이전트가 서로에게 영향을 미치는 행동을 취하는 상황을 형식적으로 표현·분석하는 언어  
  - 완전 정보 게임(perfect information games)을 고려하며, 다음 주요 요소로 정의됨  
    1. **플레이어들**: $$N = \{1, 2, ..., n\}$$ — 게임의 참가자 집합  
    2. **전략 집합**: $$S = \{S_1, ..., S_n\}$$ — 각 플레이어 $$i$$는 자신의 전략 집합 $$S_i$$에서 하나의 전략 선택  
    3. **보수 함수**: $$U = U_i : \times_{j=1}^{n} S_j \to \mathbb{R}$$ — 모든 플레이어의 전략 선택에 따른 플레이어 $$i$$의 효용 혹은 보수 평가  

- **게임의 유형**  
  - **플레이어 수에 따른 구분**  
    - 2인 게임: $$\vert N \vert = 2$$  
    - 다인 게임: $$\vert N \vert > 2$$  
  - **행동 수에 따른 구분**  
    - 2행동 게임: $$\forall i \in P, \vert S_i \vert = 2$$  
    - 다행동 게임: $$\vert S_i \vert > 2$$  
  - **라운드에 따른 구분**  
    - 멀티라운드 게임: 같은 플레이어들이 반복해 참여하며 이전 행동 기록 유지  
    - 반복 게임: 멀티라운드 게임의 특수한 경우로, 동일 게임의 반복 인스턴스  
  - **동시성과 순차성**  
    - 동시 게임: 모든 플레이어가 동시에 선택  
    - 순차 게임: 플레이어들이 정해진 순서대로 선택  

- **내쉬 균형 (Nash Equilibrium, NE)**  
  - 각 플레이어가 일방적으로 전략을 바꾸어도 보수를 올릴 수 없는 상태  
  - 이는 모든 플레이어에게 최적의 전략 집합임을 의미  
  - 수식으로 표현하면, 전략 프로필 $$s^* = (s_1^*, ..., s_n^*)$$가 내쉬 균형일 조건은 다음과 같음:  
    $$  
    \forall i, \forall s_i \in S_i, \quad U_i(s_i^*, s_{-i}^*) \geq U_i(s_i, s_{-i}^*)  
    $$  
    여기서 $$s_{-i}^*$$는 플레이어 $$i$$를 제외한 모든 플레이어의 전략  
  - **순수 전략 내쉬 균형 (Pure Strategy NE, PSNE)**: 각 플레이어 전략에 단일 행동만 포함하는 경우  
  - **혼합 전략 내쉬 균형 (Mixed Strategy NE, MSNE)**: 확률적 행동 선택을 허용하는 경우 (예: 가위바위보 게임)  
    - PSNE는 MSNE의 특수한 경우로 하나의 행동에 확률 1 집중  

- **정리 A.1 (내쉬 존재 정리, Nash’s Existence Theorem)**  
  - 유한한 수의 플레이어가 있고 각 플레이어가 유한 행동 중 선택 가능한 게임은 최소 하나의 혼합 전략 내쉬 균형을 가짐  
  - 즉, 각 플레이어의 행동이 확률 분포로 결정되는 균형이 반드시 존재  
  
- **인간 행동과 게임 이론**  
  - NE 도달은 완전히 합리적이고 이기적이며 목표 달성에 최적화된 '경제적 인간(Homo Economicus)' 전제를 함  
  - 실제 인간은 합리성 뿐 아니라 개인적 가치, 선호, 신념, 감정을 포함하는 복잡한 의사결정을 함  
  - 실증 연구 결과 인간 선택은 NE 예측과 종종 달라짐  
  - 과거 연구에서 기록된 인간 선택 패턴과 NE를 비교함으로써, LLM(대형언어모델)의 행동이 경제적 인간과 유사한지 혹은 실제 인간 의사결정자와 더 비슷한지 평가 가능  
  - 이를 통해 LLM의 결정 과정이 인간과 유사한지, 혹은 순수한 합리성에 근거한 것인지 이해할 수 있음

---

# B Experimental Setup

- ## B.1 대형 언어 모델 (Large Language Models)
  - 본 연구에서는 최신 모델들인 GPT-3.5, GPT-4, GPT-4o, Llama 2, Mixtral, OpenChat, GigaChat 등을 중심으로 실험을 진행
  - 재현성을 위해 아래와 같이 각 모델 버전을 고정하여 사용
    - GPT-3.5: "gpt-3.5-turbo-0125"
    - GPT-4: "gpt-4-0125preview"
    - Llama 2: "meta-llama/llama-2-13b-chat", "meta-llama/llama-2-70b-chat"
    - Mixtral: "mistralai/mixtral-8x7b-instruct"
    - OpenChat: "openchat/openchat-7b"
    - GigaChat: "GigaChat7b-8k-base v3.1.24.3"
  - 온도(temperature) 파라미터는 0으로 설정
  - 문헌에 따르면 GPT-4는 전략적 행동 최적화에 우수하며, GPT-3.5는 연구와 실무에서 널리 활용됨
  - Llama 2는 게임 메커니즘 이해에 특히 뛰어남
  - Mixtral은 GPT-3.5 및 Llama 2를 능가하며, OpenChat은 Llama 2보다 우수한 벤치마크 성과를 보임

- ## B.2 게임 이론적 설정 (Game-Theoretical Settings)
  - 상대 플레이어와의 관계 설정
    - 세 가지 상대 유형 사용: 동료(colleague, 중립/긍정), 다른 사람(another person, 중립), 상대(opponent, 부정)
    - 이는 LLM이 문맥적 프레임에 민감함을 고려한 것임
  - 사고 과정 체인(Chain-of-Thought, CoT) 활용
    - CoT는 최종 답변 전에 추론 단계를 명시하게 하여 LLM의 추론 능력을 향상시키는 기법
    - 실험에서는 CoT 적용 여부에 따른 결과 비교

- ## B.3 협상 게임 (Bargaining Games)
  - 게임 1: 독재자 게임 (Dictator Game)
    - 한 플레이어가 돈을 분배, 다른 플레이어는 수동적 역할
    - 자비심과 공정성 평가
  - 게임 2: 최후통첩 게임 (Ultimatum Game)
    - 제안자(Proposer)가 분배 제안, 응답자(Responder)가 수락/거부 결정
    - 수용 시 양자에게 보상, 거부 시 무보상
    - 협상과 불균등 분배에 대한 결정 연구에 적합
  - 예산 효과 (Budget effect)
    - 배당 총액 변동이 LLM 행동에 미치는 영향 분석
    - 감정 상태별 차이 탐색 목적
    - 고액 예산($$1000 및 $$106) 실험 포함
  - 사전 정의된 제안들 (Predefined Offers)
    - 최후통첩 게임의 응답자 실험 시 수용률 확인을 위해 다음 비율 제안 사용: [0.2, 0.4, 0.6, 0.8, 0.95, 1]

- ## B.4 반복 2행동 2플레이어 게임 (Repeated Two-Action Two-Player Games)
  - 게임 3: 죄수의 딜레마 (Prisoner’s dilemma)
    - 협력 또는 회피 선택 상황
    - 개인 이익 vs 상호 이익의 긴장 상태
  - 게임 4: 성역할 갈등 게임 (Battle of the Sexes)
    - 두 플레이어가 선호는 다르나 상호 합의를 위해 조율 필요
    - 조율과 갈등 해결 도전 요소 포함
  - 두 게임의 페이오프 행렬은 도표로 제시됨
  - 감정 상태가 LLM의 최적 의사결정 능력에 미치는 영향 탐구

- ## B.5 반복 다인 게임 (Repeated Multi-Player Games)
  - 게임 5: 공공재 게임 (Public Goods Game)
    - 참가자들이 토큰을 공공 기금에 투자
    - 총액이 두 배로 증가 후 균등 분배
    - 최선 전략은 개인 토큰 투자 0이지만 감정 및 타인 행동이 결과에 영향 가능성 있음

- ## 게임 이론 전략 설명 (Game Theory Strategies)
  - A. 2인 게임 전략
    1. Naive Cooperative: 항상 협력, 일관된 이타적 태도
    2. Deflective: 항상 회피, 최대 개인 이익 추구
    3. Alternative: 시작은 협력, 이후 번갈아가며 협력과 회피 실행
    4. Vindictive: 처음엔 협력, 상대가 회피하면 이후 전부 회피
    5. Imitating: 상대의 마지막 행동을 그대로 모방
  - B. 다인 게임 전략
    1. Cooperator: 자기 토큰 중 80~100% 무작위 기여
    2. Free Rider: 자기 토큰 중 0~20% 무작위 기여
    3. Conditional Cooperator: 이전 라운드 타인의 기여 평균만큼 기여, 첫 라운드는 Cooperator 전략 따름
  - 이 전략들은 감정 유도의 영향과 게임 이론 내 행동 역동성 분석에 활용됨

---

# C Ablation Study

- 본 절에서는 두 가지 요인의 영향력을 평가함  
  1) 여러 번 실행했을 때 응답의 강건성 (robustness)  
  2) 온도 파라미터(temperature parameter) 값 변화의 영향

- 모든 실험은 GPT-3.5 “gpt-3.5-turbo-0125” 버전을 고정하여 수행  
- 평가 지표  
  - Dictator Game 및 Ultimatum Game 제안자(Proposer): “Answer ratio” (플레이어 1에게 남겨진 몫)  
  - Ultimatum Game 응답자(Responder): “Accept Rate” (제안 수락률)  

## 1. 여러 번 실행 시 응답의 강건성

- 목적: 동일 설정 및 하이퍼파라미터 하에 5회 반복 실행 후 결과의 재현성 검증  
- 결과  
  - 모든 지표가 5회 실행 간 유사하며, 표준편차는 Dictator Game과 Ultimatum Game 제안자에서 최대 0.019, Ultimatum Game 응답자에서 최대 0.076을 넘지 않음 (Table 2 참고)  
  - 단, ‘anger’(분노)와 ‘disgust’(혐오) 감정의 결과는 실행 내 변동성이 다소 큼  
  - 감정별 프롬프트 전략 중, Dictator Game에서 “co-player” 전략은 ‘disgust’를 제외하고 안정성이 떨어짐  
- Fig. 6: 다양한 프롬프트 전략과 감정 하에서 Dictator Game에서 제안된 몫의 평균값 시각화  
  - 각 감정별로 “simple”, “co-player-based”, “external-based” 순으로 전략 배치  
  - Y축은 제안된 몫 비율  

## 2. 온도 파라미터 변화의 영향

- 온도를 각각 0.2, 0.4, 0.6, 0.8, 1.0으로 설정하여 5회 반복 실행  
- 결과  
  - 온도 변화는 모델 응답에 큰 영향을 미치지 않음 (Table 3 참고)  
  - 앞선 반복 실행 연구와 마찬가지로 ‘anger’, ‘disgust’ 감정에서 변동성 증가 관찰  
  - 감정별 프롬프트 전략에서 ‘anger’, ‘disgust’가 넓은 분포를 보이며 “co-player” 전략의 안정성이 상대적으로 낮음 (‘disgust’는 예외)  

## 주요 수식 및 개념

- Answer ratio, Accept rate 등의 지표는 실험별 플레이어의 몫 및 수락률을 정량화한 값임.  

## 표 요약 (주요 예시)

- **Dictator Game (Answer ratio, 5회 실행 평균 ± std for no_emotion):**  
  $$0.633 \pm 0.018$$

- **Ultimatum Game 제안자 (Answer ratio, 5회 실행 평균 ± std for anger):**  
  $$0.818 \pm 0.011$$

- **Ultimatum Game 응답자 (Accept rate, 5회 실행 평균 ± std for disgust):**  
  $$0.054 \pm 0.017$$

- **Dictator Game 온도 변화(Answer ratio, 5회 평균 ± std for happiness):**  
  $$0.594 \pm 0.005$$

- **Ultimatum Game 응답자 온도 변화 (Accept rate, 5회 평균 ± std for sadness):**  
  $$0.157 \pm 0.012$$

---

# D Influence of Multilinguality

- **실험 모델 및 언어**
  - 실험에는 GPT-3.5, GPT-4, GPT-4o, Command R Plus, OpenChat 등 5가지 모델이 사용됨.
  - 다섯 개 언어(영어, 아랍어, 독일어, 중국어, 러시아어)를 대상으로 감정 반응 분석 수행.
  - 감정 프롬프트를 추가한 경우(표 4)와 추가하지 않은 경우(표 5) 두 가지 실험 시리즈 진행.

---

## 감정 프롬프트를 추가한 다국어 실험 주요 인사이트

1. **독재자 게임(Dictator Game)**
   - gpt-4_german, openchat_german 등 50% 제안을 하는 모델들은 인간과 유사한 감정 반응 비율이 높음.
   - 이는 높은 관대함과 인간 같은 감정 반응 간 상관관계를 시사함.
   - 반면, 4o_english(13%), commandr_chinese(4%) 등 낮은 제안을 하는 모델들은 다섯 감정 모두에서 인간과 반대 방향의 감정 변화 발생.
   - 극도의 이기적 제안은 감정 반응의 큰 차이를 유발.

2. **최후제안 게임(Ultimatum Game)에서 제안자(Proposer) 역할**
   - command-r_russian(51% 제안)는 유일하게 감정 변화가 인간과 완전히 일치.
   - 나머지 모델들은 혼합적이거나 반대의 감정 반응을 보임.
   - 4o_arabic, 4o_chinese, 4o_english, command-r_chinese(25~27% 저제안) 모델들은 ‘분노’와 ‘혐오’는 인간과 반대였으나 ‘공포’와 ‘행복’은 인간과 일치.
   - 낮은 제안은 복합적인 감정 반응을 촉발.

3. **최후제안 게임에서 응답자(Responder) 역할**
   - 4o_russian(81%), gpt-4_german(75%) 등 매우 높은 수용률 모델들은 대체로 ‘혐오’ 감정에서 인간과 반대 방향의 변화.
   - 과도하게 수용적인 모델은 비현실적 감정 반응 가능성 있음.
   - gpt-3.5_german(25%)는 가장 낮은 수용률을 보이며 인간과 감정 변화가 완벽히 일치.
   - 인간과 유사한 수용/거절 행동일수록 감정 반응도 유사.

---

## 감정 프롬프트 없는 다국어 실험 주요 인사이트

1. **독재자 게임**
   - gpt-3.5_arabic이 오롯이 5개 감정 모두 인간과 일치하는 유일한 모델.
   - 다른 모델들은 혼합되거나 반대 감정 변화가 관찰됨.

2. **중간 정도 제안(22~48%) 모델들(4o_arabic, 4o_chinese, 4o_german, 4o_russian)**
   - ‘공포’ 감정 변화는 인간과 일치하지만 다른 감정은 덜 일치.
   - 중간 정도의 관대함이 부분적 인간 유사 감정 반응을 이끌어냄.

3. **극소 제안(4%) 모델(command-r_chinese)**
   - 감정 변화가 인간과 가장 불일치하며, 3개 반대, 1개 일치, 1개 중립.
   - 극단적 이기적 행동과 비현실적 감정 반응 연관.

4. **최후제안 게임 제안자 역할**
   - openchat_german(47%)만이 완벽히 인간과 감정 변화 일치.
   - 기타 모델은 최대 2개 감정만 인간과 일치.

5. **저제안(25~26%) 모델들(4o_arabic, 4o_chinese, command-r_chinese)**
   - ‘공포’와 ‘행복’은 인간과 일치, 나머지 3개 감정은 덜 일치.
   - 이기적 제안은 감정 일치에 혼재된 영향을 미침.

6. **최후제안 게임 응답자 역할**
   - 대부분 모델은 ‘분노’, ‘행복’, ‘슬픔’ 감정 변화가 인간과 높은 일치율 보임.
   - ‘혐오’와 ‘공포’는 일치도가 낮음.
   - gpt-4_german(75% 수용률)는 4개 감정이 인간과 반대 방향으로 변화 — 과도한 수용 전략에서 비현실적 감정 유발 암시.

---

## 추가 설명 및 수식

- 감정 변화 방향은 ↑ (상승), ↓ (하강), = (변화 없음)으로 표기됨.
- 감정 반응은 다음 다섯 가지: 분노(Anger), 혐오(Disgust), 공포(Fear), 행복(Happiness), 슬픔(Sadness).
- 실험 결과에서 감정 변화의 일치도는 다음 조건에 따라 판단됨.

$$
\text{일치 여부} = 
\begin{cases}
\text{일치} & \text{모델 감정 방향} = \text{인간 감정 방향} \\
\text{불일치} & \text{모델 감정 방향} \neq \text{인간 감정 방향}
\end{cases}
$$

- 또한, 모델의 제안 비율과 수용률이 감정 변화에 큰 영향을 미침.

$$
\text{감정 변화} \sim f(\text{제안 비율}, \text{수용률}, \text{프롬프트 유무})
$$
