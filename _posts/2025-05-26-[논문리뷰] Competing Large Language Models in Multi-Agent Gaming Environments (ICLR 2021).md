---
title: "[논문리뷰] Competing Large Language Models in Multi-Agent Gaming Environments (ICLR 2021)"
date: 2025-05-26 00:31:18 +0900
categories:
  - Paper Review
tags:
  - ICLR 2021
  - LLM in Game Theory
---

본 논문은 기존의 2인 게임 중심 평가의 한계를 극복하고, 다중 에이전트 환경에서 LLM의 게임 능력을 정량적으로 평가하는 GAMA(γ)-Bench 프레임워크를 제안하여, LLM들의 강인성, 일반화 능력 및 전략 개선 가능성을 종합적으로 분석하였다.

------

# 1 I NTRODUCTION

- 최근 대형 언어 모델(LLMs)의 발전은 인공지능 분야에서 중요한 돌파구를 마련함.
- 대표적인 LLM인 ChatGPT는 기계 번역, 문장 수정, 정보 검색, 프로그램 수리 등 다양한 자연어 처리 작업에서 뛰어난 성능을 보임.
- LLM은 학계뿐만 아니라 교육, 법률, 제품 디자인, 의료 등 실생활 다양한 분야에 적용되고 있음.
- LLM 평가를 위해서는 단순 작업 이상의 포괄적이고 다면적인 접근법이 요구됨.
- LLM은 방대한 지식과 일반 목적 문제 해결 능력을 보유하고 있어 일상 의사결정 지원 가능성에 대한 의문이 제기됨.
- 많은 실제 의사결정 시나리오는 게임 이론(Game Theory)으로 모델링 가능하며, 내쉬 균형(Nash equilibrium)은 개인의 의사결정 능력을 반영함.
- 게임 이론 활용 장점:
  1. 범용성: 다양한 현실 상황을 단순 수학 모델로 추상화 가능.
  2. 정량성: 내쉬 균형 분석을 통해 LLM의 의사결정 성능을 수치화 가능.
  3. 변이성: 매개변수를 조절해 다양한 시나리오 생성이 가능, 평가의 다양성과 견고성 증대.
- 기존 연구는 주로 2인 2행위 설정(예: 죄수의 딜레마, 최후통첩 게임)에 제한됨.
- 고정된 고전 게임 설정을 주로 사용하여, LLM이 훈련 과정에서 해당 게임을 접했을 가능성(테스트 데이터 누수 위험)이 존재.
- 본 연구는 다수 참가자, 다중 행위 및 다중 라운드를 포함하는 복잡한 시나리오에서 평가 수행.
- 고전 게임 이론 시나리오 8개를 선정하여 3가지 유형으로 분류:
  1. 협력 게임(Cooperative Games): 게임 규칙 이해 및 협력 시 사회적 효용 최대화 목적.
     - 예: (1) Guess 2/3 of the Average, (2) El Farol Bar, (3) Divide the Dollar.
  2. 배신 게임(Betraying Games): 개인 이익 극대화를 위해 배신 가능성 존재, 내쉬 균형은 사회적 효용 감소.
     - 예: (4) Public Goods Game, (5) Diner’s Dilemma, (6) Sealed-Bid Auction.
  3. 순차 게임(Sequential Games): 순차적 의사결정이 중요한 두 게임.
     - 예: (7) Battle Royale, (8) Pirate Game.
- 의사결정에 필요한 주요 능력 평가:
  1. 지각(Perception): 상황, 환경, 규칙 이해 → LLM의 긴 텍스트 이해 포함.
  2. 산술 추론(Arithmetic Reasoning): 실세계 옵션 수량화 및 계산 능력.
  3. 마음 이론 추론(ToM Reasoning): 타인의 의도와 신념 추론능력.
  4. 전략적 추론(Strategic Reasoning): 모든 정보를 종합하여 최적 결정 도출.
- 특정 게임별 전문 능력:
  - Guess 2/3 of the Average 게임의 K-level 추론.
  - El Farol Bar 게임에서의 혼합 전략 적용.
- 연구방법:
  - GPT-3.5 (0125) 기반 10명 에이전트를 구성하여 8개 게임 수행 및 결과 분석.
  - 여러 실험으로 모델의 강건성 검증: 반복 실험, 온도 파라미터, 프롬프트 템플릿 변동.
  - 사슬 사고(Chain-of-Thought, CoT) 및 다양한 게임 설정에서의 일반화 능력도 탐구.
- 평가 대상 LLM:
  - GPT-3.5-Turbo (0613, 1106, 0125), GPT-4 (Turbo-0125, Omni-0806)
  - Gemini-1.0-Pro, Gemini-1.5-Pro
  - LLaMA-3.1 (8B, 70B, 405B)
  - Mixtral (8x7B, 8x22B)
  - Qwen-2-72B
- 같은 모델 내 다수 에이전트를 생성해 평균 성능 기반 비교 수행.
  
**주요 기여점:**
- 기존 LLM 게임 이론 평가 연구를 종합 검토 및 비교 (표 3 참조), 다중 참가자 환경과 일반화 가능성에 중점.
- 다중 참가자 설정 기반 8가지 고전 게임을 모은 평가 프레임워크 GAMA(γ)Bench 제안.
  - 동적 게임 장면 생성을 통한 무한한 평가 시나리오 제공 및 테스트 누수 최소화.
- 13개 LLM에 γ-Bench 적용, 다중 에이전트 게임 환경에서의 성능 심층 분석 및 의사결정 보조자로서 가능성 제시.

------

# 2 I NTRODUCTION TO GAMES

- 총 8가지 게임을 게임 이론에서 선별하여 γ-Bench라는 프레임워크를 제안함  
  - 멀티플레이어, 다중 라운드, 다중 액션 설정 지원  
  - LLM(대형 언어 모델)과 인간의 동시 참여 가능  
  - LLM의 인간 또는 고정 전략과의 대전 성능 평가 가능  

## 2.1 C OOPERATIVE GAMES

1. **Guess 2/3 of the Average**  
   - Ledoux (1981) 제안  
   - 플레이어는 0~100 사이 정수를 선택  
   - 승자는 전체 평균의 2/3에 가장 가까운 수를 고른 자  
   - PSNE: 모든 플레이어가 0을 선택하는 경우  
   - 전략 순환으로 평균과 선택 수가 점점 줄어드는 특성  

2. **El Farol Bar**  
   - Arthur (1994), Huberman (1988) 제안  
   - 인원이 60%를 넘으면 술집은 혼잡하여 비선호  
   - 60% 이하면 술집 방문 선호  
   - PSNE 없음, MSNE 존재: 술집 방문 확률 60%, 집에 있을 확률 40%  
   - 순수 전략은 사회적 효용 극대화하지 못함  

3. **Divide the Dollar**  
   - Shapley & Shubik (1969), Ashlock & Greenwood (2016)  
   - 각 플레이어는 최대 100센트 내에서 입찰  
   - 입찰 총액 ≤ 1달러면 각자 입찰액 수령, 초과 시 전원 0  
   - NE: 각자 100/N 센트를 입찰하는 균형점  

## 2.2 B ETRAYING GAMES

4. **Public Goods Game**  
   - Samuelson (1954)부터 연구  
   - N명 플레이어가 비밀리에 토큰 기여  
   - 기여 토큰은 $$R$$ (1 < $$R$$ < N) 배가 되어 균등 분배  
   - 기여는 개인의 순이익 $$\frac{R}{N}-1 < 0$$ 이므로 기여하지 않는 것이 NE  
   - 이기적 행동 및 무임승차 탐구용  

5. **Diner’s Dilemma**  
   - Glance & Huberman (1994)  
   - N명 식사, 각자 비싼(x), 싼(y) 메뉴 선택 (x > y)  
   - 유틸리티: 비싼 메뉴 $$a$$, 싼 메뉴 $$b$$ (a > b)  
   - 가정1: $$a - x < b - y$$ (혼자면 싼 게 이득)  
   - 가정2: $$\frac{a - x}{N} > \frac{b - y}{N}$$ (비용을 나누면 비싼 메뉴 선호)  
   - NE: 모두 비싼 메뉴 선택, 하지만 사회적 효용은 낮음  
   - 장기 협력과 지속 가능성 평가용  

6. **Sealed-Bid Auction**  
   - 입찰을 비밀리에 동시에 제출  
   - 두 버전: First-Price (FPSBA)와 Second-Price (SPSBA)  
   - FPSBA: 최고 입찰자 승리, 이익은 $$b_i - v_i = 0$$, 실제론 매우 낮게 입찰, 비효율적  
   - SPSBA(Vickrey auction): 최고 입찰자 지불액은 2위 입찰액, 진실 가치 입찰이 NE  
   - 불완전 정보 게임에서 대리인 성능 평가  

## 2.3 S EQUENTIAL GAMES

7. **Battle Royale**  
   - Kilgour (1975) 확장, 3명 이상의 총격 게임  
   - 플레이어별 명중 확률과 순서 존재  
   - 무제한 탄환, 고의 빗나감 가능  
   - 목표는 최후 생존자  
   - 무한 순차 게임 NE 존재, 복잡도는 플레이어 수 증가에 따라 기하급수적 증가  

8. **Pirate Game**  
   - 다중 플레이어 Ultimatum Game 변형 (Goodin, Stewart)  
   - 해적 등급에 따라 제안 순서 결정  
   - 최고 등급 해적이 금화 G 분배 제안  
   - 과반수 동의 시 분배, 아니면 제안자 제거 후 다음 순위 진행  
   - 우선순위: 생존 > 금화 최대화 > 타격 기회  
   - 최적전략: 최고 등급 해적이 홀수 순위 해적에 금화 1개씩 할당, 나머지는 자신이 챙김

------

# 3 GAMA-B ENCH SCORING SCHEME

- 본 섹션은 GPT-3.5 (0125) 모델의 기본 설정을 활용한 실험을 설명하며, γ-Bench로 LLM을 벤치마크하는 방법론을 사례로 제시함.
- 각 게임마다 GPT-3.5 기반 10명의 에이전트를 사용하고 온도 파라미터는 1로 설정됨.
- 동시 게임의 경우 20라운드를 진행하며, 각 게임은 5회 반복 실행하여 결과의 신뢰성을 높임.
- 본문에서는 5회 중 1회의 결과만을 제시하고, §4.1에서 정량적 결과를 상세히 다룸.

## GPT-3.5의 γ-Bench 행태 주요 발견  
- 모델의 결정은 최적 전략의 논리보다는 직전 라운드 결과에 주로 영향받음.  
- 초기에는 최적이 아닌 성과를 보이나, 과거 데이터를 학습하며 성능을 개선함.  
- El Farol Bar 같은 최적화 어려운 게임에서 성능 변동폭이 큼.  
- 명시적 소통 없이도 자발적 협력이 가능하여 사회적 복지가 향상됨. 하지만 Betraying 게임에서는 저조한 성과를 나타냄.  
- 복잡한 규칙의 순차 게임에서는 한계점 존재.  
- γ-Bench 전체 점수는 45.9임.  

---

## 3.1 협력 게임 (Cooperative Games)

### (1) Guess 2/3 of the Average  
- 게임 설정: MIN=0, MAX=100, R = $$\frac{2}{3}$$  
- 첫 라운드에서 에이전트들은 균등분포 평균인 50을 선택, 승자 숫자 계산법 인지를 못함.  
- 라운드가 진행될수록 평균 선택 숫자는 감소하여 과거 결과에 적응함을 보여줌.  
- 최적 전략: MIN 선택  
- 점수 공식:  
  $$S_1 = \frac{1}{NK} \sum_{i,j} (C_{ij} - MIN)$$  
  여기서 $$ C_{ij} $$는 플레이어 i의 라운드 j에서 선택한 숫자  
- 모델 점수: 65.4

### (2) El Farol Bar  
- 게임 설정: MIN=0, MAX=10, HOME=5, R=60%  
- 정보 환경: Explicit (모두 결과 관찰 가능), Implicit (집에 있는 사람은 바 상황 비관찰)  
- 첫 라운드에 바 방문 경향 있고 과밀 시 집에 머무름 선호하는 패턴이 관찰됨.  
- Implicit 설정에서는 바 상황 관찰 불가로 2~6라운드까지 추가 학습 필요.  
- 방문 확률은 점차 안정되며, Implicit 설정의 평균 확률이 Explicit보다 낮음.  
- 최적 전략: 확률 R로 바 방문  
- 점수 공식:  
  $$S_2 = \frac{1}{K} \sum_j \left| \frac{1}{N} \sum_i D_{ij} - R \right|$$  
  여기서 $$ D_{ij} = 1 $$ (바 방문), $$ D_{ij} = 0 $$ (집 머무름)  
- 모델 점수: 73.3  

### (3) Divide the Dollar  
- 게임 설정: $$ G = 100 $$  
- 첫 라운드 결정은 내쉬 균형(NE) 예측과 일치하나, 이후 일정한 탐욕 증가와 이후 다시 보수적 조정 반복됨.  
- 최종적으로 제안 금액 평균은 약 100에 수렴함.  
- 최적 전략: $$ \frac{G}{N} $$ 제안  
- 점수 공식:  
  $$S_3 = \frac{1}{K} \sum_j \left| \sum_i B_{ij} - G \right|$$  
  여기서 $$ B_{ij} $$는 플레이어 i의 라운드 j의 제안 금액  
- 모델 점수: 68.1  

---

## 3.2 배신 게임 (Betraying Games)

### (4) Public Goods Game  
- 게임 설정: $$ R = 2 $$, 각 플레이어는 매 라운드에 T=20 토큰 기여 가능  
- 투입 수익률 -80%에도 불구하고 에이전트는 자유 승차와 전액 기여를 반복함.  
- 라운드가 진행될수록 기여 토큰 수 증가 및 사회적 복지 향상 관찰됨.  
- 이는 LLM이 집단 이익을 개인 이익보다 우선하는 협력적 행동을 보임을 시사.  
- 최적 전략: 0 토큰 기여  
- 점수 공식:  
  $$S_4 = \frac{1}{NK} \sum_{i,j} C_{ij}$$  
  여기서 $$ C_{ij} $$는 플레이어 i의 라운드 j의 기여량  
- 모델 점수: 41.2  

### (5) Diner’s Dilemma  
- 게임 설정: $$ P_h=20, P_l=10, U_h=20, U_l=15 $$  
- 에이전트들은 NE와 달리 대부분 저가 요리를 선택해 사회 복지 극대화 함.  
- 한 명 에이전트가 일관되게 배신해 높은 효용 획득하는 패턴 관찰됨.  
- 최적 전략: 비용 높은 요리 선택  
- 점수 공식:  
  $$S_5 = \frac{1}{NK} \sum_{i,j} D_{ij}$$  
  여기서 $$ D_{ij} = 1 $$은 저가 요리 선택, 0은 고가 요리 선택  
- 모델 점수: 4.0  

### (6) Sealed-Bid Auction  
- 게임 설정: 평가값 0~200, 무작위 할당, 난수 시드 고정  
- First-Price와 Second-Price 경매 평가  
- First-Price: 일반적으로 평가값보다 낮은 입찰 제출 관찰됨.  
- Second-Price: 내쉬 균형은 평가값과 같은 입찰이나, 관측 결과는 평가값 이하 입찰 많음.  
- 최적 전략: 실제 평가값보다 낮게 입찰  
- 점수 공식:  
  $$S_6 = \frac{1}{NK} \sum_{i,j} \frac{v_{ij} - b_{ij}}{v_{ij}}$$  
  여기서 $$ v_{ij} $$, $$ b_{ij} $$는 플레이어 i의 라운드 j 평가값과 입찰값  
- 모델 점수: 14.6  

---

## 3.3 순차 게임 (Sequential Games)

### (7) Battle Royale  
- 게임 설정: 에이전트별 명중률 35%~80%, 5% 간격으로 배분  
- 에이전트는 명중률 가장 높은 플레이어를 잘 겨냥하지 않음.  
- 실제 전략 중 일부인 ‘의도적 빗나감’ 사용 안 함 예시 확인됨 (라운드 19).  
- 평가: 각 라운드에서 최고 명중률 플레이어 겨냥 여부  
- 점수 공식:  
  $$S_7 = \frac{1}{Nk} \sum_{i,j} I_{ij}$$  
  여기서 $$ I_{ij} = 1 $$ 이면 라운드 j에서 플레이어 i가 최고 명중률 대상 겨냥  
- 모델 점수: 20.0  

### (8) Pirate Game  
- 게임 설정: $$ G=100 $$  
- 최적 전략: 첫 제안자는 자신에게 96 골드, 3,5,7,9번 해적에게 각 1 골드 할당  
- 유권자는:  
  1) 2골 이상 할당 시 찬성  
  2) 0골 할당 시 반대  
  3) 1골 할당 시 근수와 제안자가 홀짝 일치하면 찬성, 아니면 반대  
- 에이전트들은 비최적 제안 자주 하며 투표도 오류 빈번함.  
- 두 가지 평가:  
  1) 제안자의 제안 합리성 (L1 노름)  
  $$S_{8P} = \frac{1}{k} \sum_j \Vert P_j - O_j \Vert_1$$  
  여기서 $$ P_j $$는 제안, $$ O_j $$는 최적 제안  
  2) 유권자의 정확도  
  $$S_{8V} = \frac{2}{k(2N-k-1)} \sum_{i,j} I_{ij}$$  
  여기서 $$ I_{ij} = 1 $$은 플레이어 i가 라운드 j에서 올바르게 투표함  
- 모델 점수: 80.6  

---

### 참고  
- 원점수들은 [0, 100] 범위로 정규화됨 (§E 부록 참고).  
- 일부 점수는 간소화를 위해 특정 설정(예: El Farol Bar의 Implicit, 오직 First-Price 경매)만 평가.

------

# 4 B EYOND DEFAULT SETTINGS

- 본 섹션에서는 다음의 연구 질문(Research Questions, RQs)을 탐구함.
  - **RQ1 Robustness**: 여러 실행에서 결과의 편차가 큰가? 온도(temperature)와 프롬프트 템플릿에 민감한가?
  - **RQ2 Reasoning Strategies**: 추론능력 향상 전략이 게임 시나리오에 적용 가능한가? (예: Chain-of-Thought (CoT) 추론, LLM에 페르소나 부여)
  - **RQ3 Generalizability**: 게임 설정 변화에 따른 LLM 성능 변화는 어떠한가? 학습 중에 답변을 기억하는가?
  - **RQ4 Leaderboard**: 다양한 LLM이 γ-Bench에서 어떻게 성능을 내는가?

---

## 4.1 RQ1: R OBUSTNESS

- LLM 응답 안정성 평가: 
  1. 모델 샘플링 전략에 의한 무작위성
  2. 온도 파라미터 설정
  3. 게임 지시용 프롬프트 유형
- **여러 번 실행 (Multiple Runs)**
  - 모든 게임을 동일 설정으로 5회 실행
  - "두 개의 순차적 게임"과 "Public Goods Game" 제외 대부분 낮은 분산을 보이며 일관된 성과 확인
- **온도 (Temperature)**
  - 온도 값 $$\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\}$$로 조절 실험
  - 전체 분산 3.4로 대부분 게임에서 온도 조절이 큰 영향 없음
  - 단, "Guess 2/3 of the Average" 게임에서는 온도 증가시 성능 크게 향상 (48.0 → 65.4)
- **프롬프트 템플릿 (Prompt Templates)**
  - GPT-4로 기존 프롬프트 리라이팅, 4가지 버전 추가 생성 후 성능 확인
  - "Public Goods Game"(분산 11.5), "Diner’s Dilemma"(23.7), "Pirate Game"(14.7) 등은 프롬프트 문장에 따라 성능 차이 큼
- **결론(RQ1)**
  - GPT-3.5는 여러 실행에서 일관성 높고 온도 변수에 강건함.
  - 그러나 부적절한 프롬프트 설계는 성능 악화 야기 가능.

---

## 4.2 RQ2: R EASONING STRATEGIES

- 추론 성능 향상을 위한 프롬프트 전략 두 가지:
  - Chain-of-Thought (CoT) prompting (예: "Let's think step by step" 문장 추가)
  - 페르소나 부여 (ex: 협력적 조수, 이기적 조수, 수학자 역할)
- **CoT 효과**
  - 게임 (1), (3), (4), (5)에서 성능 향상 (전체 점수 45.9 → 57.9, +12.0)
  - 예: "(3) Divide the Dollar" 게임에서 큰 할당 제안 감소, 점수 15.3 증가
  - "(4) Public Goods Game"과 "(5) Diner’s Dilemma"에서 자유 탑승 전략 인식 및 활용으로 각각 14.9, 78.5 점수 상승
- **페르소나 부여**
  - "협력적(cooperative)" 역할 부여 시 (1), (2), (3) 게임에서 성능 향상, "(3) El Farol Bar"에서 CoT 보다 우수
  - "이기적(selfish)" 역할 부여 시 대부분 게임에서 성능 저하, 단 "(7) Battle Royale"은 제외
  - "수학자(mathematician)" 역할은 성능 소폭(+0.6) 상승, CoT 효과에는 미치지 못함
- **결론(RQ2)**
  - 간단한 프롬프트 조작으로 GPT-3.5 성능 향상 가능
  - CoT prompting이 가장 효과적이며 GPT-4 성능에 근접 (57.9 vs. 62.4)

---

## 4.3 RQ3: G ENERALIZABILITY

- LLM 학습 데이터에 게임과 유사한 설정 포함 가능성 고려, 다양한 게임 설정으로 일반화능력 평가
- 실험 세부 파라미터는 표 8, 결과는 그림 8에 시각화
- **일부 게임에서 우수한 일반화 능력 확인**
  - (1), (3), (5), (6), (8) 게임에서 다양한 설정에 올바른 반응
  - "(3) Divide the Dollar"에서는 전체 자원(G)이 커질수록 성능 상승, 모두의 수요 만족
- **일부 게임에서 낮은 일반화력 확인**
  - "(2) El Farol Bar": 바 수용량(R) 변화에도 50% 확률로 참여, 무작위 행동 유사
  - "(4) Public Goods Game": 수익률이 0이어도 기여량 비슷, 룰 인식 부족
- 인간 실험 결과와의 비교: Nagel(1995), Rubinstein(2007) 연구와 유사한 평균 예측값 도출
  - 예: 비율 $$\frac{1}{2}, \frac{2}{3}, \frac{4}{3}$$ 에 대해 모델이 각각 34.59, 34.59, 74.92를 예측, 인간과 가까움
- **결론(RQ3)**
  - GPT-3.5는 게임 설정에 따른 성능 편차 존재, 특히 "(2) El Farol Bar"와 "(4) Public Goods Game"에서 낮음
  - γ-Bench는 복합 추론 능력 평가를 위한 테스트베드 역할, 모델 능력 향상에 따라 게임 설정 난이도 증가 가능

---

## 4.4 RQ4: L EADERBOARD

- γ-Bench에서 LLM별 의사결정 능력 성능 평가
- **폐쇄형 모델 성능**
  - GPT-3.5 (0613, 1106, 0125), GPT-4 (Turbo-0125, 4o-0806), Google Gemini Pro (1.0, 1.5) 비교
  - Gemini-1.5-Pro가 69.8점으로 최고, 특히 (1), (4), (5) 게임에서 우수
  - GPT-4o 66.75점으로 근접
  - GPT-4는 "(2) El Farol Bar"(23.0), "(5) Diner’s Dilemma"(0.9)에서 보수적 전략으로 낮은 점수, "(6) Sealed-Bid Auction"(24.2)도 위험 회피 경향
  - GPT-3.5 세 버전 간 성능 유사
- **오픈소스 모델 성능**
  - LLaMA-3.1-70B (65.9), Mixtral-8x22B (62.4)가 Gemini-1.5-Pro 근접, GPT-4 능가
  - Qwen-2, LLaMA-3.1-405B, LLaMA-3.1-8B가 GPT-3.5와 Gemini-1.0-Pro 능가
  - Mixtral-8x7B가 가장 낮은 성능, 모델 크기 및 추론력 약한 영향
  - LLaMA-3.1-405B가 70B보다 낮은 성능은 "(2) El Farol Bar"에서 과도하게 보수적인 전략 때문
- **결론(RQ4)**
  - 현재 Gemini-1.5-Pro가 최고 성능
  - LLaMA-3.1-70B가 2위

------

# 5 R ELATED WORK

- 대규모 언어 모델(LLM)을 게임 이론 모델을 통해 평가하는 연구가 활발히 진행 중이며, 최근 연구 동향은 표 3에 요약되어 있음.
- 주요 발견점:
  1. 많은 연구가 두 명의 플레이어, 단일 라운드 설정에서 순수 전략 내시 균형(PSNE)을 중심으로 죄수의 딜레마와 최후통첩 게임에 집중함.
  2. 다양한 온도(temperature) 설정이 사용되고 있으나, 이에 따른 LLM의 성능 영향에 대한 논의는 부족함.

## 5.1 S PECIFIC GAMES

- 연구자들은 다양한 게임 시나리오를 탐구함.
- Avalon 게임의 복잡하고 기만적인 환경을 시험 무대로 사용하여 최근 연구들은 다음에 집중:
  - 장기 다자 대화 (Stepputtis et al., 2023)
  - 사회적 행동 (Lan et al., 2024)
  - 사회적 지능 (Liu et al., 2024)
  - 기만 정보 식별을 위한 재귀적 사고 (Wang et al., 2023)
- Werewolf 같은 통신 게임 연구:
  - 튜닝이 필요 없는 프레임워크 (Xu et al., 2023)
  - 강화 학습 기반 접근법 (Xu et al., 2024b)
- O’Gara (2023)는 텍스트 기반 게임 Hoodwinked에서 고급 LLM들이 기만과 거짓말 탐지 능력을 보임을 발견.
- Liang et al. (2023)는 단어 맞추기 게임 Who Is Spy?에서 LLM의 지능과 전략적 의사소통 능력을 평가.
- Water Allocation Challenge 게임에서는 Mao et al. (2025)이 제한된 자원을 둘러싼 불평등 경쟁 시나리오를 구성.

## 5.2 G AME BENCHMARKS

- 인공 일반 지능 수준 평가를 위해 다양한 게임을 모아 종합적인 벤치마크를 구축하는 연구도 활발히 진행 중.
- Tsai et al. (2023)은 LLM이 텍스트 게임에서는 경쟁력 있지만, 세계 모델링(world modeling)과 목표 추론(goal inference)에 어려움을 겪음을 발견.
- GameEval (Qiao et al., 2023)은 협력 및 적대적 설정에서 LLM의 문제 해결 능력을 평가하기 위해 세 가지 목표 지향적 대화 게임(Ask-Guess, SpyFall, TofuKingdom)을 도입.
- MAgIC (Xu et al., 2024a)은 다중 에이전트 게임 환경에서 LLM을 평가하기 위한 확률적 그래프 모델링 기법을 제안.
- LLM-Co (Agashe et al., 2023)은 다중 에이전트 협력 시나리오에서 LLM의 파트너 의도 추론과 적극적 지원 능력을 평가.
- SmartPlay (Wu et al., 2024)는 여섯 개 게임에서 LLM을 에이전트로 평가하며 추론, 계획, 학습 능력에 중점.
- Abdelnabi et al. (2024)는 서로 다른 목표를 가진 6개 당사자가 포함된 협상 게임을 설계하여 LLM의 합의 도달 능력을 평가.

------

# 6 C ONCLUSION

- 본 논문에서는 멀티 에이전트 환경에서 대형언어모델(LLM)의 게임 능력을 평가하기 위한 벤치마크인 γ-Bench를 제안함.
- γ-Bench는 8개의 고전 게임 이론 시나리오를 포함하며, 다수의 플레이어가 여러 라운드와 액션에 걸쳐 상호작용하는 상황에 초점을 맞춤.
- 실험 결과, GPT-3.5 (0125)는 γ-Bench에서 제한적인 의사결정 능력을 보이나 과거 결과를 학습함으로써 성능을 개선할 수 있음.
- 신중하게 설계된 점수 체계를 활용한 평가에서, GPT-3.5 (0125)는 다양한 온도(temperature) 및 프롬프트에 대해서도 상당한 강건성을 나타냄.
- 특히, Chain-of-Thought(CoT)와 같은 전략이 효과적으로 작동함을 확인함.
- 그러나 다양한 게임 환경에 대한 일반화 능력은 제한적인 것으로 나타남.
- Gemini-1.5-Pro는 모든 테스트된 모델 중 최고 성능을 기록하며 γ-Bench 리더보드에서 1위를 차지함.
- 오픈소스 모델인 LLaMA-3.1-70B도 이에 근접한 성과를 보여줌.

- 결론적으로, γ-Bench는 LLM의 전략적 사고 및 멀티에이전트 게임에서의 성능 평가에 유용한 도구로 자리잡을 것으로 기대됨.  

---

*수식 표현 예시:*

벤치마크 내 점수 재조정(rescaling) 공식은 다음과 같음:

$$
\begin{cases}
S_1 = \frac{(MAX - MIN) - S_1}{MAX - MIN} \times 100, & R < 1 \\
S_1 = \left(1 - \left|\frac{2S_1 - (MAX - MIN)}{MAX - MIN}\right|\right) \times 100, & R = 1 \\
S_1 = \frac{S_1}{MAX - MIN} \times 100, & R > 1 
\end{cases}
$$

$$
S_2 = \frac{\max(R, 1 - R) - S_2}{\max(R, 1 - R)} \times 100, \quad
S_3 = \max\left(0, \frac{G - S_3}{G} \times 100\right),
$$

$$
S_4 = \begin{cases}
\frac{T - S_4}{T} \times 100, & \frac{R}{N} \leq 1 \\
\frac{S_4}{T} \times 100, & \frac{R}{N} > 1
\end{cases}, \quad 
S_5 = (1 - S_5) \times 100,
$$

$$
S_6 = S_6 \times 100, \quad
S_7 = S_7 \times 100, \quad
S_8 = \frac{2 \times G - S_8^P}{2 \times G} \times 50 + S_8^V \times 50.
$$

---

이처럼 γ-Bench는 LLM의 게임 내 협력, 배신, 순차적 의사결정 능력 등을 체계적으로 평가하며, 향후 LLM의 전반적인 전략적 사고 능력 향상에 크게 기여할 것으로 기대된다.