---
title: "[논문리뷰] Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs (CVPR 2024)"
date: 2025-03-09 00:00:00 +0900
categories:
  - Paper Review
tags:
  - CVPR 2024
  - Multimodal
  - MLLM
---

요약: 본 연구는 최근 다중 모달 모델의 시각적 능력에서 발생하는 문제점을 분석하고, CLIP의 시각 임베딩 공간과 자가 감독 학습 간의 차이를 조사하며, MLLMs의 시각적 이해 향상을 위해 혼합 피처(MoF) 접근 방식을 제안합니다.

---

# 1. Introduction

<img width="885" alt="image" src="https://github.com/user-attachments/assets/271db980-8a40-41bf-9303-69c5140fbb67" />

- 최근 다중 모드 대형 언어 모델(MLLM; Multimodal Large Language Models)이 급속히 발전하고 있음.
- MLLM은 이미지와 대형 언어 모델(LLM; Large Language Models)을 통합하여 이미지 이해, 시각 질문 응답, 지시 따르기와 같은 과제에서 뛰어난 능력을 발휘함.
- 특히, 최근 발표된 GPT-4V(ision)가 이전보다 높은 성능을 기록함.
- 그러나 이러한 모델들은 여전히 **시각적 결점이 있으며**, 일부는 초보적이고 뚜렷한 문제들임.
- 이 논문에서는 이러한 문제의 원인이 **시각적 표현과 관련**이 있다고 제안함.
- 대부분의 MLLM은 사전 학습된 시각 및 언어 모델에 기반하여 다양한 어댑터를 사용하여 서로 다른 모드를 통합함.
- 연구들은 미리 학습된 텍스트 인코더의 실패 패턴이 후속 단계에서의 실패로 이어질 수 있음을 보여줌.
- MLLM의 주요 **시각 인코더인 CLIP 모델의 한계에 대한 검토**와 그로 인해 발생하는 문제들을 다룸.
- 본 연구에서는 CLIP 모델이 적절히 인코딩하지 못하는 실패 사례들을 식별하고 이를 기반으로 새로운 벤치마크인 MultiModal Visual Patterns(MMVP)를 제안함.
- MMVP는 시각적 능력을 평가하기 위해 CLIP-blind 쌍을 대상으로 클리어한 질문을 제기하도록 설계됨.
- 평가 결과, MLLM 모델들이 간단한 시각적 질문에 대해 대부분 무작위 수준 아래에서 성과를 보였음.
- MLLM에서 CLIP 모델의 한계와 그것이 모델의 성능에 미치는 영향을 연구하고자 하며, 디노(v2)와 같은 비전 중심 표현을 통합하여 시각적 기초를 향상시키기 위한 방안을 모색함.

---

# 2. The Multimodal Visual Patterns (MMVP) Benchmark

- **MLLM의 발전**: 최근 Multimodal Large Language Models (MLLMs)은 이미지와 언어 모델을 통합하여 이미지를 이해하고, 시각적 질문에 답하며, 지침을 수행하는 능력이 향상되었다.
- **문제의 확인**: 이러한 모델들에도 불구하고 몇 가지 기본적인 시각적 문제를 여전히 경험하고 있으며, 이는 시각적 표현의 문제에서 기인할 수 있다.
- **CLIP 모델의 활용**: 많은 MLLMs는 pretrained Contrastive Language-Image Pre-Training (CLIP) 모델을 시각적 인코더로 사용하고 있어 CLIP 모델이 제대로 인코딩하지 못하는 이미지 쌍을 탐색하여 문제를 밝혀내고자 한다.
- **CLIP-blind 쌍**:
  - CLIP가 비슷하게 인코딩한 두 이미지 중 적어도 하나는 모호하게 인코딩되었을 가능성이 있는 이미지 쌍을 CLIP-blind 쌍이라고 부른다.
  - 시각적 유사성을 측정하기 위해 DINOv2와 같은 비전 전용 자가 감독 인코더를 사용한다.
- **MMVP 벤치마크 구축**: CLIP-blind 쌍을 통해 MMVP 벤치마크를 개발하고, 이 벤치마크에서 다양한 최신 MLLM 모델을 평가하였다.
- **모델 평가 결과**: 
  - 대부분의 MLLM 모델들이 단순한 시각적 질문에서 무작위 추측 수준 이하의 성능을 보였다.
  - 예외로 GPT-4V는 비교적 높은 성능을 보였지만, 여전히 인간의 성능에는 미치지 못했다.
- **실험 단계**:
  1. **CLIP-blind 쌍 찾기**: CLIP 인코더가 비슷하게 인코딩하였지만 DINOv2에서 다르게 인코딩된 이미지 쌍을 발견한다.
  2. **차이점 발견**: 시각적 차이를 수작업으로 정의하고 이와 관련된 질문을 만들었다.
  3. **MLLM 평가**: 수집된 CLIP-blind 쌍과 질문을 사용하여 MLLM 모델의 성능을 평가하였다.
- **단순한 질문**: 질문은 의도적으로 간단하고 명확하게 구성되어 MLLM의 시각적 과제를 평가할 수 있도록 하였다.
- **결과 요약**: 
  - 인간 참가자는 평균 95.7%의 정확도로 질문을 맞혔지만, MLLM 모델들은 낮은 성능을 보였다.
  - CLIP 모델의 문제점은 MLLMs에도 비슷한 문제를 야기했으며, CLIP 모델의 한계가 MLLM의 성능에 영향을 미쳤음을 시사한다.

---

# 2.1. Finding CLIP-blind Pairs

<img width="885" alt="image" src="https://github.com/user-attachments/assets/a08aa445-3a8d-4027-8d4b-9a712154cc54" />

- CLIP 비전 인코더가 "적절히" 인코딩하지 못하는 인스턴스(이미지)를 직접 찾는 것은 어려움.
- 이 문제를 해결하기 위해, Tong et al.의 아이디어를 확장하여 비전 모델에서 블라인드 쌍을 자동으로 찾아내는 방법 제안.
- 기본 원리는:
  - **시각적으로 뚜렷한 차이가 있음에도 불구하고 CLIP 비전 인코더에 의해 유사하게 인코딩된 두 이미지는 적어도 하나가 모호하게 인코딩**되었을 가능성이 높음.
- 두 이미지 간의 시각적 차이를 측정하기 위해:
  - 참조 모델(언어 guidance 없이 훈련된 비전 전용 자기 지도 모델)에서 이미지 표현을 조사.
  - 예: DINOv2 모델은 더 많은 시각적 세부 사항과 정보를 캡처할 수 있음.
- CLIP-blind pairs 수집을 위해 사용하는 데이터셋:
  - ImageNet과 LAION-Aesthetics.
- 각 이미지 쌍에 대해:
  - CLIP-ViT-L-14 모델을 사용하여 CLIP 임베딩 계산.
  - DINOv2-ViT-L-14 모델을 사용하여 DINOv2 임베딩 계산.
- 반환 조건:
  - **CLIP 임베딩의 코사인 유사도가 0.95를 초과하고,**
  - **DINOv2 임베딩의 코사인 유사도가 0.6 미만인 쌍.**

---

# 2.2. Designing Benchmark from CLIP-blind Pairs

<img width="851" alt="image" src="https://github.com/user-attachments/assets/8283fd7b-4af2-4dec-bfde-b98fdb094604" />

- MMVP(다중모달 시각 패턴) 벤치마크 및 VQA(시각 질문 응답) 벤치마크를 소개
- 수집된 CLIP-블라인드 쌍을 활용하여 150 쌍의 이미지와 300개의 질문을 신중하게 설계
- 각 CLIP-블라인드 쌍의 이미지에서 CLIP 비전 인코더가 간과하는 시각적 세부사항을 수동으로 파악
- 예시 질문: “개의 방향이 왼쪽인가 오른쪽인가?” 
- 질문은 간단하고 명확하게 설계하여 MLLM 모델이 기본적인 질문에 실패하고 중요한 시각적 세부 사항을 간과하는지를 판단하는 것이 주된 목표

---

# 2.3. Benchmark Results

- SOTA 오픈 소스 모델 (LLaVA-1.5, InstructBLIP, Mini-GPT4) 및 클로즈드 소스 모델 (GPT-4V, Gemini, Bard) 평가
  - 모델 접근 방법은 부록 B.1 참고
  - 각 질문은 독립적으로 쿼리하여 채팅 기록의 편향 제거

- 사용자를 대상으로 한 연구 평가

  <img width="415" alt="image" src="https://github.com/user-attachments/assets/ea3d438e-d719-44da-8603-d44dbf138e7c" />

  - 300개의 질문을 랜덤 순서로 제시
  - 이미지 쌍에 대해 두 질문이 정확히 응답된 경우를 올바른 것으로 간주
  - 인간 참여자들은 평균 95.7%의 질문을 정확히 답변

- 현재 MLLM은 시각적 세부 사항에서 어려움을 겪음
  - 모델들은 단순한 시각적 질문에 대하여 성능 차이 존재
  - **대부분의 MLLM이 무작위 추측 수준(25%) 이하의 점수 기록**
  - GPT-4V와 Gemini는 상대적으로 높은 점수지만, 여전히 기본적인 시각적 질문에서 어려움

- 모형의 성능 저하 원인
  - 시각적 무능력에서 비롯된 것으로 확인 (부록 B.3의 절삭 연구를 통해)
  - 언어 모델의 환각이 원인이 아님을 분석

-benchmark의 정확도 비교
  - 인간 정확도: 95.7%
  - Gemini: 40.7%
  - GPT-4V: 38.7%
  - 나머지 모델은 25% 미만의 성적 기록

- 결과는 모델의 크기나 훈련 데이터와 관계없이 시각적 세부 사항에서의 실패를 보여줌

---

# 3. Systematic Failures in CLIP

- **CLIP-블라인드 쌍 식별**: 이전 섹션에서 CLIP-블라인드 쌍을 식별하여 MLLMs의 실패를 찾음.
  
- **시스템적 비주얼 패턴 분석**: 
  - (i) CLIP-블라인드 쌍에서 발견된 시스템적 비주얼 패턴 분석.
  - (ii) 이러한 비주얼 패턴이 CLIP 기반 모델의 대규모 확장에 도전과제를 제공하는지 여부.
  - (iii) CLIP 모델의 실패 패턴과 MLLMs의 실패 패턴 사이의 상관관계.

- **비주얼 패턴 예시**: 각 패턴에 따라 다음의 이미지 쌍들로 구성됨:
  - 방향 및 자세: 오른쪽과 왼쪽을 바라보는 토끼.
  - 특정 특징의 존재: 튤립 유무.
  - 상태 및 조건: 날개가 열린 나비와 닫힌 나비.
  - 수량 및 개수: 1잔의 음료 vs 2잔의 음료.
  - 위치 및 관계 컨텍스트: 슬리퍼 오른쪽과 왼쪽의 안경.
  - 색상 및 외관: 밝은 파란 하늘 vs 어두운 파란 하늘.
  - 구조적 특성: 반으로 잘린 과일 vs 잘리지 않은 과일.
  - 텍스트: 시간을 나타내는 텍스트 (“11:54” vs “11:59”).
  - 관점 및 전망: 위에서 본 꽃 vs 옆에서 본 꽃.

- **모델 성능**: MMVP-VLM 벤치마크에서 CLIP 모델이 텍스트 설명에 따라 올바른 이미지를 선택하지 못하는 사례가 나타남.

---

# 3.1. Visual Patterns in CLIP-blind Pairs

- CLIP-blind 쌍을 식별한 후, CLIP 비전 인코더가 지속적으로 잘못 해석할 수 있는 체계적인 시각 패턴을 요약함.
- CLIP-blind 쌍에서 체계적인 시각 패턴을 직접 포착하는 것은 너무 추상적임.
- MMVP 벤치마크의 질문과 옵션을 활용하여 추상적인 시각 패턴을 보다 명확한 언어 기반 설명으로 전환함.
- 이 작업에서 GPT-4를 사용하여 일반적인 패턴을 범주화함.
  
### 식별된 9가지 시각 패턴:
1. 방향과 방향성
2. 특정 특징의 존재
3. 상태 및 조건
4. 수량과 개수
5. 위치적 및 관계적 맥락
6. 색상과 외관
7. 구조적 및 물리적 특성
8. 텍스트
9. 시점과 관점

- 이러한 시각 패턴은 CLIP 비전 인코더가 고차원 세멘틱 이해에 지나치게 집중하고, 시각 세계의 복잡한 세부 사항을 간과함을 시사함.
- 시각 패턴의 전체 설명은 부록 D에서 확인 가능.

---

# 3.2. The MMVP-VLM Benchmark

<img width="847" alt="image" src="https://github.com/user-attachments/assets/6d869af4-0714-4459-9e3b-e2119a4be6f7" />

<img width="847" alt="image" src="https://github.com/user-attachments/assets/a61bc581-351b-4b28-bf3d-c5d592da2f67" />

- CLIP 기반 모델은 첫 번째 논문 발표 이후 빠르게 발전함.
- 최근 CLIP 모델들이 여전히 시각적 패턴에 어려움을 겪는지 테스트하기 위해 MMVP-VLM이라는 새로운 벤치마크를 도입.
- MMVP 벤치마크에서 질문의 하위 집합을 더 간단한 언어 설명으로 정제하고, 이를 시각적 패턴으로 분류.
- 각 시각적 패턴에 균형 잡힌 질문 수를 유지하기 위해 필요한 경우 몇 가지 질문을 추가하여 각 패턴이 15개의 텍스트-이미지 쌍으로 대표되도록 함.
- 쌍이 정확히 답변된 것으로 간주되려면 모델이 두 이미지-텍스트 조합을 모두 정확하게 일치시켜야 함.
- 다양한 CLIP 모델에서 MMVP-VLM을 평가.
- 이 모델들은 크기, 훈련 데이터 및 방법론 등 여러 측면에서 차이가 있음.
- 네트워크 크기와 훈련 데이터 증가가 ‘색상과 외관’, ‘상태와 조건’ 두 시각적 패턴 식별에 도움이 됨.
- 나머지 시각적 패턴은 모든 CLIP 기반 모델에 계속해서 도전 과제가 됨.
- ImageNet-1k의 제로샷 정확도가 모델의 시각적 패턴 성능을 위한 결정적인 지표가 아님을 발견.
- 이는 이미지 분류 이외의 영역에서 모델의 능력을 정확히 평가하기 위해 MMVP-VLM과 같은 추가 평가 메트릭의 필요성을 강조함.

---

# 3.3. How CLIP’s Errors Affect MLLMs

<img width="421" alt="image" src="https://github.com/user-attachments/assets/e2388f52-a97c-4ec9-b769-20177673033d" />

- CLIP 모델이 어려움을 겪는 시각적 패턴을 분석한 후, CLIP와 멀티모달 언어 모델(MLLM) 간의 성능 상관관계에 대한 질문 제기.
- MLLM의 성능을 시각적 패턴에 따라 분류하여 평가함.
- CLIP이 특정 시각적 패턴(예: ‘방향’)에서 낮은 성능을 보일 경우, MLLM도 유사한 성능 저하를 보임.
- LLaVA 1.5 및 InstructBLIP와 같은 오픈소스 모델은 CLIP을 비전 인코더로 직접 사용함.
- CLIP 모델과 MLLM의 성능 간의 Pearson 상관 계수를 계산한 결과, LLaVA 1.5와 InstructBLIP의 상관 계수가 0.7을 초과함.
- 이로 인해 CLIP 모델에서의 시각적 패턴 인식 약점이 MLLM에 전이됨을 확인.

---

# 4. Mixture-of-Features (MoF) for MLLM

<img width="853" alt="image" src="https://github.com/user-attachments/assets/6fdabe2e-2f49-417a-b1e9-4477ef0194e9" />

- **배경**: 이전 섹션에서 논의한 내용을 바탕으로, 오픈소스 MLLM의 시각적 한계가 CLIP 비전 인코더에서 비롯된다면, 더 유능한 비전 인코더를 어떻게 구축할 수 있을지에 대한 질문이 생김.
  
- **주요 내용**:
  - **MoF(Mixture-of-Features) 초기 단계**: CLIP 특징과 비전 전용 SSL 모델 특징을 혼합하는 addititve MoF를 연구.
  - **결과**: 각 인코더가 MLLM의 사전 훈련 모델로 사용될 때 고유한 장점과 제한이 있음을 확인 (Section 4.2).
  
- **제안사항**:
  - **Interleaved MoF**: CLIP과 SSL의 특징을 결합하여 시각적 구력을 향상시키되, 모델의 지침 따르기 능력을 저해하지 않도록 함 (Section 4.3).

---

# 4.1. Experiment Setting

- LLaVA 프레임워크를 사용하여 MLLM에서 시각 인코더를 연구함.
- LLaVA는 미리 훈련된 CLIP 인코더를 사용하고, LLM의 언어 토큰과 시각 토큰을 정렬하기 위한 어댑터를 훈련함.
- DINOv2를 비전 전용 SSL 모델로 채택, 이는 현재 가장 확장 가능한 비전 전용 모델임.
- 두 가지 시각 인코더(CLIP-ViT-L-14 및 DINOv2-ViT-L-14)를 사용하여 실험함.
- 일관되고 공정한 비교를 위해 동일한 실험 설정에서 모델을 훈련하고 미세 조정함.
- 추가적인 실험 세부 사항은 부록 A에 포함됨.

---

# 4.2. Additive MoF

<img width="417" alt="image" src="https://github.com/user-attachments/assets/8a7ba795-01fd-4572-9efb-778714dc6863" />

- MLLM에 사전 훈련된 DINOv2 인코더를 추가하고 CLIP 인코더와 혼합.
- 조정 계수 α를 사용하여 CLIP 특징의 비율을 제어하고, 1 - α를 통해 DINOv2 특징의 양을 제어.
- CLIP과 DINOv2의 특징을 선형적으로 혼합.

- 세 가지 Mixture-of-Feature(MoF) 전략 설명:
  - **Standard MLLM**: CLIP을 사전 훈련된 비전 인코더로 사용.
  - **Additive-MoF (A-MoF)**: 어댑터 전에 CLIP과 DINOv2 특징을 선형적으로 혼합.
  - **Interleaved-MoF (I-MoF)**: 어댑터 후 CLIP 비주얼 토큰과 DINOv2 비주얼 토큰을 공간적으로 교차.

- 모델의 시각적 기초 능력은 MMVP를 통해 평가하고, 지시 따르기 능력은 LLaVA 벤치마크로 평가.
- 실험 결과:
  - DINOv2 특징 비율을 0%에서 100%까지 전환하는 다섯 가지 실험 실시.
  - DINOv2 비율: {0.00, 0.25, 0.50, 0.75, 1.00} 및 추가 비율 {0.625, 0.875} 실험 진행.

- 주요 발견:
  1. DINOv2 특징 비율 증가 시 MLLM의 지시 따르기 능력 감소.
     - DINOv2 비율이 87.5%에 도달할 때 급격한 감소 발생.
  2. DINOv2 특징 비율이 높을수록 모델의 시각적 기초 능력이 향상되지만, 0.75%를 초과하면 이점이 감소.
  
- 결론: 
  - DINOv2 특징 추가 또는 CLIP을 DINOv2로 완전히 대체할 경우 시각적 기초 능력과 지시 따르기 간의 트레이드오프 발생.
  - DINOv2 특징 비율이 높을수록 시각적 인지는 향상되지만 지시 따르기 능력은 저하됨. CLIP 특징은 언어 이해를 강화하지만 시각적 기초 능력은 감소함.

---

# 4.3. Interleaved MoF

<img width="417" alt="image" src="https://github.com/user-attachments/assets/282fbf8c-f68e-494b-bceb-55b086009023" />

- **제안 배경**:
  - Interleaved MoF는 CLIP과 DINOv2 임베딩의 장점을 활용하여 이미지 표현을 향상시키기 위해 제안됨.
  
- **방법론**:
  - 이미지가 CLIP과 DINOv2 인코더를 동시에 통과.
  - 생성된 임베딩은 각각 어댑터를 통해 처리됨.
  - 처리된 특징은 원래의 공간적 순서를 유지하며 교차 결합(interleave)됨.
  - 교차 결합된 특징은 LLM에 공급됨.

- **성능 결과 (표 2 및 표 3)**:
  - LLaVA 설정에서 Interleaved MoF는 MMVP에서 10.7% 향상을 보이며, 모델의 지시 수행 능력은 유지됨.
  - 해상도와 관계없이 LLaVA-1.5 설정에서도 유사한 성능 향상을 달성.

- **시각적 기반의 개선**:
  - Interleaved MoF는 원래 LLaVA 모델에 비해 시각적 기반에서 일관된 개선을 보여줌.
  - 이미지 해상도 증가만으로는 시각적 기반 능력이 향상되지 않음.
  - Vision-only SSL 모델과 VLM 모델 간의 MoF의 교차 결합이 성능 향상에 기여함.

- **추가 평가**:
  - MAE 또는 MoCoV3와 같은 비전 전용 SSL 모델을 I-MoF에 사용한 추가 실험에서도 유사한 시각적 기반 개선이 관찰됨.
  - MM-Bench 및 GQA와 같은 추가 벤치마크에서도 Interleaved MoF가 유사한 성능을 달성함.

- **결론**:
  - Interleaved MoF는 비전 모델과 언어 모델의 결합을 통해 시각적 이해를 증진시키는 효과적인 방법임.

---

# 5. Related Works

- **다중 모드 LLMs**  
  - 다중 모드 LLM의 한계를 연구하고 개선 방안을 모색함.
  - 사전 학습된 대형 언어 모델과 CLIP 비전 인코더를 기반으로 구축됨.
  - MLP, Q-Former, gated attention 등 어댑터를 사용하여 CLIP 비전 인코더를 LLM에 통합.
  - 최근 instructBLIP와 LLaVA-1.5는 고품질 훈련 데이터의 중요성을 강조.
  - 비전 인코더의 영향에 대한 연구가 부족하며, 이를 체계적으로 다루는 것이 목표.

- **다중 모드 LLM 평가**  
  - MMVP는 CLIP-불투명 쌍에서 구성된 시각적 질문 응답(VQA) 질문을 사용하여 MLLMs 평가.
  - TextVQA, VQAv2, GQA와 같은 기존 벤치마크는 전통적인 VQA 쿼리에 집중.
  - MM-Vet, POPE, MM-Bench와 같은 최근 작업은 다중 모드 LLM의 허위 생성, 추론 및 강건성을 평가하는 데 중점.
  - 과거의 평가에서 다중 모드 LLM이 허위 생성, 재앙적 망각 및 부족한 강건성 문제를 겪는 것으로 나타남.
  - 최신 다중 모드 LLM인 GPT-4V, Gemini, Bard, LLaVA-1.5조차 기본 시각적 질문에 어려움을 겪고 있다는 점을 발견.

- **비전 인코더**  
  - MMVP-VLM은 다양한 CLIP 변종의 비전 역량을 상세히 분석.
  - 이러한 모델들은 주로 대량의 이미지-텍스트 쌍에 대해 대조 손실을 사용하는 방식을 따름.
  - 다만, 모든 CLIP 변종은 간단한 시각적 패턴(orientation, count, specific features의 존재 등)에 대해 어려움을 겪음.
  - 비전 전용 자가 감독 학습(SSL) 연구 또한 진행 중.
  - SLIP는 CLIP과 대조 기반 SSL 간의 시너지를 탐구하나, 표준 분류 작업에 중점.
  - 현재 평가 방법은 기초적인 평가 수준을 제공하나, 최근 사용 사례의 필요와 동떨어짐.
  - CLIP 비전 모델과 비전 전용 SSL 모델이 보완적 특징을 학습하지만, ImageNet에서의 선형 프로빙 정확도는 MLLMs의 특성 유용성을 충분히 이해되지 않음.

- **임베딩 모델의 모호성**  
  - CLIP 비전 임베딩 공간 내 CLIP-불투명 쌍을 활용하여 CLIP 모델과 MLLM에서의 실패 사례를 생성.
  - 텍스트 임베딩 모델의 실패 모드를 문서화하는 연구와 관련 있음.
  - 최근 연구들은 CLIP이 텍스트 쿼리를 처리하는 데 있어 바인딩 문제를 다루고, 텍스트 입력을 단어의 가방으로 취급함.
  - 이미지 캡셔너를 CLIP의 대안으로 제안하였으며, 우리의 연구는 시각적 패턴에 초점을 맞춤.

---

# 6. Discussion

- 비전이 언어에 충분한지에 대한 질문으로 돌아가보면, 현재로서는 그렇지 않은 것으로 보임.
- 연구 결과는 비전 모델이 다중 모달 시스템에서 병목 현상이 될 수 있음을 보여줌.
- MLLMs는 사전 훈련된 CLIP 비전 인코더가 이미지의 중요한 시각적 세부사항을 간과하여 간단한 질문에 실패함.
- CLIP 타입 모델은 여전히 가장 확장 가능하고 널리 사용되는 비전 모델임.
- 데이터와 모델 확장만으로 CLIP 모델의 고유한 결함을 해결할 수 없음을 본 연구가 입증함.
- 인기 있는 비주얼 표현 학습 모델(비전-언어 모델 및 비전 전용 자가 감독 학습 모델)은 각각 다른 강점을 지님.
- 이들 모델의 능력 차이는 기존 벤치마크(예: 선형 프로빙, 제로-샷 정확도) 이상의 의미가 있음.
- Mixture-of-Features 접근 방식을 통해 시각적 한계를 완화하고 두 학습 패러다임의 강점을 활용할 수 있지만, 새로운 평가 메트릭 개발이 필요함.
- 이 연구가 비전 모델의 혁신을 촉진할 수 있기를 희망함. 


---

# 독자 의견

- 본 논문은 VLLM의 성능에 visual encoder가 큰 영향을 미치고 있다는 가정에서 시작한다.
- 실제로 VLLM의 visual encoder로 주로 사용되는 CLIP 모델이 시각적 세부사항을 잘 인코딩하지 못한다는 것을 실험적으로 보여준다.
- CLIP의 성능을 보완하기 위해 DINOv2의 visual encoder를 앙상블 및 교차결합 방법으로 결합하는 실험을 진행하였고, 성능 향상을 이끌어 내었다.
- 이러한 실험을 통해 VLLM의 성능을 향상시키기 위해 visual encoder의 중요성을 강조하고 있다.
- 하지만 이런 접근 방법보다는 CLIP 모델의 성능을 향상시키는 방향으로 연구를 진행하는 것이 더 좋지 않을까 생각한다.
- 또한 본 논문에서는 CLIP이 색상, 개수, 방향, 텍스트 등의 시각적 패턴을 잘 인코딩하지 못한다는 것을 지적하였는데, 이는 별도의 툴을 두어 (ReAct 에이전트와 같이) 해결하는 것이 더 좋지 않을까 생각한다.
