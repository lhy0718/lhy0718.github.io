---
title: "[논문리뷰] Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation (ACL 2024)"
date: 2025-03-08 22:30:00 +0900
categories:
  - Paper Review
tags:
  - NLP
  - ACL 2024
  - Empathetic Dialogue Systems
---

요약: 감정 지원 대화(ESC) 과제에서는 일상적인 대화를 통해 개인의 감정적 고통을 완화하는 것을 목표로 하며, 최근 대형 언어 모델(LLMs)이 효과적인 감정 지원을 제공하는 데 어려움을 겪고 있음을 분석하고, 특정 전략에 대한 선호가 효과적인 지원을 저해함을 밝혀냈다. 이 연구는 LLM이 감정 지원의 효율성을 개선하기 위한 접근 방안을 제시한다.

---

# 1 Introduction

- 감정 지원 대화(ESC)는 개인의 정서적 강도를 완화하고 개인적인 도전을 해결하도록 안내하는 것을 목표로 함.
- 효과적인 감정 지원은 유용한 지원을 제공할 뿐 아니라 질 낮은 지원을 피하는 것도 중요함.
- 질 낮은 지원은 이미 스트레스가 많은 상황을 악화시킬 수 있음.
- 감정 지원 제공은 복잡하고 인간에게도 도전적인 과제임.
- Hill의 도움의 기술 이론을 바탕으로 Liu et al.이 일반적으로 세 단계(탐색 → 위로 → 행동)를 따르는 감정 지원 프레임워크를 제안함.
- 최근 대규모 언어 모델(LLMs)은 대화 시스템에서 널리 사용되며 감정 지원 제공에 대한 관심이 증가하고 있음.
- 그러나 LLM은 감정 지원 제공에서 어려움을 겪고 있음.
- ESC 작업은 전략 선택 및 대응 생성을 포함하며, 적절한 전략 선택이 효과적인 지원에 중요함.
- LLM의 전략 선호도가 특정 전략에 쏠리는 경향이 나타남.
  
### 연구 질문:
- RQ1: 선호도가 감정 지원 제공에 미치는 영향은? 
  - 다양한 LLM의 전략과 단계에서의 능숙함을 평가함.
- RQ2: LLM의 선호도 편향을 완화하는 방법은? 
  - 외부 도움을 통해 LLM의 편향을 줄일 수 있다는 것을 발견함.
- RQ3: 선호도 편향 개선이 감정 지원 제공 능력 향상에 기여하는가?
  - 심리학자들과 함께 개발한 기준을 사용하여 감정 지원 품질을 분석함.

### 기여 사항:
- LLM의 전략 선호 현상을 소개함.
- 능숙도, 선호도, 선호도 편향을 중점으로 한 새로운 메트릭을 제안함.
- 전략 선택에서 편향의 중요성을 강조함.
- LLM의 편향 문제 해결에 외부 도움의 필요성을 제시함.
- 감정 지원의 효과를 평가하기 위한 포괄적인 기준을 구성함.
- 선호도 편향 완화가 질 낮은 응답 비율 감소에 중요함을 입증함.

---

# 2 Preliminaries & Related Work

- **사전 지식**:
  - 연구의 기초가 되는 기본 개념과 정의에 대해 설명.
  - 문제의 맥락과 연구의 필요성을 제시.

- **기존 연구**:
  - 관련 문헌 및 이전 연구 성과에 대한 리뷰.
  - 다양한 접근 방식 및 기술의 장단점을 비교 분석.

- **문제 정의**:
  - 연구가 해결하고자 하는 특정 문제의 명확한 정의.
  - 문제가 왜 중요한지에 대한 설명.

- **기술적 배경**:
  - 사용되는 기술 및 방법론에 대한 설명.
  - 방법론이 문제 해결에 어떻게 기여하는지에 대한 논의.

- **연구의 차별성**:
  - 본 연구가 기존 연구와 어떤 점에서 다른지 및 기여도를 강조.
  - 연구의 독창성 및 중요성을 명확히 함.

---

# 2.1 Emotional Support Conversation

- Liu et al. (2021)의 연구는 정서적 지원 대화(ESC) 과제를 제안하고 다양한 상황을 포괄하는 ESConv 데이터세트를 발표함.
- ESC는 감정적 고통을 느끼는 사용자(도움 요청자)와 편안함을 제공하고자 하는 시스템(지원자) 간의 상호작용에 중점을 두며, 사용자의 정서적 강도를 완화하는 것을 목표로 함.
- ESC는 전문 상담과 구별되며, 주로 친구 또는 가족과의 사회적 상호작용 내에서 지원을 강조함.
- ESConv의 정서적 지원 절차는 일반적으로 세 단계(탐색 → 위안 → 행동)로 진행됨.
- 향상된 정서적 지원 응답의 품질을 위해 각 단계에서 적절한 응답을 제공하는 것이 중요함.
- 전략을 ESC 시스템에 통합하는 연구는 감정, 의미 및 인물 요소와 함께 지원 전략의 통합을 강조함.
- LLMs의 출현으로 LLM을 정서적 지원자로 활용하려는 연구가 증가하고 있으며, 최근 연구에서는 미세 조정 방식 대신 LLM을 컨텍스트 학습을 통해 사용할 수 있는 잠재력을 탐구함.
- 그러나 LLM이 제공하는 정서적 지원의 한계도 나타남.
- 연구에서는 LLM의 전략 예측 능력을 분석하여 정서적 지원의 효율성을 높이기 위한 방법론적 접근을 탐색함. 

이상으로 LLM의 정서적 지원 제공 방식에 대해 전략 중심으로 분석하고 있으며, 이는 향후 연구에서 정서적 지능 향상에 기여할 가능성이 있음.

---

# 2.2 Incorporating Strategies into ESC Systems

- Liu et al. (2021)는 감정 지원 대화 과제를 제안하고 다양한 상황을 다룬 데이터셋 ESConv를 발표함.
- ESC(Emotional Support Conversation)는 감정적 고통을 겪고 있는 사용자(도움 요청자)와 위로를 제공하는 시스템(지원자) 간의 상호작용에 중점을 둠. 
- ESC는 감정적 지원을 제공하는 데 주력하며, 전문 상담과는 차별됨.
- 대화에서 감정적 지원의 과정은 일반적으로 세 단계로 나뉘며 (탐색 → 위로 → 행동), 각 단계에서 적절한 응답을 제공하는 것이 중요함.
- 적절한 지원 전략과 감정, 의미론, 그리고 개인화 요소와의 통합이 ESC 시스템 구축에서 강조됨.
- 일부 연구는 사용자의 상태와 전략을 결합한 모델링에 집중함.
- Deng et al. (2023)는 생성형 상식 지식을 전략 예측의 보조 작업으로 포함시켜 더 나은 정서적 지원을 제공하도록 함.
- 그러나 많은 접근 방식이 모델 아키텍처 수정이나 사전 훈련된 매개변수 조정 등으로 이뤄져 LLM(대규모 언어 모델)에선 적용하기 어려운 경우가 많음.
- LLM의 등장으로 감정 지원자로서의 가능성을 탐구하는 연구가 증가하고 있으며, 최근 연구들은 사전 훈련 방식 대신 in-context 학습을 통해 LLM을 ESC 시스템으로 활용하려고 시도함.
- 연구 결과, LLM은 감정 지원을 제공하는 능력에 한계가 있음이 밝혀짐.
- 사용자들이 LLM의 감정 지원 응답의 책임 부족으로 불편함을 느낄 수 있음.
- 대다수의 ESC 연구가 지원 전략 활용에 중점을 두고 있지만, LLM 내 전략에 대한 종합적인 분석은 미비함.

---

# 2.3 Emotional Support from LLMs

- LLM의 등장으로 감정 지원 역할에 대한 연구 증가
- LLM을 ESC 시스템으로 활용하기 위한 새로운 접근법 (in-context learning)
- 기존의 미세 조정 접근법 대신 프롬프트를 통한 학습 방식 시도
- LLM의 감정 지원 능력에 한계가 있음 (최근 연구 결과)
  - Chung et al. (2023), Farhat (2023), Eshghie and Eshghie (2023), Song et al. (2024) 등의 사례
- 연구 결과:
  - 사용자가 LLM의 감정 지원 추천에 대해 불편함 또는 우려를 경험할 수 있음
  - 이는 LLM의 책임 부족 때문
- ESC 연구의 대부분이 지원 전략에 중점을 두었지만 LLM의 전략 분석은 충분히 이루어지지 않음
- 지원 전략의 비율에 대한 데이터 제공 (표 1): 질문, 응답, 참조, 선택, 감정, 제안, 정보, 기타 전략별 비율 분석.

---

# 3 Evaluation Setup

- **목적**: 평가 시스템의 설계 및 진행 방법 설명.
  
- **데이터셋**:
  - 다양한 데이터셋 사용.
  - 검증 데이터와 테스트 데이터의 구분.
  
- **평가 기준**:
  - 정확도, 정밀도, 재현율 등의 성능 지표 정의.
  
- **실험 설정**:
  - 실험 조건과 변수에 대한 명세.
  - 여러 조건 하에서의 반복 실험 수행.
  
- **분석 방법**:
  - 결과 해석을 위한 분석 툴 및 방법론.
  
- **결과 보고**:
  - 결과 요약 및 시각화 방안 제시.
  - 피드백 수집 방법 설명.

---

# 3.1 Task and Focus

- **과제**: 감정 지원 응답 생성
  - 기계 생성 응답의 효과는 적절한 전략 선택에 크게 의존함.
  - 감정 지원 응답 생성 과제로 대화 배경(I), 요청자의 사전 조사(예: 감정, 상황), 대화 맥락(C)을 기반으로 응답 생성.
  - 모델 θ는 먼저 전략 S를 예측하고, 그 다음에 I, C, S를 바탕으로 응답 R을 생성.
  
- **집중**: 전략 중심 분석
  - LLM이 감정 지원 제공에 어려움을 겪는 여러 이유 중에서 이 연구는 전략에 초점을 맞춤.
  - 전략은 ESC 시스템 내에서 핵심 요소.
  - 실제 전략에 따라 생성된 응답의 질을 탐색하여 전략 중심 분석의 유효성을 강조.
  - 모델이 전략을 정확히 예측할 수 있다면 감정 지원 응답의 질 향상 가능성 큼.

---

# 3.2 Evaluation Set

- 총체적인 분석을 위해 ESConv의 단계를 바탕으로 세 가지 테스트 세트(Dt)를 구성 
- 대화 내용을 5-15 턴 샘플로 무작위로 잘라냄 
- 각 샘플에 단계 매핑 및 단계에 따라 분류 
- '기타' 전략의 비율을 최소화하여 감정 지원과 덜 관련된 응답 감소 
- 각 테스트 세트 간 대화 내용이 겹치지 않도록 일부 샘플 제거 
- 데이터 구성에 대한 보다 상세한 설명은 부록 B.2에 있음

---

# 3.3 Metrics

- **능력(proficiency)**: 모델이 적절한 전략을 선택하는 정도로, 각 전략에 대한 F1 점수로 정량화됨.
  - 사용되는 F1 점수 유형:
    - **매크로 F1 점수(Q)**: 모든 전략에 대한 전반적인 능력을 나타내며 전체 테스트 세트(D)에서 평가됨.
    - **가중 F1 점수**: 특정 단계에 해당하는 데이터만 포함된 테스트 세트(Dt)에서 모델을 평가하기 위해 사용됨.
  
- **선호(preference)**: 모델이 특정 전략을 다른 전략보다 얼마나 선호하는지를 정의.
  - **Bradley-Terry 모델**을 사용해 각 전략의 선호를 정량화.
  - 선호값(pi)은 초기 값으로 1로 설정되며, 반복을 통해 업데이트되고, 최종적으로 총합을 8로 조정하여 평균값(¯p)을 1로 만들어 특정 전략에 대한 선호가 강할 경우 pi > 1로 나타냄.

- **선호 편향(preference bias)**: 여러 전략 간의 선호의 표준 편차로 정의.
  - 높은 값의 B는 모델이 선호하는 전략과 비선호하는 전략에 대한 명확한 편향을 보여줌.

- **평가 목표**:
  - 이 문서에서는 LLM의 전략 선정 능력 및 선호가 정서적 지원을 제공하는 데 미치는 영향을 조사하고, 여러 접근 방식을 통해 선호 편향을 줄이기 위한 방법을 탐구함.

---

# 4 Proficiency and Preference of LLMs on Strategy

# 4.1 Models & Implementation Details

- **모델 분류**:
  - **폐쇄형 모델**: API를 통해 접근 가능, 예: ChatGPT, GPT-4.
  - **개방형 모델**: 파라미터로 접근 가능한 모델, 예: LLaMA2-7B/70B, Tulu-70B, Vicuna-13B, Solar-10.7B, Mistral-7B.

- **프롬프트 구성**:
  - 각 전략에 대한 이해를 높이기 위한 설명 포함.
  - 원하는 출력 형식 준수를 위한 난이도로 인해 무작위로 선택한 2-shot 예시 포함.
  - 비교를 위해 폐쇄형 모델의 2-shot 예시도 제공.
  - 모델에 대한 더 많은 정보는 부록 C.3에, 프롬프트에 대한 정보는 부록 C.4에 포함.

- **모델의 능숙도 (Q)**:
  - GPT-4가 능숙도 Q에서 가장 높은 점수를 기록.
  - 소형 모델들(Solar, LLaMA2-7B)도 비교적 좋은 성능 보임.
  - 각 모델의 성능은 테스트 세트에 따라 달라짐 (D1에서 낮은 성과, D2/D3에서 높은 성과).

- **감정 지원의 품질과 모델 선호도**:
  - LLM의 성능은 특정 상황에서 감정 지원이 부족할 수 있음을 시사.
  - 감정 지원은 탐색, 위로, 행동의 단계로 진행되며, 탐색 단계에서 저조한 성과는 다음 단계로 전환하는 데 방해가 됨.

- **선호도 편향과 안정성**:
  - 각 LLM이 전략에 대해 다른 선호도를 보임.
  - GPT-4는 탐색 단계에서 낮은 선호도를 가지며, 이는 D1에서의 낮은 성과와 일치.
  - LLaMA2-70B는 전략에 대한 균일한 선호를 보여 더 견고한 성능을 나타냄.

- **결론**: 
  - 높은 능숙도가 항상 도움이 되는 감정 지원을 보장하지 않음.
  - 전략 선택에서의 선호도 편향이 낮은 성과를 초래할 수 있으며 이는 감정 지원의 질적 저하로 이어질 수 있음.

---

# 4.2 RQ1: Does the preference affect providing emotional support?

- **LLMs의 능력**:
  - GPT-4가 가장 높은 능력 점수를 기록하며, 전반적으로 전략에 잘 맞춘다.
  - 소형 모델들(Solar 및 LLaMA2-7B)도 상대적으로 좋은 성능을 보인다.
  - 성능은 테스트 세트에 따라 다르며, 대부분의 모델이 D2 또는 D3에서 높은 점수를 기록하지만 D1에서는 낮은 점수를 기록한다.

- **감정적 지원 제공에서의 문제**:
  - LLM들은 탐색 단계(D1)에서는 감정적 지원의 질이 떨어지고, 이는 이후 단계로의 전환을 방해할 수 있다.
  - 높은 능력 점수를 기록하더라도, 효과적인 감정적 지원을 보장하지는 않는다.

- **선호 편향의 영향**:
  - 각 LLM은 전략에 대한 선호(pi)가 다르며, 높은 성능을 보이는 단계에서 선호도 역시 높다.
  - GPT-4는 탐색 단계에 대한 선호가 낮아 D1에서 성능이 낮다.
  - LLaMA2-70B는 전략에 대한 비교적 균일한 선호도를 보이며 다양한 단계에서 성능이 일관되게 나타난다.

- **결론**:
  - 높은 능력 점수를 가진 LLM이 특정 단계에서 강한 선호 편향을 보일 경우, 각 단계에서의 성능이 저하되어 전체적인 감정적 지원의 품질이 떨어질 수 있음을 시사한다.
  - 이는 감정적 지원을 제공할 때 LLM의 일관성과 효과성을 저해할 수 있는 요소로 작용한다.

---

# 5 Methodological Study: Mitigating Preference Bias

- **목표**: LLM의 선호 편향 완화 방법론 제안
- **모델 사용**:
  - ChatGPT (폐쇄형 모델)
  - LLaMA2-70B (개방형 모델)

---

# 5.1 Methods

- **접촉 가설(Contact Hypothesis)**: 두 그룹 간의 편견을 줄이는 데에 상호작용이 도움을 줄 수 있다는 이론에 기반하여, LLM(대규모 언어 모델)의 편향을 완화하기 위한 외부 지원의 가능성을 가설함.
  
- **카테고리화**: LLM을 위한 방법론을 두 가지로 분류:
  1. **자기 접촉(Self-contact)**
  2. **외부 접촉(External-contact)**

- **자기 접촉 방법(Self-contact approaches)**:
  - **정의**: 외부 상호작용 없이 LLM의 능력만을 활용하는 방법.
  - **사용된 방법**:
    - **Direct-Refine**: 모델이 최초 생성한 응답을 스스로 개선.
    - **Self-Refine**: 자기 피드백을 통한 응답 개선.
    - **Emotional-CoT**: 사용자 상태를 생성하여 응답 생성을 위한 추론 경로를 마련.

- **외부 접촉 방법(External-contact approaches)**:
  - **정의**: LLM이 자신의 내부 지식을 활용하는 것 외에 외부 지식으로부터 지원을 받는 방법.
  - **구현된 방법**:
    - **Commonsense Knowledge (COMET)** 활용: KEMI 모델과 유사하게 외부의 상식 지식을 이용.
    - **전략 계획자(Strategy Planner)**: 대화 맥락을 기반으로 지원자가 취해야 할 다음 전략을 예측하여 LLM의 응답을 생성.
    - **예제 확장(Example Expansion)**: 프롬프트에서 무작위로 선택한 예제의 수를 확대 (n=4).

- **사용된 자동 메트릭**: 다양한 방법의 효과를 측정하기 위해 Q, B, BLEU-2, ROUGE-L 같은 지표 사용. 결과의 질을 비교하기 위해 Appendix C.5에 자세한 내용 제공.

- **최종 목표**: 외부 지원을 통해 LLM의 감정적 지원 능력을 강화하고, 편향을 줄여서 보다 효과적인 응답을 생성하는 것.

---

# 5.2 RQ2: How to mitigate the preference bias on LLMs?

- **선행 연구 기반**: 선행 연구인 접촉 가설(Contact Hypothesis)을 기반으로, 두 집단 간의 편향을 줄일 수 있는 가능성을 제시함.
  
- **외부 지원의 필요성**: LLM의 편향을 완화하기 위해 외부 지원이 필요하다고 가정함.
  
- **방법 분류**:
  - **자기 접촉(self-contact)**: 외부 상호작용 없이 LLM의 능력만을 활용하는 방식.
    -  (1) **Direct-Refine**: 모델이 스스로 생성한 응답을 수정.
    -  (2) **Self-Refine**: 자체 피드백을 통해 응답을 정제.
    -  (3) **Emotional-CoT**: 사용자 상태를 생성하여 응답 생성 경로를 확립.
  
  - **외부 접촉(external-contact)**: LLM 내부 지식뿐만 아니라 외부 지식의 도움도 받는 방법.
    - **COMET**와 같은 상식 지식을 활용.
    - **전략 플래너**로 LLaMA2-7B 모델을 미세 조정하여 대화 상황에 따른 다음 전략을 계획함.
    - 예시 수(n)를 임의로 확장(n = 4)함.

- **방법의 부정적 효과**:
  - 자기 접촉 방법들은 LLM의 능력(Q) 감소 및 편향 증가(B)와 연관됨.
  - 자기 접촉 방법은 편향을 심화시켜 감정적 지원 역할을 못한 경우가 많음.

- **외부 접촉의 효과**:
  - 외부 접촉 방법들은 편향을 줄이고 성능을 향상시키며, 특히 전략 플래너와 더 많은 예시 활용이 긍정적인 결과를 보임.
  
- **푸시 효과**:
  - 예시 수가 증가할 때 편향이 악화되는 경향이 있음.
  
- **모델 간 성능 비교**:
  - Self-contact가 성능을 저하시킨 반면, External-contact 방법이 성능 개선에 기여함.
  
- **향후 연구 방향**:
  - LLM의 감정 지능 향상을 위해 외부 지원의 중요성을 강조하고, 후속 연구를 통해 LLM의 감정적 지원 능력을 증진할 필요성. 

이러한 요약은 LLM의 편향을 완화하기 위한 방법을 탐색하며, 자기 접촉과 외부 접촉의 영향에 대한 실질적인 통찰을 제공함.

---

# 5.3 RQ3: Does improving preference bias help to become a better emotional supporter?

- **연구 배경**: 인간의 편향을 줄이기 위해 서로 다른 집단 간의 접촉이 필요하다는 '접촉 가설(Contact Hypothesis)'에 기반하여, LLM의 편향을 완화할 수 있는 방법에 대해 연구함.
  
- **접촉 방법의 분류**:
  - **자체 접촉(self-contact)**: 외부의 도움 없이 LLM의 내부 능력만을 활용하여 응답을 개선하는 방법.
  - **외부 접촉(external-contact)**: LLM이 외부 지식의 도움을 받아 응답을 개선하는 방법.

- **자체 접촉 방식**:
  - *Direct-Refine*: 모델 스스로 초기 응답을 개선.
  - *Self-Refine*: 자기 피드백을 통해 응답을 개선.
  - *Emotional-CoT*: 사용자 상태를 기반으로 응답을 생성.

- **외부 접촉 방식**:
  - *COMET 활용*: 일반 상식 지식을 활용하여 응답을 개선.
  - *전략 계획자(strategy planner) 활용*: 대화 맥락에 기반해 다음 응답 전략을 예측하고, 이에 따라 응답 생성.
  - *예시 확장*: 프롬프트의 예시 수를 늘려 응답의 품질을 향상.

- **성과 결과**:
  - 연구에서는 외부 접촉 방식이 LLM의 편향을 줄이는 데 효율적임을 보여줌.
  - 외부 도움을 받을 경우, LLM의 응답 품질이 개선되며, 더 나은 정서적 지원이 가능해짐.
  - 특히, 전략 계획자를 활용한 경우 응답의 질과 전략 일치도가 높아짐.

- **편향 완화의 이점**:
  - 높은 편향을 경험한 LLM은 품질 저하가 발생하며, 정서적으로 부정적인 영향을 주는 응답 비율이 증가.
  - 편향을 완화할 경우, 정서적 지원의 품질을 높이고 나쁜 품질의 응답 수를 줄일 수 있음.
  
- **결론**:
  - 편향 개선이 LLM의 정서적 지원 능력을 향상시키는 데 유사한 역할을 한다는 것이 확인됨.
  - 향후 LLM의 정서적 지능을 향상시키기 위한 기초 연구로 가치가 있음.

---

# 6 Discussion and Conclusions

- 이 연구는 LLMs(대형 언어 모델)가 감정 지원을 제공하는 데 어려움을 겪는 이유를 전략 중심으로 분석함.
- 결과적으로 LLMs는 특정 전략에 대한 선호 편향이 있으며, 세 가지 감정 지원 단계에서 전략을 예측하는 데 강건성이 결여되어 있음.
- 특정 단계에서의 어려움이 다음 단계로의 진행을 방해할 수 있음.
- LLMs가 인간과 마찬가지로 심리적 접촉 가설과 일치한다는 것을 실증적으로 보여주며, 외부 지원이 LLMs의 선호 편향을 완화할 수 있음을 나타냄.
- 선호 편향 완화가 각 단계에서 적절한 전략의 선택 강건성을 강화하여 전반적으로 감정 지원의 질이 향상되고, 저품질 응답의 수가 크게 감소함.
- 이 연구가 LLMs의 감정 지능 향상을 위한 향후 연구의 유망한 발판이 되기를 희망함.

### 제한 사항
1. Cheng et al. (2022)의 연구에 따르면, '타인' 전략은 응답 생성 향상에 도움이 되지 않을 수 있음.
2. 오픈 소스 LLMs에 대해 2-shot 예제를 포함하여, 원하는 출력 형식 준수에 어려움을 겪음.
3. 선호 편향의 이유를 이해하기 어려움.
4. LLMs에서 오라클 전략을 사용할 때도 여전히 감정 강도를 증가시키는 응답이 존재함.
5. LLMs가 일반적으로 전략에 잘 정렬된 응답을 생성하지만, 일부 불일치 사례가 있음.

### 윤리적 고려사항
- ESConv 데이터세트는 공공에 사용 가능하고 잘 구축된 감정 지원 대화 벤치마크임.
- 모든 인간 평가 참여자는 자발적이며 연구 의도에 대해 투명하게 정보를 제공받고 합당한 임금을 지급받음.
- "감정 지원"이라는 용어는 주로 일상 대화에서의 사회적 맥락 내 지원을 나타내며, 전문 상담이나 진단은 포함되지 않음.
- LLMs의 사용은 해로운 콘텐츠 발생을 피하기 위한 특별한 주의가 요구됨.

---

# 독자 의견

- 해당 연구는 기존 ESConv 데이터셋과 그 프레임워크인 ESC에 대한 3가지 물음 (1. LLM의 전략 선호도가 감정 지원에 어떤 영향을 미치는가? 2. LLM의 전략 선호도를 완화하는 방법은 무엇인가? 3. LLM의 전략 선호도 완화가 감정 지원 능력을 향상시키는가?)에 대한 연구를 수행함.
- 본 연구는 어더한 데이터셋을 만들어내진 않지만, 기존 감정 지원 대화 시스템의 LLM에서의 활용에 대한 연구를 통해 LLM의 감정 지원 능력을 향상시키는 방법을 제시함.
- 본 논문에서 제안하는 방법론들을 통해 더 나은 감정 지원 대화 시스템을 구축할 수 있을 것으로 기대됨.