---
title: "[논문리뷰] Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction (ACL 2024)"
date: 2025-03-08 21:00:00 +0900
categories:
  - Paper Review
tags:
  - NLP
  - ACL 2024
  - Persona-based Dialogue
---

요약: 이 논문은 음성 인식을 통합하여 인간 의사소통의 뉘앙스를 이해하는 다중 모달 대화 시스템 'PerceptiveAgent'를 제안하며, 이를 통해 화자의 진정한 의도를 더 잘 파악하고 공감적인 반응을 생성하는 방법을 다룹니다.

---

# 1 Introduction

- 인공지능(AI) 에이전트는 인간과 유사한 지능과 기능을 복제하기 위해 설계된 엔티티로, AI 시스템의 필수 구성 요소임.
- 이상적인 에이전트는 환경을 감지하고, 정보에 기반한 결정을 내리며, 사용자나 상황에 대응하여 행동해야 함.
- 최근 대형 언어 모델(LLM)은 다양한 작업에서 뛰어난 능력을 보여주며, 인간과 유사한 상호작용을 위한 기회를 제공함. 
- 그러나 현재의 텍스트 기반 대화 시스템은 실험적 환경과 현실적 시나리오 간의 간극을 메우는 데 한계가 있음.
- 음향 정보를 대화에 통합하면 보다 인간과 유사한 에이전트를 개발하고, 공감적인 경험을 향상시킬 수 있는 잠재력이 있음.
  
- 공감적 반응은 두 가지 주요 측면인 인지 공감과 정서 공감을 포함함.
  - 인지 공감은 대화 상대방의 생각과 관점을 이해하여 적절한 반응을 가능하게 함.
  - 정서 공감은 대화 이력에서 관찰된 감정 표현에 기반하여 반응함으로써 자연스러운 언어 생성을 돕는 역할을 함.
  
- 최근 연구들은 LLM의 강력한 맥락 이해 및 콘텐츠 생성 능력을 활용하여 공감적인 언어를 합성하는 성과를 보였지만, 인지 공감과 정서 공감 간에 여전히 불일치가 존재함.
  
- 다중 모달 콘텐츠 인식 및 생성의 발전이 이루어졌지만, 음향 특성이 간과될 경우 발화자의 의도가 잘못 해석될 수 있음.
  
- 본 논문에서는 PerceptiveAgent라는 공감적 다중 모달 대화 시스템을 제안하며, 발화의 음향 특성을 기반으로 심층적인 의미를 파악하도록 설계됨.
  - PerceptiveAgent는 발화자의 의도를 정확히 이해한 후 적절한 반응 콘텐츠를 생성하고, 다감각적 음성을 합성하는 MSMA-Synthesizer를 활용함.
  
- 주요 기여사항:
  - 음향 정보를 자연어로 표현할 수 있는 발화 캡셔너 모델을 개발함.
  - 오디오 인식과 공감적 언어 생성을 통해 발화자의 진정한 의도를 파악할 수 있는 공감적 대화 시스템을 개발함.
  - PerceptiveAgent가 발화자의 실제 감정과 반대되거나 일관되지 않은 문자적 해석을 잘 구별할 수 있음을 실험을 통해 입증함.

---

# 2 Related Work

- **다중 모드 대화 시스템 (Multi-modal Dialogue Systems)**  
  - 최근 다중 모드 대화 시스템은 음성을 이산 잠재 표현으로 변환하는 데 중점.
  - Zhang et al. (2023), Chen et al. (2023), Wu et al. (2023)는 음성 인코더를 활용하여 음성을 인식하고 LLM으로부터 얻은 이산 음향 단위를 사용하여 응답 생성.
  - Nguyen et al. (2022), Mitsui et al. (2023)와 같은 연구에서는 두 채널의 대화 음성을 자율 생성해 현실적인 상호작용을 모사.
  - 이산 음향 단위는 언어 정보를 효과적으로 캡처하지만, 운율적 특성을 대체로 무시.
  - 이 제한을 극복하고 운율 정보를 최대한 보존하기 위해, 우리는 음성 자막을 통해 운율을 인식하고 LLM과 음성 합성기를 사용하여 공감하는 방식으로 응답하는 다중 모드 대화 시스템을 개발.

- **교차 모드 텍스트 생성 (Cross-Modal Text Generation)**  
  - 교차 모드 텍스트 생성은 오디오 및 비전과 같은 다른 모드에 조건화된 텍스트 생성을 포함.
  - 주요 도전 과제는 다중 모드 특성을 텍스트 잠재 공간과 정렬하는 것.
  - 최근 방법론은 미리 학습된 LLM을 학습 가능한 시각 인코더와 정렬하여 다중 모드 표현을 학습 가능한 쿼리 임베딩으로 변환, LLM과 시각 인코더는 고정.
  - 오디오 캡션 작업에 대해, 오디오 임베딩을 프리픽스 벡터 시퀀스로 매핑하고 이를 캡션 생성의 컨텍스트 입력으로 사용.
  - 우리가 아는 한, 우리는 대화에서 음향 정보를 인식할 수 있는 음성 자막 생성기를 처음으로 구축.

- **표현력 있는 텍스트-음성 합성 (Expressive Text-to-Speech Synthesis)**  
  - 텍스트 음성 합성(TTS) 모델은 주어진 전사에 대해 제로샷 음성 프롬프트 또는 원하는 스타일의 텍스트 프롬프트에 조건화하여 음성 변화를 달성.
  - 제로샷 TTS 시스템은 맥락 학습을 통해 음성 프롬프트의 화자 특성과 음향 환경을 재현.
  - 그러나 이러한 시스템은 운율, 감정 및 음향 환경에 대한 독립적인 제어 부족.
  - 더 자연스럽고 일반적인 음성 합성을 위해 텍스트 프롬프트가 도입됨.
  - 자연어로 말하기 스타일을 표현하는 방법과 명시적 라벨을 사용하여 프롬프트에 맞는 다양한 음성을 생성하는 방법이 있음.
  - 우리는 후자의 방향을 따르며 여러 말하기 스타일 라벨을 가진 음성 합성 모델을 구축.

---

# 3 Methods

- **PerceptiveAgent**: 다중 모달 대화 시스템으로, 오디오 모달리티 인식 및 공감적인 음성 생성을 수행.
  - 자연어에서 표현된 운율 정보를 통합하여 기능을 구현.

- **Speech Caption Model**: 음성 입력에서 운율적 특징을 캡처하고 이를 텍스트 설명으로 전사.
  - ImageBind의 음성 인코더로 음성을 인코딩하고, GPT-2 디코더로 설명 생성.
  - Q-former를 도입해 음성 인코더와 텍스트 디코더 간의 브릿지 역할.

- **Fine-tuning Strategies**:
  - **Multi-modal Embedding Alignment**: Q-former의 출력을 문맥 공간에 맞추기 위해 prefix tuning을 사용.
    - 고정 차원의 쿼리 벡터를 생성하고, 셀프 어텐션 및 크로스 어텐션 레이어를 통해 상호작용.
  - **Instruction Tuning**: Q-former와 텍스트 디코더의 목표 차이를 해소하기 위해 사용.
    - 쿼리 벡터, 지시문, 캡션으로 구성된 데이터셋을 구축.
    - 모델의 출력이 기대하는 응답 특성과 도메인 지식에 맞게 제한.

- **PerceptiveAgent의 프레임워크**:
  - 세 가지 상호 연결된 단계로 구성: 의도 파악(음성 캡셔너), 감지 통합(LLM), 표현적 음성 합성(MSMA-Synthesizer).
  - 자연어를 통해 음성 정보를 인식하고 표현하며, LLM을 인지의 핵심으로 사용.

- **각 단계의 세부 설명**:
  - **의도 파악**: 음성 캡셔너를 사용해 오디오 입력의 음향 정보를 해석.
    - 음성을 잠재적 특징으로 인코딩하고 이를 쿼리 벡터로 압축.
  - **감지 통합**: LLM 모듈 통합, GPT-3.5-Turbo 사용.
    - 텍스트 내용과 생성된 캡션을 LLM에 통합하여 의도로부터 대화 내용을 추론.
  - **표현적 음성 합성**: MSMA-Synthesizer로 공감적인 음성 응답 생성.
    - 생성된 대화 내용과 캡션을 조건으로 하여 음성을 합성.
    - 다양한 운율 속성을 활용해 음성을 조절하고, T2U 모델로 응답 내용을 음향 단위로 변환.

---

# 4 Experiments

- **실험 설정**  
  - **데이터셋**  
    - TextrolSpeech 데이터셋 사용, 236,220개의 자막-음성 샘플 쌍 포함  
    - 자막은 성별, 감정, 음조, 속도, 에너지의 다섯 가지 요소 설명  
    - MSMA-Synthesizer는 이전 연구에서 제안한 보코더를 재현  
      - EXPRESSO, LJSpeech, VCTK 데이터셋 활용  
      - 보코더 훈련 후 자막 생성 및 속성 인식을 위해 EXPRESSO 데이터셋 레이블링  
    - MELD 대화 데이터셋으로 성능 평가  
      - 감정 레이블 제공, 사실적 환경 소음 포함  
      - 전체 훈련 과정에서 영어 데이터셋 사용, 다국어 확장 가능  
  - **구성**  
    - ImageBind의 음성 인코더, BuboGPT의 Q-former, GPT-2 활용  
    - fine-tuning: 43,000 스텝, 배치 크기 16  
    - 디코딩: Top-k 샘플링, 최소 및 최대 시퀀스 길이 20, 50 설정  
    - 보코더: 400,000 스텝 훈련, MSMA-Synthesizer: 200,000 스텝 훈련  
    - T2U 모델: Transformer 구조, 4개의 인코더, 디코더 레이어, 4개의 주의 헤드  
    - 모든 실험은 NVIDIA GeForce RTX 4090 GPU 4대에서 수행  

- **평가**  
  - **Speech-GPT3.5**: 대화 시스템의 기초 모델로, 오직 언어 정보에 중점을 두어 GPT-3.5-Turbo로 대화 맥락 이해  
  - **지표**  
    - 인지적 공감 및 정서적 공감을 평가: 
      - 인지적 공감: 생성된 텍스트 응답의 품질  
      - 정서적 공감: 생성된 오디오 응답의 표현력 평가  
    - BERTScore를 사용하여 대화 텍스트 생성 품질 평가  
    - 오디오 생성의 표현력은 감정 분류기로 평가  
    - Speech Captioner의 인식 능력 평가: 여러 속성을 가진 다중 속성 분류 작업  
    - F0 Frame Error (FFE)로 프라소디 보존 평가  

- **결과 분석**  
  - **PerceptiveAgent**: 전체 성능이 인지적 및 정서적 공감에서 Speech-GPT3.5보다 우수  
    - 생성된 답변이 화자의 의도를 더 정확히 포착하는 경향  
  - **Speech Captioner**: 성별, 감정, 음조 인식에서 높은 F1-score  
    - 성별에 따른 성능 차이 관찰됨, 남성과 여성의 예측 정확성 차이  
  - **MSMA-Synthesizer**: 감정과 프라소디 보존에서 높은 정확도를 기록  
    - 여러 속성 통합이 감정적으로 표현력 있는 오디오 생성에 효과적  
  - **ABI Study (Ablation Study)**:  
    - 자막의 효과성: 자막이 없는 시스템보다 성능 향상  
    - 스타일 요소의 효과성: 기본 스타일 갖춘 모델이 가장 높은 성능 기록

---

# 5 Case Study

- Figure 3에서는 Speech-GPT3.5와 PerceptiveAgent 간의 응답 품질을 비교한 두 가지 사례를 제시
- 자막을 통해 음향 정보를 명시적으로 포함함으로써 LLM이 화자의 의도를 더 정확히 이해하고 더 적합한 응답을 생성 가능
- 첫 번째 예시:
  - 비계획적인 만남 중 두 친구 간의 대화
  - 텍스트 내용만 보면 화자 B가 대화에 매우 흥분하고 기뻐하는 것처럼 보임
  - 그러나 "lower vocal"과 "subbed energy"라는 키워드를 통해 회피적인 태도가 드러남
  - 화자 A의 질문 "Were you here waiting for me?"에 대한 화자 B의 반응은 대화를 길게 하고 싶지 않음을 암시
  - Speech-GPT3.5는 미세한 자막의 부재로 잘못 해석하여 대화 지속을 원한다는 응답 생성
  - 반면 PerceptiveAgent는 화자의 의도를 반영하여 적절한 응답 제공
- 두 번째 예시:
  - 화자 A가 어머니로부터 종이를 받고 친구들과 공유하고 싶어함
  - "treble tone"과 "energetically" 키워드를 통해 화자 A의 흥분한 기분 파악 가능
  - PerceptiveAgent는 화자 A의 흥분을 반영하여 응답 생성, 실제와 잘 맞음
  - Speech-GPT3.5는 화자 A의 흥분을 인지하지 못하고 평범한 질문만 제기
- 결론:
  - 텍스트 내용이 화자의 의도와 일치하는 경우에도 PerceptiveAgent가 대화 상황에 맞는 응답을 제공할 수 있음

---

# 6 Conclusion

- PerceptiveAgent는 공감적이고 다중 모달 대화 시스템으로, 말하는 사람의 의도를 정확히 파악합니다.
- 시스템은 다음 세 가지 모듈로 구성됩니다:
  - 의도 인식을 위한 음성 캡셔너
  - 감각 통합을 통한 이해를 가진 대형 언어 모델(LLM)
  - 표현적인 음성 합성을 위한 MSMA-Synthesizer
- 음성 캡셔너는 대화 중 각 발화에서 음향 특징을 잡아냅니다.
- LLM 모듈은 발화자의 의도를 이해하여 적절한 응답 내용을 생성합니다.
- MSMA-Synthesizer는 표현적인 음성을 합성하는 역할을 합니다.
- 실험 결과, PerceptiveAgent는 공감적 응답 생성에서 강력한 능력을 보여주며, 언어적 내용과 음향 정보에서 높은 표현력을 나타냅니다.
- 사례 연구를 통해 PerceptiveAgent는 말하는 사람의 진정한 감정과 반대되거나 일치하지 않는 경우에도 의도를 정확히 식별할 수 있음을 보여줍니다.

---

# 7 Limitations

- **데이터셋 제한**: 
  - PerceptiveAgent의 인식 능력은 훈련 데이터셋의 포괄성에 의해 제한됨.
  - 현재 화자의 신원 및 배경 소음을 음성에서 구별할 수 없음.

- **시간 지연 제한**: 
  - PerceptiveAgent는 세 개의 상호 연결된 구성 요소로 구성되어 있어 응답 시간에 누적 지연이 발생함.

- **길이 제한**: 
  - LLM의 최대 토큰 길이로 인해 다중 턴 대화에 제약이 있을 수 있음.

---

# 독자 의견

- 본 논문은 사용자 음성의 운율(강세, 음조, 속도 등) 정보를 텍스트로 변환하고, 이를 통해 공감적인 대화를 생성하는 방법을 제안함.
- 제안하는 주 모델은 음성의 켑션을 다는 Speech Caption Model이며, 그 안의 Q-former가 음성 인코더와 텍스트 디코더를 연결하는 역할을 함.
- 모델의 학습은 Speech Caption Model에서만 이루어지며, Q-former와 Text Decoder(GPT-2)를 켑셔닝 데이터에 대해 학습함.
- Speech Caption Model 이후 추론이 진행되는 LLM과 MSMA-Synthesizer는 학습이 아닌 추론만을 수행함.
- 이는 모델이 end-to-end로 학습되지 않았다는 것을 의미하며, 학습 데이터에 대한 과적합이 발생할 수 있음.
- 하지만 LLM을 직접 학습하는 것은 매우 비용이 많이 들기 때문에, 이러한 방식을 선택한 것으로 보임.
- 사용자의 음성 정보와 발화 텍스트를 latent space로 변환하여 음성 출력까지 전달하는 방법은 없을까 고민해 볼 필요가 있음.