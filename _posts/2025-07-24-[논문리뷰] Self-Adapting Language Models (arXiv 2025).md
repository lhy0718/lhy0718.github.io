---
title: "[논문리뷰] Self-Adapting Language Models (arXiv 2025)"
date: 2025-07-24 16:29:07 +0900
categories:
  - Paper Review
tags:
  - arXiv 2025
  - NLP
---

본 논문은 LLM이 스스로 미세조정 데이터를 생성하고 가중치를 지속적으로 업데이트하여 새로운 과제나 지식에 적응할 수 있도록 하는 Self-Adapting LLMs(SEAL) 프레임워크를 제안한다.

---

# 1 Introduction

- 대규모 언어 모델(LLMs)은 방대한 텍스트 코퍼스에서 사전학습되어 언어 이해 및 생성에 뛰어난 능력을 보임.
- 하지만, 특정 과제 적응, 신규 정보 통합, 새로운 추론 능력 습득은 과제별 데이터 부족으로 어려움이 있음.
- 본 연구는 LLM이 자체적으로 훈련 데이터와 학습 절차를 변형 또는 생성하여 스스로 적응할 수 있는지 탐구.
- 인간 학생이 기계학습 시험을 준비할 때, 원자료(raw content) 대신 노트로 내용을 재구성하고 재작성하여 이해도를 높이는 현상에 착안.
  - 인간은 정보를 시각적 도표, 텍스트, 수학적 기술 등 다양한 방식으로 재구성함.
- 기존 LLM은 새로운 과제에 대해 주어진 데이터를 "있는 그대로" 사용하여 파인튜닝이나 인컨텍스트 학습을 수행하나, 데이터 형식이나 양이 최적이 아닐 수 있음.
- 현재 방법들은 모델이 훈련 데이터를 가장 잘 변형·학습하는 맞춤 전략을 개발하는 것을 지원하지 않음.
- 제안: LLM에 자체 훈련 데이터와 파인튜닝 지침을 생성하는 능력을 부여하여 확장 가능하고 효율적인 적응 달성.
- 구체적으로, 강화학습 알고리즘을 통해 자연어로 된 "self-edits"—데이터 및 선택적 최적화 하이퍼파라미터 지침—를 생성하도록 LLM을 훈련.
  - 이를 Self-Adapting LLMs(SEAL)라고 명명.
- SEAL의 두 가지 응용 평가:
  1. 새로운 사실 지식 통합:
     - 원본문 텍스트 대신 SEAL이 생성한 합성 데이터를 이용하여 파인튜닝 수행.
     - 강화학습 후, no-passage-in-context 방식의 SQuAD 데이터셋에서 질문응답 성능이 33.5%에서 47.0%로 상승.
     - SEAL 자체 생성 데이터가 GPT-4 생성 합성 데이터를 능가함.
  2. 소수 예제 학습:
     - ARC-AGI 벤치마크의 단순화된 하위 집합에서, SEAL이 합성 데이터 증강과 최적화 하이퍼파라미터(학습률, 학습 epoch, 토큰 유형별 선택적 손실 계산 등)를 자율적으로 선택.
     - SEAL을 이용한 자동 도구 선택 및 구성은 표준 인컨텍스트 학습 및 강화학습 없는 self-editing 대비 성능 향상 입증.
- 종합적으로, SEAL은 언어 모델이 스스로 적응할 수 있도록 하는 다목적 프레임워크임.

---

# 2 Related Work

- **합성 데이터 생성 (Synthetic Data Generation)**
  - 대규모 사전학습 데이터셋[15, 16, 17], 과제별 데이터 증강[18, 19], 지침 튜닝 세트[20, 21] 등에서 합성 데이터 생성이 점점 보편화되고 있음.
  - Yang et al.[22]은 그래프 기반 프롬프트를 사용해 소규모 코퍼스를 합성 데이터로 생성함.
  - SEAL은 강화학습(reinforcement learning)을 이용해 합성 데이터 생성 정책을 학습하며, 수동 조정된 정적인 생성 전략 대신 그라디언트 기반 자체 업데이트에 유용한 데이터를 직접 극대화함.

- **지식 업데이트 (Knowledge Updating)**
  - 언어 모델에 사실적 지식을 주입하거나 수정하기 위해 가중치 업데이트를 사용하는 연구가 있음.
  - 일부는 개별 사실에 대응하는 파라미터를 직접 찾으려 하며[23, 24, 25], 다른 연구는 컨텍스트 정보를 활용해 추가 미세조정 데이터를 생성함[26, 27, 22, 28, 29].
  - 본 연구는 Akyürek et al.[27]과 Lampinen et al.[28]의 방법을 따르며, 사실에 대한 논리적 함의를 생성해 미세조정하는 방식을 RL로 최적화함.
  - Park et al.[29]은 QA 쌍 생성을 통한 프롬프트가 함의 생성 방식보다 더 나은 성능을 보임을 제시.
  - SEAL은 프롬프트와 출력을 변경하는 유연성을 지니며, §C에서 QA 쌍 등 다양한 형식 학습도 가능함.
  - 충분한 계산 자원이 있다면, 모델이 프롬프트 없이 최적 포맷을 발견하는 콜드 스타트 설정도 가능함.

- **테스트 시 학습 (Test-Time Training, TTT)**
  - TTT는 모델 입력에 따라 임시로 가중치를 조정하는 방식임[30, 31, 32, 33].
  - Akyürek et al.[33]는 TTT와 ICL(컨텍스트 내 학습)을 결합했을 때, 그라디언트 업데이트가 소수 샘플 상황에서 표준 ICL을 능가함을 보여줌.
  - SEAL은 내부 최적화 루프에서 TTT 단계를 포함해 다수의 업데이트를 수행하며, 향상된 성능을 내는 데이터를 보상함.
  - 단일 예 기반 TTT 에피소드로 학습되었으나, 데이터 직접 컨텍스트 제공이 어려운 경우의 연속적 사전학습 설정에도 일반화됨을 입증함.

- **대형 언어 모델을 위한 강화학습 (Reinforcement Learning for LLMs)**
  - 강화학습은 RLHF[34]를 통해 LLM 행동 개선에 핵심 역할을 했음.
  - 최근에는 검증 가능한 보상을 활용해 직접 과제 성공률을 높이는 RL 적용 예가 증가[35, 36, 37].
  - SEAL은 최종 답변이나 추론 과정 최적화가 아니라, 가중치 업데이트에 활용될 자체 편집 데이터 생성을 RL로 최적화함.

- **메타러닝 및 자기 수정 시스템 (Meta-Learning and Self-Modifying Systems)**
  - SEAL은 외부 최적화 루프를 통해 효과적인 자체 편집 생성 전략을 학습하는 메타러닝 원리를 구현[38, 39, 40].
  - 메타러닝은 RL에서도 신속 적응을 위한 메타목적 함수 학습으로 활용됨[41, 42, 43, 44, 45].
  - 자기 참조 네트워크(self-referential networks)는 모델이 스스로 파라미터를 수정함[46, 47].
  - LLM 분야에서는 적은 모델을 이용해 토큰별 가중치를 출력하는 방법 등 지식 주입 과제에 메타러닝이 적용됨[48, 49].
  - SEAL은 기존 생성을 활용해 업데이트를 파라미터화한다는 점에서 보다 범용적임.

- **자기 향상 (Self-Improvement)**
  - RLAIF[50, 51], 자기 보상 언어 모델[52, 53] 등 모델 내부에서 보상 신호를 생성하는 자기 향상 기법 연구가 있음.
  - 다수결 투표나 모델 신뢰도를 보상으로 삼아 레이블 없이도 수학 문제 성능을 개선하는 방법도 존재[55, 56, 57, 58, 59].
  - 그러나 이들 방법은 모델의 평가 능력과 자기 일관성에 근본적 한계가 있음.
  - 반면 SEAL은 외부 데이터를 활용하는 상호작용을 통한 자기 향상을 더 강력하고 확장 가능하다고 봄.
  - SEAL은 외부 데이터를 최대한 효율적으로 이용하는 법을 학습함.

---

# 3 Methods

- 본 논문에서는 Self-Adapting LLMs (SEAL)이라는 프레임워크를 제안함.
- SEAL은 언어 모델이 새로운 데이터에 반응하여 스스로 합성 데이터와 최적화 파라미터("self-edits")를 생성함으로써 스스로를 개선할 수 있게 함.
- 모델은 모델 컨텍스트에 제공된 데이터를 통해 토큰 생성을 직접하여 이러한 self-edits를 생성하도록 훈련됨.
- self-edit 생성은 강화학습(RL)을 통해 학습되며, 적용 시 모델 성능이 향상되는 self-edits (SE)를 생성할 경우 보상을 받음.
- SEAL은 두 개의 중첩된 루프를 가진 알고리즘으로 해석 가능:
  - 외부 RL 루프: self-edit 생성을 최적화함.
  - 내부 업데이트 루프: 생성된 self-edit를 사용하여 그래디언트 하강법으로 모델을 업데이트함.
- 이 방법은 메타러닝의 한 형태로 볼 수 있으며, 효과적인 self-edit 생성 방법을 메타학습(metalearning)함.

---

# 3.1 General Framework

- 모델 파라미터를 $$\theta$$, 언어 모델을 $$LM_{\theta}$$로 표기  
- SEAL은 개별 태스크 인스턴스 $$(C, \tau)$$에서 작동  
  - $$C$$: 태스크 관련 정보를 담고 있는 컨텍스트  
  - $$\tau$$: 모델 적응을 평가하는 다운스트림 평가 방식  
  - 예시:  
    - 지식 통합: $$C$$는 모델 내부 지식에 통합할 지문, $$\tau$$는 지문에 대한 질문과 3개의 정답 세트  
    - Few-shot 학습: $$C$$는 몇 개의 예시 데모, $$\tau$$는 쿼리 입력과 정답 출력

- 주어진 $$C$$에 대해 모델은 도메인에 따라 형태가 달라지는 자기 편집(Self-Edit) $$SE$$을 생성(§3.2 참고)  
- 생성한 $$SE$$를 이용해 감독 학습된 미세 조정(Supervised Fine-Tuning, SFT)을 수행하며 파라미터 업데이트:  
  $$
  \theta' \leftarrow SFT(\theta, SE)
  $$

- 자기 편집 생성 과정을 강화학습으로 최적화  
  - 모델은 행동(자기 편집 생성)을 수행  
  - 평가 기준 $$\tau$$에 대해 $$LM_{\theta'}$$의 성능으로 보상 $$r$$을 받음  
  - 정책 업데이트로 기대 보상 최대화 목표  
  $$
  L_{RL}(\theta_t) := -\mathbb{E}_{(C,\tau) \sim D} \left[ \mathbb{E}_{SE \sim LM_{\theta_t}(\cdot|C)} [r(SE, \tau, \theta_t)] \right] \quad (1)
  $$

## 알고리즘 1: Self-Adapting LLMs (SEAL) - Self-Edit Reinforcement Learning Loop
1. 입력: 모델 $$LM_{\theta}$$, 데이터셋 $$D = \{ (C, \tau) \}$$  
2. 반복 $$t=1,2,\ldots$$  
3. $$ (C, \tau) \sim D $$에서 샘플링  
4. 자기 편집 생성 $$SE \sim LM_{\theta}(\cdot \vert C)$$  
5. 내적 루프 업데이트: $$\theta'_t \leftarrow SFT(\theta_t, SE)$$  
6. 평가: $$Ans \sim LM_{\theta'_t}(\cdot \vert \tau)$$  
7. 보상 계산: $$r \leftarrow r(Ans, \tau)$$  
8. 업데이트: $$\theta_{t+1} \leftarrow RL\_Update(\theta_t, r, SE)$$  

## 주요 특징 및 문제점
- 보상이 행동 당시에 파라미터 $$\theta$$에 의존 (행동 후 파라미터 $$\theta'$$로 평가됨)  
- 따라서 RL 상태는 정책 파라미터를 포함하여 $$(C, \theta)$$으로 정의됨  
- 과거 파라미터 $$\theta_{old}$$로 수집된 (상태, 행동, 보상) 데이터는 현재 $$\theta_{current}$$와 맞지 않아 낡은 정보가 될 수 있음  
- 이에 따라 현재 모델 정책에서 샘플링하고 평가도 현재 모델로 수행하는 온-폴리시(on-policy) 접근 방식 채택  

## 학습 방법 및 보상 함수
- GRPO, PPO 등 기존 온-폴리시 방법은 불안정함 발견  
- 대신 필터링된 행동 클로닝 기반 ReSTEM [36] 채택 (`rejection sampling + SFT`)  
- ReSTEM:  
  - E-스텝: 현재 정책에서 후보 샘플 생성  
  - M-스텝: 보상 양성인 샘플만 감독 학습으로 강화  
- 보상 함수는 다음의 이진 함수:  
  $$
  r(SE, \tau, \theta_t) = \begin{cases}
    1, & \text{만약 } SE \text{를 이용한 적응이 } LM_{\theta_t} \text{ 성능 개선 시} \\
    0, & \text{그 외}
  \end{cases}
  \quad (2)
  $$

## 최적화 및 기울기 계산
- 목적 함수의 그라디언트 $$\nabla_{\theta_t} L_{RL}$$를 계산해야 하나,  
- $$r(SE, \tau, \theta_t)$$는 미분 불가능하므로 보상을 상수로 간주하여 근사  
- 미니배치 샘플 $$N$$개 컨텍스트, 각 컨텍스트당 $$M$$개 자기 편집으로 Monte-Carlo 추정:  
  $$
  \nabla_{\theta_t} L_{RL} \approx -\frac{1}{NM} \sum_{i=1}^N \sum_{j=1}^M r_{ij} \nabla_{\theta_t} \log p_{\theta_t}(SE_{ij}|C_i) \quad (3)
  $$
  $$
  = - \frac{1}{NM} \sum_{i=1}^N \sum_{j=1}^M r_{ij} \sum_{s=1}^T \nabla_{\theta_t} \log p_{\theta_t} \left( y^{(i,j)}_s \middle| y^{(i,j)}_{<s}, C_i \right) \quad (4)
  $$
  - 여기서 $$p_{\theta_t}$$는 모델의 자기회귀 분포  
  - $$y^{(i,j)}_s$$는 $$SE_{ij}$$의 $$s$$번째 토큰  
  - 보상 $$r=0$$인 샘플은 무시 가능하여, ReSTEM은 "보상이 좋은 자기 편집에 대해 SFT하기"로 (1)과 (2)를 최적화함  

## 추가 사항
- 현재는 하나의 모델이 자기 편집 생성과 학습 역할을 모두 수행  
- 분리하여 `teacher-student` 구조도 가능  
  - teacher 모델: 편집 생성, RL 학습으로 student 성능 향상 유도  
  - student 모델: teacher의 편집을 통해 업데이트

---

# 3.2 Domain Instantiations

- SEAL 프레임워크는 두 가지 상이한 도메인에서 인스턴스화됨:
  1. **지식 통합(Knowledge Incorporation)**: 문맥에 의존하지 않고 모델 가중치 내에 새로운 정보를 통합하여 회상할 수 있는 능력 (무문맥 SQuAD를 사용해 평가).
  2. **Few-Shot 학습**: 소수의 예시만으로 새로운 작업에 일반화할 수 있는 능력 (ARC 데이터셋으로 평가).

---

### 지식 통합 (Knowledge Incorporation)

- 목표: 문서 내 정보를 모델의 가중치에 효율적으로 통합하는 것.
- 최근 접근법:
  - Passage에서 파생된 내용을 LM으로 생성한 뒤, 원문과 생성된 내용을 함께 미세조정(finettuning)하는 방법이 있음.
- 본 연구에서는 다음과 같은 정형화된 형식을 채택:
  - 문서에서 도출된 **함의(implications)** 집합 SE = $$\{s_1, s_2, \ldots, s_n\}$$ 생성 (deductive closure training 방법론 적용).
  - 모델에게 "내용에서 파생된 여러 함의를 나열하라"고 프롬프트를 주어 생성.
  - 생성 결과는 추론, 논리적 결과, 혹은 원문의 재진술 등을 포함.
- 대안 프롬프트 실험도 수행하였으며, "다른 방식으로 문서 재작성"이나 "질문-답변 형식으로 재작성"도 성능 향상에 효과적임을 확인 (§C 참고).
  
#### 학습 과정

- 생성된 함의들은 **self-edit**로 사용되어 감독학습(SFT)의 데이터가 됨.
- 각 시퀀스 $$s_i$$에 대해 표준 인과적 언어 모델링 손실을 계산하고 모델 파라미터 $$\theta$$를 업데이트해 $$\theta'$$를 획득.
- 데이터가 적고 업데이트가 많으므로, 효율적이고 경량화된 LoRA [67]를 사용해 미세조정 수행.
- 최종적으로 적응된 모델 $$LM_{\theta'}$$는 원문 없이 해당 문제(τ)에 대해 평가됨.
- RL 단계에서는 τ 작업에서의 정확도가 보상 $$r$$으로 사용되어 외부 RL 최적화를 구동.
- 모델은 finetuning을 통해 가장 효과적으로 정보를 흡수할 수 있도록 passage를 재구성하도록 학습됨.

---

### Few-Shot 학습

- 사용 데이터셋: Abstraction and Reasoning Corpus (ARC) [8], 매우 적은 학습 예시로 추상적 추론 및 일반화 능력 평가.
- 프로토콜: Akyürek et al. [33]의 테스트 시점 학습(Test-Time Training, TTT).
  - 소수의 예시를 데이터 증강하여 경사기반 적응 수행.
- 차별점:
  - 데이터 증강과 최적화 설정을 수동으로 조정하지 않고 SEAL이 자동으로 학습.
  - 모델이 증강 적용 여부, 최적화 파라미터 설정 등을 스스로 선택.
- 구현 방법:
  - Akyürek 외 [33]에서 정의한 여러 도구(tool) 함수들을 사용.
  - 데이터 증강 종류: 회전, 뒤집기, 반사, 전치, 크기 조정(격자 해상도 변경), 연쇄(chained) 또는 반복 변환.
  - 최적화 매개변수: 학습률, epoch 수, 전체 토큰 혹은 출력 토큰에 대한 손실 계산 여부.
- 과정:
  - 작업의 few-shot 데모를 입력으로 주고 self-edit를 생성.
  - self-edit는 사용할 도구 및 설정을 지정하는 명세.
  - 이를 기반으로 LoRA 미세조정을 통해 모델 적응.
  - 적응된 모델은 테스트 입력에 대해 평가되고, 결과가 self-edit 생성 정책 학습의 보상으로 사용됨.

---

요약하면, SEAL은  
- 지식 통합에서는 문서에서 함의를 생성해 이를 기반으로 모델을 미세조정하여 새로운 정보를 가중치에 흡수함으로써 무문맥 질문에 답할 수 있게 함.  
- Few-shot 학습에서는 소수 예시 증강과 최적화 전략을 자동으로 결정 및 적용해 새로운 작업에 빠르게 적응하도록 함으로써, 두 도메인의 상호 보완적인 적응 능력을 보여줌.

---

# 4 Results

- 본 섹션에서는 SEAL을 두 가지 적응 도메인, 즉 소수 샷 학습(few-shot learning)과 지식 통합(knowledge incorporation)에서 실험적으로 평가함.
- 전체 훈련, 하이퍼파라미터 및 평가 세부 사항은 부록 §A와 §B에 제공됨.

---

# 4.1 Few-Shot Learning

- 실험은 ARC(Abstraction and Reasoning Corpus) 특정 사전학습 없이, 소형 오픈소스 모델인 Llama-3.2-1B-Instruct를 사용하여 수행함.
- 대부분의 ARC 작업은 ARC 사전학습이 없는 모델에게 어려움이 있으므로, ARC 학습 데이터셋에서 11개 작업, 평가 데이터셋에서 8개 작업을 선별하여 최적의 TTT(Test-Time Training) 구성 하에서 해결 가능한 작업만 사용함.
- 선별된 작업 수는 적지만, Akyürek et al. [33]에서 모든 작업에 동일한 TTT 구성을 사용하였기 때문에 고정된 자기 수정(self-edit)을 학습하는 데 많은 훈련 데이터가 필요하지 않음.
- 구체적인 내용은 부록 §A에 있음.

- 모델 학습 방법:
  - ReST EM 기법을 사용하여 훈련 작업마다 15개의 자기 수정을 샘플링함.
  - 각 자기 수정을 개별적으로 적용하여 15개의 업데이트된 모델을 생성함.
  - 각 모델을 대응하는 테스트 예제에 평가함.
  - 테스트 입력에 대해 올바른 출력을 생성하는 적응된 모델만 강화학습으로 보강함.

- 평가 방법:
  - 평가 작업마다 5개의 자기 수정을 생성하여 각각 독립적으로 적용함.
  - 올바른 출력을 내는 자기 수정의 비율을 성공률(success rate)로 보고함.
  - 이는 학습된 자기 수정 생성 정책의 품질을 반영함.

- 비교 대상 베이스라인:
  1. ICL (In-Context Learning): 
     - Llama-3.2-1B-Instruct에 Akyürek et al. [33]의 프로토콜대로 주어진 소량 예시를 프롬프트로 주고 테스트 입력에 바로 질의함.
  2. TTT + Self-Edit (prior RL 없이): 
     - Llama-3.2-1B-Instruct가 소량 예시와 합성 증강 데이터를 사용하여 테스트 시간 훈련을 수행하나, 어떤 증강이나 구성으로 학습할지 RL 없이 무차별 적용함.
  3. Oracle TTT:
     - Akyürek et al. [33]가 제시한 최적의 인간 설계 TTT 구성을 사용하여 테스트 시간 훈련을 수행함.
     - 최상한 성능을 제공함.

- 결과 (성공률, %):
  
  | 방법                    | 성공률(%) |
  |-------------------------|-----------|
  | ICL                     | 0         |
  | TTT + Self-Edit (prior RL 없이) | 20        |
  | SEAL (본 연구 방법)      | 72.5      |
  | Oracle TTT              | 100       |

- 주요 결과:
  - SEAL이 기존 베이스라인 대비 적응 성공률을 크게 향상시킴(72.5% 대 20% 및 0%).
  - 다만, Oracle TTT보다 성능이 낮아 추가 개선 여지가 있음.

---

# 4.2 Knowledge Incorporation

- Qwen2.5-7B 모델로 SQuAD 데이터셋 [13]의 새로운 사실 정보 통합 실험 수행  
  - SQuAD 데이터가 기본 모델이 문맥 내에서 “완전히 이해” 가능하지만, 문맥 없이 질문에 신뢰성 있게 답변하기 어려움  
  - 문맥 50개 배치로 2회 ReSTEM 반복 수행 (상세는 §B 참고)

- 비교 대상 방법:  
  1. **Base Model**: 사전학습된 모델을 문맥 없이 QA 태스크에 바로 적용  
  2. **Train on Passage Only**: 표준 언어모델 손실로 문맥에 대해 직접 파인튜닝 (합성 데이터 없이)  
  3. **Train on Passage + Synthetic Data**: 문맥과 모델이 생성한 합성 함의를 함께 학습 (SEAL과 같으나 RL 훈련 전)  
  4. **Train on Passage + GPT-4.1 Synthetic Data**: GPT-4.1 API로 생성한 합성 함의를 포함해 학습

- 주요 결과 (Table 2 요약):
  - **싱글 패시지(n=1)**
    - Base model: 32.7% 정확도
    - Train on Passage Only: 33.5% (거의 개선 없음 → 원시 데이터만으로는 부족)
    - Train on Passage + Synthetic (Qwen 자체 생성): 39.7% (6.2%p 향상)
    - Train on Passage + GPT-4.1 Synthetic: 46.3% (12.8%p 향상)
    - SEAL (RL로 합성 데이터 개선 후): 47.0% (GPT-4.1 합성 데이터보다 우수)
  
  - **계속된 사전학습 (Continued Pretraining, n=200)**
    - Base model: 32.7%
    - Train on Passage Only: 32.2% (향상 없음)
    - Train on Passage + Synthetic: 41.0%
    - Train on Passage + GPT-4.1 Synthetic: 39.4%
    - SEAL: 43.8% (가장 우수, 하지만 싱글 패시지보다 성능 약간 낮음, 이는 그래디언트 간섭 때문으로 추정)

- RL 반복에 따른 성능 변화 (Figure 4):  
  - 각 RL 반복은 50개 문맥 미니배치, 5개 self-edit 샘플링 포함  
  - 2번의 ReSTEM 반복 후 SEAL이 GPT-4.1 합성 데이터 성능 추월  
  - 추가 반복은 미미한 향상만 보여, 정책이 빠르게 수렴하여 핵심 사실만 추출하는 편집 스타일 학습함  
  - (Qualitative 예시는 Figure 5 참고)

- 결론:  
  - SEAL의 편집 정책은 단일 생성 기반 RL 설정을 넘어 여러 문맥 상황에 일반화됨  
  - RL로 생성된 자기 편집(self-edit)은 모델에 더 상세하고 학습하기 쉬운 사실을 제공하여 지식 통합 성능 향상에 기여함

- 수식 표현:  
  - 정확도 비교에서, 예를 들어 SEAL과 베이스 모델 차이는  
    $$  
    \Delta \text{Accuracy} = \text{Accuracy}_{SEAL} - \text{Accuracy}_{Base} = 47.0\% - 32.7\% = 14.3\%
    $$  

- 표 요약 (Table 2):

| 방법                          | Single Passage (n=1) | Continued Pretraining (n=200) |
|-----------------------------|---------------------|-------------------------------|
| Base model                  | 32.7%               | 32.7%                         |
| Train on Passage            | 33.5%               | 32.2%                         |
| Train on Passage + Synthetic| 39.7%               | 41.0%                         |
| Train on Passage + GPT-4.1 Synthetic| 46.3%       | 39.4%                         |
| SEAL                        | **47.0%**           | **43.8%**                     |

---

# Limitations

- **파국적 망각(Catastrophic Forgetting)**  
  - SEAL의 주요 동기 중 하나는 모델이 시간이 지나면서 새로운 정보를 계속해서 학습할 수 있도록 하는 것임.  
  - 순차적인 자기 편집 시퀀스를 통해 이전 지식을 보존하면서 새로운 정보에 적응할 수 있는지 확인하는 것이 핵심 도전 과제.  
  - 현재 훈련 방식은 보존(retention)을 명시적으로 최적화하지 않음에도 불구하고, 파국적 망각 현상에 어느 정도 내성을 보임.  
  - Figure 6에 따르면, 편집 횟수가 증가할수록 이전 작업에 대한 성능이 점차 악화됨을 확인할 수 있음.  
  - 완전한 붕괴는 아니며, 여러 차례 업데이트가 가능하다는 점에서 개선 가능성 존재.  
  - 향후 연구에서는 다음 방법들로 이 문제를 해결할 수 있음:  
    - 보상 셰이핑(reward shaping)을 통해 이전 작업 성능 저하에 페널티 부여  
    - null-space constrained edits나 representational superposition과 같은 지속 학습 전략 통합

- **계산 비용(Computational Overhead)**  
  - TTT 보상 루프는 인간 선호 기반 보상 신호나 단순 패턴 매칭 방법보다 훨씬 계산 집약적임.  
  - 보상 계산을 위해 전체 모델에 대해 미세조정 및 평가 과정이 필요하며, 각 자기 편집 평가에 약 30~45초 소요됨.  
  - 이로 인해 상당한 계산 오버헤드가 발생함 (§B.5 참고).

- **문맥 의존적 평가(Context-dependent Evaluation)**  
  - 현재 방식은 모든 문맥에 명시적 하위 작업이 연결되어 있어 보상 계산이 단순해짐.  
  - 이로 인해 레이블이 없는 코퍼스에 대해 SEAL의 강화 학습 적용이 제한적임.  
  - 해결책으로 모델이 자기 편집뿐 아니라 평가용 질문(예: QA 항목이나 합성 테스트 케이스)을 생성하도록 하는 방법을 제안함.  
  - 이러한 모델 작성 쿼리는 강화 학습에 필요한 즉각적인 감독 신호를 제공하여 외부 질문-답변 세트가 없는 일반적인 훈련 도메인에 적용 범위를 넓힐 수 있음.

---

# 6 Discussion and Conclusion

- Villalobos et al. [75]에 따르면, 2028년까지 최첨단 LLM은 공개된 모든 인간 생성 텍스트를 학습할 전망임.  
- 이에 따라 “데이터 벽(data wall)” 문제로 인해 합성 데이터 증강(synthetic data augmentation)의 도입이 필수적일 것이라 주장함.  
- 웹 규모 코퍼스가 소진되면, 향후 진보는 모델이 자체적으로 높은 가치의 학습 신호(training signal)를 생성하는 능력에 달려있음.  
- 자연스러운 다음 단계는 SEAL 합성 데이터 생성기 모델을 메타 학습하여 새로운 사전학습 코퍼스를 만들어내는 것임.  
  - 이를 통해 미래의 모델들은 추가 인간 텍스트에 의존하지 않고도 스케일을 확장하고 데이터 효율성을 높일 수 있음.  
- 미래에는 LLM이 학술 논문과 같은 새로운 데이터를 흡수하여, 기존 지식과 문맥 내 추론을 바탕으로 대량의 설명과 함의를 스스로 생성할 수 있을 것으로 예상됨.  
- 이런 자기 표현(self-expression)과 자기 정제(self-refinement)의 반복 루프는 외부 감독 없이도 희귀하거나 저대표 주제에 대해 성능을 향상시킬 수 있음.  
- 현재의 추론 모델들은 종종 RL로 사유 과정(chain-of-thought, CoT) 추적을 생성하도록 학습되는데, SEAL은 보완적 메커니즘을 제공하여 모델이 스스로 언제 어떻게 가중치를 업데이트할지 학습할 수 있게 함.  
- 두 접근법의 시너지는 다음과 같음:  
  - 추론 중간에 중간 가중치 업데이트를 수행하여 현재 추론 경로를 안내하거나,  
  - 추론 완료 후 핵심 통찰을 매개변수에 증류하여 내부화된 학습을 강화함.  
- 이 지속적 정제 루프는 장기 상호작용에 따라 동적으로 목표에 적응하는 에이전트 시스템 구축에 유망함.  
  - 에이전트는 행동하며 점진적으로 지식을 획득하고 유지해야 함.  
  - 제안한 방식은 상호작용 후 자기 편집(self-edit) 생성과 가중치 업데이트를 통한 구조화된 자기 수정(self-modification)을 가능하게 하여,  
  - 경험 기반 행동 정렬과 반복 감독에 대한 의존성 감소를 지원함.  
- SEAL은 사전학습 후에도 대형 언어 모델이 정체되지 않고, 자체 합성 자기 편집 데이터를 생성하고 경량 가중치 업데이트로 적용하여 자율적으로 새로운 지식을 흡수하고 새로운 작업에 적응할 수 있음을 보여줌.  
- 향후에는 SEAL 프레임워크를 사전학습, 지속 학습(continual learning), 에이전트 모델에 확장하여, 데이터 제약 환경에서 언어 모델이 자기 학습(self-learn)하며 확장할 수 있게 하는 것을 목표로 함.