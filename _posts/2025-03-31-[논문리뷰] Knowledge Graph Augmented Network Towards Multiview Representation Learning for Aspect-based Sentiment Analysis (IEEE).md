---
title: "[논문리뷰] Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis (IEEE)"
date: 2025-03-31 00:00:00 +0900
categories:
  - Paper Review
tags:
  - IEEE
  - ABSA
---

이 논문은 지식 그래프를 활용한 네트워크(KGAN)를 제안하여 세 가지 관점(맥락, 구문 및 지식)에서 감정 특성을 학습하고, 이를 통해 세분화된 감정 분석 문제인 Aspect-Based Sentiment Analysis(ABSA)의 성능을 향상시키는 방법을 다룹니다. KGAN은 다양한 정보의 상호 보완성을 활용하여 높은 성능을 기록하며, 여러 벤치마크에서 최첨단 결과를 얻었습니다.

---

# 1 Introduction

<img width="458" alt="image" src="https://github.com/user-attachments/assets/8d343598-03c1-4542-9906-71d413444b3f" />

- DNN 기반 ABSA 모델은 훈련 데이터에서 학습한 표현을 활용하여 핸드크래프트 특징 기반 모델보다 뛰어난 성능을 보임.
- 모델을 두 가지 범주로 나눌 수 있음:
  - **맥락 기반 방법**
    - CNN 및 LSTM 사용하여 맥락에서 개별적인 특징 표현 추출.
    - 예를 들어, TD-LSTM은 순차적 패턴 학습 능력 덕분에 특정 주제 정보를 포착.
    - Wang et al.는 주제와 의견 단어 간의 잠재적 상관관계를 탐구하기 위해 TD-LSTM 개선.
    - Ma et al.는 두 개의 LSTM을 사용하여 맥락과 주제 용어를 인코딩하고, 상호주의적 주의 메커니즘을 통해 더 관련성 높은 정보 추출.
  - **구조 기반 방법**
    - 복잡성과 비효율성을 해결하기 위해 많은 연구가 CNN을 사용하여 구성 구조와 n-그램 특징 포착.
    - Xue와 Li는 게이트를 사용한 합성곱 네트워크를 제안하여 컨텍스트 특징 추출.
    - Huang와 Carley는 CNN에 주제 정보를 통합하는 두 개의 신경 단위를 도입.
    - Li et al.은 시퀀스 정보 손실 문제를 해결하기 위해 주제 특화 변환 성분을 제안.
- 구문 기반 모델에서는 문장의 구문 구조를 활용하여 주제와 관련된 의견 단어 간의 관계를 효과적으로 수립하는 데 초점을 맞춤.
  - Zhang et al.은 의존 트리를 사용하여 문장을 나타내고, GCN을 통해 구문 정보를 모델링.
  - Wang et al.은 새로운 주제 지향 의존 트리 구조를 소개하고 관계형 그래프 주의 네트워크를 사용하여 트리 구조 인코딩.
  - Pang et al.은 명확한 구조가 없는 문장에서 구문 및 의미 정보를 최적으로 융합하기 위해 다중 채널 GCN을 도입.
- Li et al.은 구문 구조와 의미적 상관관계의 상호 보완성을 활용하기 위해 SemGCN과 SynGCN 모듈을 사용하는 이중 GCN 모델 제안.

---

# 2 Related Works

- 언어 지식과 상식의 통합이 자연어 이해에 유익함을 보여주며, 이는 여러 분야에서 활발하게 연구되고 있음.
- ERNIE 모델: 대규모 말뭉치와 지식 그래프를 활용하여 지식 기반의 사전 훈련 언어 모델을 구축. 다양한 지식 기반 하위 작업에서 우수한 성능을 달성.
- ABSA(Aspect-Based Sentiment Analysis) 작업에서 지식 탐색 부족 문제 존재:
  - 기존 데이터셋에 지식이 명시적으로 표현되지 않음.
  - 최근 연구들은 외부 지식을 통합하여 이 문제를 해결하려고 시도함.
  
- Wu et al.의 연구: 감정 및 구조 지식을 통합한 통일 모델 제안.
- Zhou et al.: 구문 정보와 외부 상식 지식을 공동 인코딩.
- Xing et al.: 감정 도메인 지식으로 향상된 BERT 모델 제안.
- 본 연구: ABSA를 위한 외부 지식을 제공하기 위해 지식 그래프 도입.
- 기존의 AR-BERT 모델과 유사하나, 더 간단하고 효율적인 지식 그래프 모델링 전략을 사용함.
- 외부 지식을 맥락 및 구문 정보와 종합적으로 결합하여 풍부한 특성 표현을 얻고 감정 분석 성능을 향상시킴.

---

# 2.1 Aspect-based Sentiment Analysis

- **정의**: 
  - ABSA(Aspect-based Sentiment Analysis)의 목표는 문장 S와 특정 측면 T에 대한 감정 극성을 예측하는 것.
  - 이때 S는 m개의 단어로 구성된 문장이며, T는 S의 subsequence n개로 이루어진 집합이다.
  - 감정 극성 y는 {0, 1, 2}로 긍정, 중립, 부정을 각각 나타낸다.

- **모델 개요(KGAN)**:
  - KGAN은 세 가지 가지(context, syntax, knowledge)로 구성되어 감정 정보의 다양한 특성을 학습한다.
  - **Contextual 및 Syntactic Branches**: 문장의 맥락적 및 구문적 특성을 추출하고, 상관성을 형성.
  - **Knowledge Branch**: 외부 지식을 통합하여 의미적 특징을 강화.
  - 지식 그래프를 분산 표현으로 변환한 후, 소프트 어텐션 메커니즘으로 특정 측면의 지식을 학습.

- **다중 뷰 표현 학습**:
  - **Context-based Representations**: 
    - 사전 훈련된 워드 임베딩을 사용하여 각 단어를 저차원 벡터 공간으로 Embedding.
    - 문장 S와 측면 T에 워드 임베딩을 생성하고, BiLSTM을 사용해 통계적 의존성을 포착.
    - 수식: $$ h^s_i = [\overrightarrow{LSTM}(x^s_i), \overleftarrow{LSTM}(x^s_i)] $$ for S와 $$ h^t_j = [\overrightarrow{LSTM}(x^t_j), \overleftarrow{LSTM}(x^t_j)] $$ for T.

  - **Syntax-based Representations**: 
    - 구문 정보를 활용하여 문장의 구문 인식을 촉진. GCN을 사용해 문장의 구문적 특성 추출.
    - adjacency matrix A를 기반으로 GCN을 통해 구문 정보를 인코딩.
    - 수식: $$ H^{(l+1)}_s = \mathrm{ReLU}(A H^{(l)}_s W^{(l)}(D + 1) + B^{(l)}) $$.

  - **Knowledge-based Representations**: 
    - WordNet 등을 사용하여 external knowledge로 의미적 특징을 강화.
    - 지식 그래프의 엔티티를 “entity-relation-entity” 삼중 구조로 모델링하고, KGE를 통해 지식 임베딩 학습.
    - 얻은 지식을 S와 T를 표현하는 데 결합하고, 소프트 어텐션 메커니즘을 활용하여 측면 특정 지식 표현 Rk를 캡처.

- **Hierarchical Fusion Module**: 
  - 서로 다른 뷰에서 얻은 표현들을 효과적으로 융합하여 성능을 향상.
  - 지역적 fusion과 전역적 fusion을 사용하여 각 표현의 보완성을 극대화. 

이러한 반복 과정을 통해 KGAN은 다중 관점에서 감정 정보를 포착하여 더 정교한 감정 분석을 수행할 수 있다.

---

# 2.2 Incorporating External Knowledge

- 외부 지식 통합의 중요성:
  - 모델의 성능 향상에 기여
  - 특정 도메인 지식 활용 가능

- 외부 지식의 형태:
  - 데이터베이스: 구조화된 정보 
  - 문서: 비구조화된 정보 

- 통합 방법:
  - 지식 그래프 활용
  - 특정 쿼리에 대한 외부 정보 검색
  - 데이터 증강 과정에 외부 데이터 포함

- 도전 과제:
  - 외부 지식을 어떻게 효과적으로 연관할 것인가
  - 새로운 지식의 업데이트 및 관리 문제

- 수식 예시:
  - 외부 정보가 모델에 미치는 영향은 $$ E(X) = \mathbb{E}[Y \mid X] $$로 표현 가능
  - 지식 확대를 위한 목표는 $$ G(\mathcal{K}) = \sum_{i=1}^{n} P(y_i\vert K) $$으로 나타낼 수 있음, 여기서 $K$는 지식 집합을 의미.

---

# 3 Knowledge Graph Augmented Network

- 최근 연구 [38], [39]는 상황 인식 표현이 언어 이해 능력을 향상시킬 수 있음을 보여줌.
- 주어진 문장-측면 쌍 {S, T}에 대해:
  - 각 단어를 표현하기 위해 사전 훈련된 단어 임베딩 모델 사용.
  - 각 단어 $w_i$를 임베딩 행렬 $E \in \mathbb{R}^{\vert V\vert  \times d_w}$로 저차원 벡터 공간에 임베드.
    - 여기서 $\vert V\vert $는 어휘의 크기, $d_w$는 단어 임베딩의 차원.
  - 임베딩 행렬은 일반적으로 GloVe와 같은 사전 훈련된 모델의 임베딩으로 초기화됨.

- BERT [41] 및 RoBERTa [42]와 같은 대규모 사전 훈련 언어 모델의 성공에 영감을 받아, 
  - 이 모델들을 임베딩 추출기로 사용 가능.
  - S와 T는 BERT/RoBERTa에 입력되어, 최종적으로 매핑된 출력 임베딩인 토큰 임베딩을 얻음.

- 문맥 단어와 측면 간의 상대적 위치 정보의 이점을 고려하여, 
  - 단어 임베딩에 상대 위치 특성을 인코딩.
  - 결과적으로 문장 S와 측면 T는 대응하는 단어 임베딩 $X_s = \{x_s^1, x_s^2, ..., x_s^m\}$ 및 $X_t = \{x_t^1, x_t^2, ..., x_t^n\}$으로 변환됨. 

- 단어 임베딩을 기반으로 두 개의 양방향 LSTM(BiLSTM)을 사용하여 문장과 측면의 통계적 의존성을 캡처:
  - LSTM의 전방 연산을 $−−−−→LSTM$으로, 후방 연산을 $←−−−−LSTM$으로 명시.
  - 숨겨진 상태 벡터는 다음과 같이 계산됨:
    - $h_s^i = [−−−−→LSTM(x_s^i), ←−−−−LSTM(x_s^i)], i \in \{1, m\}$
    - $h_t^j = [−−−−→LSTM(x_t^j), ←−−−−LSTM(x_t^j)], j \in \{1, n\}$

- 결과적으로, BiLSTM의 숨겨진 출력을 통해 문장 Hs를 얻음:
  - $H_s = \{h_s^1, h_s^2, ..., h_s^m\}$ (문맥 정보 보존).
- 두 개의 주의(attention) 메커니즘을 도입하여 측면 특화된 문맥 특성 캡처:
  - 자기 주의 메커니즘을 사용하여 문맥의 장거리 의존성을 학습.
  - 다른 소프트 주의 메커니즘을 통해 각 단어 S가 T에 대해 가중치를 할당하여,
    - 측면 특화된 문맥 표현, 즉 $R_c$를 얻음.

---

# 3.1 Problem Formulation

- 주제는 구문 정보를 활용하여 모델이 구문 인식 표현인 $R_s$를 학습하도록 유도함
  - 구문 인식 표현은 여러 NLP 작업에 유용함 (예: 기계 번역)
  
- 기본적으로 동일한 사전 훈련된 단어 임베딩 모델과 BiLSTM을 사용하여 숨은 상태 벡터 $H_s$를 얻음
  - 단어 임베딩과 BiLSTM의 매개변수를 공유하여 계산량을 줄이고 모델 크기를 경량화함
  - 즉, $H_s = H_c^s$ 를 사용함

- 두 층의 GCN 모듈을 사용하여 문장의 구문 특징을 추출함
  - 구문 종속 트리를 생성하고 인접 행렬 $A$를 얻음
  - 각 단어가 자식 노드와 자신에 인접하며, 인접 노드 값은 1로 설정됨
  
- GCN은 구문 정보를 $G$에서 $H_s$로 인코딩하는 데 사용됨
  - 두 층 GCN이 하나의 층 GCN보다 성능이 좋음
  - 더 많은 층은 성능을 개선하지 않음

- 수식:
  $$ H^{(l+1)}_s = \text{ReLU}(A H^{(l)}_s W^{(l)} (D + 1) + B^{(l)}), \ l \in \{0, 1\} $$  
  - 여기서 $A$는 인접 행렬, $D$는 $A$의 차수 행렬, $W^{(l)}$와 $B^{(l)}$는 (l+1)-번째 GCN 층의 가중치 및 바이어스 행렬임
  - $H^{(0)}_s$는 초기 숨은 상태 벡터이고, $H^{(2)}_s$는 GCN의 최종 출력임

- 그래프 기반 주의 모듈을 통해 특정 측면의 $R_s$를 학습함
  - 주의 모듈은 $H^{(2)}_s$에서 측면 마스킹을 수행하여 비측면 단어를 0으로 마스킹함
  - 점곱 주의 메커니즘을 사용하여 관련 측면 특성이 초기 $H_s$에서 정제된 측면 특성 $H^{(2)}_s$로 이동하게 함

---

# 3.2 Overview of the KGAN Model

<img width="929" alt="image" src="https://github.com/user-attachments/assets/9d4c294d-f926-4454-873d-ce7695bdc8e1" />

- **외부 지식 통합**: 
  - WordNet 2의 지식 그래프를 외부 지식 기반으로 사용
  - 166,000개 이상의 단어 형태 및 의미 쌍 포함
  - 동의어 집합을 사용하여 개념 표현

- **개념 학습**:
  - 인간 언어 습득처럼, 기본 개념에서 시작하여 점차 추상적인 개념으로 발전
  - 희귀한 어려운 단어는 관련된 일반 단어를 통해 이해 가능

- **WordNet 활용**:
  - 예: "Tinca"는 "어류 속"의 하위 개념으로, "어류" 또는 "식품"과 관련 지어 이해 용이
  - Zhou et al. [17]과는 달리 지식 그래프 데이터 처리에 간단하고 효율적인 전략 도입

- **지식 그래프 임베딩**: 
  - 지식 그래프 내의 의미적 관계를 분산 표현으로 모델링
  - “entity-relation-entity” 형태의 그래프 데이터로부터 개체 임베딩 훈련
  - 오픈 KGE 툴킷 OpenKE 사용

- **임베딩 초기화**:
  - 훈련된 지식 임베딩으로 새로운 임베딩 행렬 초기화
  - S와 T 단어를 지식 임베딩 행렬로 표현
  - 매핑된 지식 임베딩은 숨겨진 상태 벡터 Hs와 결합

- **소프트 어텐션 메커니즘**:
  - S와 T의 각 단어의 의미 관련성 계산하여 중요한 의미적 특성 캡처
  - 이러한 표현은 특정 지식 표현 Rk로 나타냄

- **사례 설명**:
  - 예: 문장 “Try the local food, especially the decent Tinca.”와 "food"라는 측면 단어
  - “Tinca”는 “food”의 하위 개념이며, 서로 인접하여 KGAN이 관련성 쉽게 캡처 가능

- **브랜치 훈련**:
  - 입력 문장-측면 쌍 {S, T}에 대해, 문맥 및 구문 브랜치가 같은 임베딩 행렬 사용
  - 지식 브랜치는 다른 지식 임베딩 행렬 사용
  - 세 가지 브랜치의 병렬 처리를 통해 KGAN은 다양한 관점에서 측면 특정 정보를 캡처 가능

---

# 3.3 Multi-View Representation Learning

- 다양한 관점에서 얻은 표현 {Rc, Rs, Rk}는 상호 보완성이 부족하여 단순한 융합으로는 효과를 극대화하기 어려움.
  
- 이를 해결하기 위해 **계층적 융합 모듈**을 채택하여 로컬에서 글로벌로 점진적으로 융합하여 성능 향상.
  
- **로컬 융합 절차**:
  - 세 개의 특징 표현 중 두 개를 이어붙여서 조합: 
    - $$ [ Rc; Rs]$$, $$ [ Rc; Rk]$$, $$ [ Rs; Rk]$$
  - 융합된 표현을 각각 별도의 완전 연결층에 통과시켜 예측된 감정 특징 $Rcs$, $Rck$, $Rsk$를 얻음.
  - 완전 연결층의 매개변수는 공유하지 않음.

- **글로벌 융합**:
  - 얻은 감정 특징을 열 방향으로 조합: $$ [ Rcs, Rck, Rsk ]^T $$
  - 3x3 컨볼루션 층에 입력하여 특징을 선택적으로 통합.

- 이러한 **로컬 및 글로벌 융합 절차**를 통해 각 특징 표현이 단계적으로 서로에게 이익을 줌.

- 외부 지식은 문맥 및 구문 정보와 더 나은 통합되어 더욱 유망한 성능 달성 가능.

- 이 과정은 이질적인 특징(텍스트와 그래프)을 융합하고, 지식 임베딩의 희소성 및 부정확성의 부정적 영향을 줄이는 데 도움을 줌.

---

# 3.3.1 Context-based Representations

- **사용된 데이터셋**:
  - Laptop14, Restaurant14, Twitter, Restaurant15, Restaurant16
  - Laptop14 및 Restaurant14: SemEval2014 ABSA 챌린지
  - Restaurant15 및 Restaurant16: SemEval2015 및 SemEval2016 챌린지
  - Twitter: 트윗 수집
  
- **데이터 정제**:
  - 충돌하는 감성 다수 제거
  - 최종 통계는 표 2에서 확인 가능

- **성능 평가 지표**:
  - 정확도(Accuracy, “Acc”) 및 매크로 F1 점수(Macro-F1, “F1”)

- **모델 및 하이퍼파라미터 설정**:
  - KGAN의 효과성을 검증하기 위해 세 가지 사전 훈련 모델 사용: 
    - GloVe 5 (300 차원)
    - BERT (12층, 768 차원)
    - RoBERTa (12층, 768 차원)
  - BERT/RoBERTa의 최대 시퀀스 길이는 512
  - 지식 임베딩은 훈련 중 고정
  - 학습률: 
    - GloVe 기반 KGAN: $1 \times 10^{-3}$
    - KGAN-BERT: $5 \times 10^{-5}$
    - KGAN-RoBERTa: $3 \times 10^{-5}$
  - 배치 크기: 대부분의 데이터셋에 대해 32, Restaurant14에 대해서는 64
  - 드롭아웃 비율: 0.5

- **비교 모델**:
  1. **컨텍스트 기반 방법**:
     - ATAE-LSTM: LSTM에서의 임베딩 및 주의 메커니즘 활용
     - RAM: 여러 주의 및 메모리 네트워크 이용
     - TNet-AS: CNN 사용으로 대상 정보 통합
     - MGAN: 거친 수준의 분류 작업을 통해 세밀한 분류 작업 향상
     - MCRF-SA: 복수의 CRF를 사용하여 의견 범위 추출
   
  2. **구문 기반 방법**:
     - ASGCN & ASCNN: 의존 구조 표현 및 GCN 사용
     - R-GAT: 의존 트리 구조 재구성
     - DGEDT: 트랜스포머 도입
     - RGAT: 관계 그래프 주의 네트워크 제안 
     
  3. **외부 지식 기반 방법**:
     - Sentic-LSTM: 상식 지식 활용
     - MTKEN: 다양한 지식 출처 통합
     - SK-GCN: 구문 및 지식 모델링
     - Sentic GCN: SenticNet의 지식 통합
     
- **KGAN의 성능**:
  - KGAN은 BERT 및 RoBERTa 기반의 강력한 방법들과 비교됨
  - Restaurant15 및 Restaurant16 데이터셋에서 RoBERTa 기반 모델 활용 빈도가 낮음

---

# 3.3.2 Syntax-based Representations

<img width="370" alt="image" src="https://github.com/user-attachments/assets/d6fcb31d-d818-4ed8-b403-73fe9cc47ad9" />

- **경쟁 모델 결과**:
  - KGAN 모델이 GloVe 설정에서 대부분 데이터셋에서 다른 최신 방법들을 초월함.
  
- **데이터셋별 성능**:
  - Laptop14 데이터셋에서 KGAN이 DualGCN보다 정확도 0.43%, macro-F1 score 0.47% 향상 (Acc: 78.91%; F1: 75.21%).
  - Restaurant14 데이터셋에서 KGAN의 성능이 상대적으로 낮지만 (Acc: 84.46%; F1: 77.47%), 최소 0.19% 정확도를 초과하여 모든 모델을 초월함.

- **다중 뷰 표현 학습의 우수성**:
  - KGAN은 외부 지식을 단일 맥락 또는 구문 정보와 결합하는 방법들보다 우수함.
  
- **맥락 기반 vs. 구문 기반 모델**:
  - Restaurant14 벤치마크에서 맥락 기반 모델의 평균 성능이 구문 기반 모델보다 저조함.
  - Restaurant14에서 다면적 인스턴스 비율(26.58%)이 Laptop14(20.05%)보다 높아 syntactic dependent trees의 비비선형 모델링 능력이 효과적으로 작용함.

- **데이터셋별 모델 비교**:
  - Twitter 데이터셋에서 CNN 기반 모델(TNet-AS)이 LSTM 기반 모델(ATAE-LSTM, RAM)을 크게 초월함. 
  - Twitter 데이터셋은 비문법적이고 노이즈가 많아 LSTM의 효과성을 저해함.

- **프리트레인 언어 모델과의 보완성**:
  - KGAN이 BERT 및 RoBERTa와 함께 사용할 때 일관된 개선을 나타냄. 
  - RoBERTa에서 KGAN은 모든 벤치마크에서 새로운 SOTA 기록을 달성. 
  - BERT에서도 KGAN은 대부분의 최신 모델을 초월함. 

이러한 결과들은 KGAN의 효과성과 우수성을 입증함.

---

# 3.3.3 Knowledge-based Representations

- **다양한 표현 조합의 효과**
  - 여러 표현 조합({Rc, Rs, Rk})을 사용한 결과를 제시.
  - 모든 표현이 KGAN에 긍정적인 영향을 미침.
  - Rk가 Rc와 Rs보다 성능이 우수함. 
  - 지식 임베딩(Rk)을 통해 비지식 표현 “[Rc, Rs]”에서 평균 +1.08% 성능 향상 확인.

- **다양한 융합 전략의 효과**
  - 제안된 계층적 융합 모듈의 효과를 검증하기 위해 다양한 정보 융합 접근법과 비교.
  - 일반적인 접근법:
    - CONCAT: 직접적인 벡터 결합
    - SUM: 각기 다른 Fully Connected Layer에서 요소-wise 합
    - ATT: 다중 시각 주의 메커니즘 사용
    - VOTING: 각 뷰에 대해 개별 분류기 훈련 후 투표
  - KGAN의 계층적 융합 모듈이 다른 접근법에 비해 명백히 우수함을 입증.

- **지식 그래프 임베딩의 다양한 접근법 효과**
  - KGE의 다양한 접근법(TransE, ComplEx, ANALOGY, DistMult) 실험.
  - TransE는 성능이 좋지 않음. 대신, 관계적 의미를 잘 캡처하는 방법들이 성능이 뛰어남.
  - 대규모 고품질 지식 그래프(Wikidata)의 사용이 KGAN 성능 향상에 기여.

- **결론**
  - KGAN은 다양한 관점에서 감정 기능을 캡처하여 외부 지식을 통합한 모델.
  - 제안된 방법은 효율성과 성능 간의 좋은 균형을 이루며, 약간의 노이즈에도 강건성을 보여줌.
  - 향후 다른 언어 이해 작업에서 KGAN의 다중 시각 표현 접근법 검증 예정.

---

# 3.4 Hierarchical Fusion Module

- 제안된 계층적 융합 모듈의 효과성을 검증하기 위해 여러 정보 융합 접근 방식과 비교:
  1. **C ONCAT**: 다중 뷰 표현을 행으로 직접 연결하고 완전 연결층을 통해 융합.
  2. **SUM**: 표현을 세 개의 개별 완전 연결층에 공급하고 원소-wise 합산을 통해 융합.
  3. **A TT**: 다중 뷰 주의 메커니즘을 사용하여 다중 뷰 표현을 융합.
     - 표현 {Rc, Rs, Rk}를 연결하여 주의 메커니즘의 키와 값으로 사용.
     - 각각 Rc, Rs, Rk를 쿼리로 사용하여 주의 점수를 계산.
     - 여러 뷰 점수를 합산하여 소프트맥스 층에 공급하고 최종 주의 점수를 계산 후 값에 곱함.
     - 융합된 표현은 MLP 층에 공급되어 감정 예측을 수행.
  4. **V OTING**: 투표 메커니즘을 통해 표현을 융합.
     - 각 뷰 표현에 대해 개별 분류기를 학습하고 소프트 투표를 통해 최종 예측 결정.
  5. **O URS**: 제안된 계층적 융합 모듈을 사용하여 표현을 융합.

- 결과:
  - 그림 6에 나타난 바와 같이, 제안된 계층적 융합 모듈은 정확도 및 매크로-F1 지표 모두에서 다른 융합 전략보다 유의미하고 일관되게 우수한 성능을 나타냄.
  - Restaurant14 데이터셋에서 매크로-F1 점수가 2.16% 향상되었고, Laptop14에서 정확도가 최소 0.63% 증가함.
  - 직접적으로 다중 뷰 표현을 융합하는 방법(C O N C A T 및 S U M)이 최적이 아님을 입증.
  - 계층적 융합 모듈은 지역-글로벌 방식으로 다중 뷰 표현을 융합하며, 이로 인해 상호 보완성을 최대한 활용함.
  - Twitter 데이터셋에서 V O T I N G보다 약간 성능이 우수함 (Acc: 78.13%; F1: 77.10%).
  - Twitter의 드문 및 문법적이지 않은 단어가 KGE의 적절한 커버를 방해해 성능 저하의 원인으로 판단됨.
  - 더 나은 KGE(예: 더 큰 지식 그래프에서 사전 학습된 경우)를 사용할 경우 성능 저하 문제는 완화될 수 있음.

---

# 4 Experiments

- 여러 간단한 지식 그래프 임베딩(KGE) 접근 방식을 사용하여 지식 그래프를 연속 임베딩으로 모델링
- 실험 목적: a) 다양한 KGE 접근 방식 및 b) 다양한 지식 그래프의 성능 분석

### a) KGE 접근 방식
- 사용한 KGE 접근 방식:
  - TransE: 번역 기반 방법
  - ComplEx: 의미적 매칭 방법
  - ANALOGY: 의미적 매칭 방법
  - DistMult: 의미적 매칭 방법
- 실험 결과 (표 4 (a)):
  - TransE는 다른 의미적 매칭 방법들보다 성능이 낮음
  - TransE는 관계 정보에만 집중하여, KGAN의 의미적 특성 강화 실패
  - RELATIONAL semantics를 포착하는 모델들이 더 나은 성능, 특히 ANALOGY (정확도: 78.91%; F1: 75.21%)에서 성과
  - DistMult의 성능 불안정: Laptop14에서 최적 성능 미달
    - 그 이유: DistMult가 노트북 도메인의 의미적 관계 모델링에 부족

### b) 지식 그래프
- 더 큰 지식 그래프: Wikidata (2천만 개 이상의 엔티티와 594개의 관계 포함)
- Wikidata 임베딩: OpenKE 툴킷으로 사전훈련된 임베딩 사용
  - 두 가지 차원(50, 100)의 TransE 기반 임베딩 제공
- 공정한 비교를 위해 WordNet 임베딩도 TransE 방법으로 훈련하고, 동일한 차원 설정
- 실험 결과 (표 4 (b)):
  - 더 큰 고품질 지식 그래프 사용 시 KGAN의 성능 개선
  - Twitter에서 정확도는 1.42%, 매크로 F1 점수는 1.92% 상승
  - Wikidata 임베딩 성능이 Laptop14 및 Restaurant14에서 Analogy 및 DistMult보다 약간 낮음
- 결론: 대규모 지식 그래프와 강력한 KGE 방법으로 훈련된 지식 임베딩이 KGAN의 성능을 더 향상시킬 수 있음.

---

# 4.1 Datasets and Experimental Settings

- 여러 평가된 데이터셋에서 사례를 선택하여 심층 사례 연구를 수행.
- 실험 세부 사항:
  - 대규모 지식 그래프에서 임베딩을 사전 훈련하는 것은 본 연구의 초점이 아니며, 시간 소모가 크므로 추가 실험을 진행하지 않음.
  
- 성과 비교에 대한 표:
  - 표 5에는 중괄호[]로 감싸인 단어가 측면 용어를 나타냄.
  - p, n, o는 각각 긍정, 부정, 중립을 의미.
  - 다섯 가지 모델의 성과 제시.
    - RAM 및 TNet-AS와 같은 맥락 기반 모델이 R-GAT와 같은 구문 기반 방법보다 성능이 낮음.
    - KGAN은 대다수의 사례에서 정확한 예측을 보여줌.
    - 복잡하고 비공식적인 문장에서 KGAN의 뛰어난 성능이 관찰됨.
    
- 특정 사례 분석:
  - 첫 번째 사례: "staff"라는 측면에 대한 두 가지 표현이 의견 단어 "bit more friendly"에 집중하여 올바른 예측 도움.
  - 두 번째 사례: "orange donut"가 지식 그래프에서 "had"와 더 가까워, 중요 단어 추출 용이.
  
- KGAN의 훈련 효율성 분석:
  - 효율성 비교 표 6에서 KGE 기반 및 GCN 기반 KGAN의 성능 비교.
  - 다양한 GCN 기반 구현(K-KGAN 및 SK-KGAN) 포함.
  - KGAN(전체)은 평균적으로 더 높은 정확도 및 F1 점수를 기록함.

---

# 4.2 Main Results and Analysis

- **KGE 기반 모델과 서브그래프 기반 모델 비교 (4.5.1)**  
  - 서브그래프 기반 방법은 특정 측면 서브그래프를 구축하기 위한 추가 과정이 필요하며, 이는 과도한 계산을 초래함.  
  - KGE(Knowledge Graph Embedding)를 이용하여 KGAN의 학습 효율성과 성능을 비교.  
  - 세 가지 구현 방법:
    - **K-KGAN**: KGAN의 지식 브랜치에서 KGE 모듈을 두 개의 GCN으로 대체.
    - **SK-KGAN**: 문법적 종속 그래프 및 측면 특정 지식 그래프를 통합하여 GCN 모듈로 공동 모델링.
    - **AR-KGAN**: GraphSAGE 알고리즘을 사용하여 서브그래프 기반의 측면 임베딩 학습.  
  - KGE 기반 KGAN이 데이터 처리 시간(“DP. (s)”)과 계산량(“GFLOPs”)이 다른 모델보다 적고 더 효율적임.  
  - KGE 및 서브그래프 기반 방법이 외부 지식을 통합하여 향상된 성능을 보임. KGE 기반 KGAN이 모든 데이터셋에서 최고의 성능 달성.

- **모델 지연 시간 및 모델 크기 (4.5.2)**  
  - 외부 지식을 도입함으로써 지연 시간과 모델 크기가 증가함.  
  - KGAN은 이전 모델(more complex than ATAE-LSTM)에 비해 성능이 획기적으로 개선되었으며, 지연 시간도 비슷하거나 더 좋음.  
  - KGAN은 매개변수 공유 메커니즘을 통해 효율성과 성능의 균형을 잘 맞추고 있음.

- **KGAN의 견고성 (4.5.3)**  
  - 외부 지식이 정규화 역할을 할 수 있는지에 대한 의문을 조사하기 위해, 노이즈 비율을 달리하여 KGAN의 학습을 평가.  
  - KGAN은 1%, 2%, 5%의 경미한 노이즈를 견딜 수 있는 반면, 20%의 노이즈가 추가되면 성능 저하.  
  - 결론적으로, KGAN의 다중 시각 지식 표현 방식은 약한 노이즈에 대해 견고하며, 정규화 효과는 없음.

- **결론**  
  - KGAN은 외부 지식을 통합하여 감정 정보를 증대시키는 새로운 네트워크 모델.  
  - 다양한 측면(문맥, 문법, 지식)에서 감정 특징을 포착하고, 다중 특징 표현을 계층적으로 융합.  
  - 실험 결과 KGAN이 성능과 효율성을 동시에 증명하며, 경미한 노이즈 공격에 대해 견고함을 보임.  
  - 향후 연구에서는 KGAN을 다른 언어 이해 과제에 적용할 계획.

---

# 4.3 Ablation Study

- 외부 지식을 도입하는 것은 지연 시간과 모델 크기를 증가시킴.
- KGAN 모델과 기존 모델 간의 효율성과 성능 간의 균형을 조사.
- 실험은 Nvidia GTX-1660 SUPER에서 수행됨.
- KGAN은 ATAE-LSTM보다 복잡하지만 평균 +10.87%의 매크로 F1 점수를 보임.
- R-GAT 및 DM-GCN와 비교하여 KGAN은 더 나은 성능을 유지하며 유사한 지연 시간을 가짐.
  - 이는 복잡한 모듈을 사용한 다른 모델들과 달리 KGAN이 원래 BiLSTM을 특징 추출기로 사용하고 매개변수를 세 개의 가지에 공유하여 모델 파라미터를 대폭 줄였기 때문.
- **결론**: KGAN은 매개변수 공유 메커니즘 덕분에 효율성과 성능 간의 좋은 트레이드오프를 확립함.

- KGAN의 견고성 실험:
  - 외부 지식이 정규화 역할을 하는지 의문을 제기하는 연구자들을 위해 노이즈가 섞인 지식 임베딩을 도입하여 실험 진행.
  - 약간의 노이즈(1%, 2%, 5%)를 주어도 성능 유지 가능.
  - 그러나 20%의 높은 노이즈 비율에서는 성능 저하 발생, 노이즈가 정규화 역할을 하지 않음을 나타냄.
- **결론**: 
  1. 다중 뷰 지식 표현 접근 방식은 약간의 노이즈에 견고함.
  2. KGAN은 외부 지식의 이점이 노이즈의 이점보다 크다.
