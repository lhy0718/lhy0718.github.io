---
title: "[논문리뷰] TopicGPT- A Prompt-based Topic Modeling Framework (NAACL 2024)"
date: 2025-03-07 08:00:00 +0900
categories:
  - Paper Review
tags:
  - NLP
  - NAACL 2024
  - Topic Modeling
  - LLM
---

요약: TopicGPT는 대형 언어 모델을 활용하여 텍스트에서 잠재적 주제를 발견하고, 인간의 분류와 더 잘 일치하며 해석 가능성을 높인 주제 모델링 프레임워크이다. 이 모델은 사용자가 주제를 보다 쉽게 지정하고 수정할 수 있도록 하여, 고품질의 해석 가능한 주제를 제공한다.

---

# 1 Introduction

- 주제 모델링은 방대한 텍스트 문서 집합에서 잠재적인 주제 구조를 발견하는 데 사용됨.
- 기존의 주제 모델(예: LDA)은 문서를 여러 주제로 구성된 혼합물로 표현, 각 주제는 단어에 대한 분포로 나타냄.
- 주제는 가장 확률이 높은 단어로 나타내지만, 이 과정에서 불일치하거나 관련 없는 단어가 포함되어 사용자가 해석하기 어려움.
- 사용자 상호작용을 통해 주제를 안내하는 모델도 있으나, 백-오브-워드 형식으로 인해 사용성에 제한이 있음.
  
- **TopicGPT 소개**:
  - TopicGPT는 대형 언어 모델(LLM)을 활용하여 인간 중심의 주제 모델링 접근법을 제안.
  - 문서 샘플과 이전에 생성된 주제 목록을 기반으로 새로운 주제를 반복 생성.
  - 생성된 주제를 정제하여 중복 및 자주 사용되지 않는 주제를 제거.
  - 새로운 문서에 대해 생성된 주제 중 하나 이상을 할당하고, 할당 지원을 위해 인용 제공.

- **주요 장점**:
  - TopicGPT는 LDA, SeededLDA, BERTopic보다 더 높은 품질의 주제를 생성.
  - Wikipedia 기사와 의회 법안을 대상으로 한 연구에서 인간 주석과의 정합성이 높음.
  - 주제의 불일치율이 LDA보다 낮음: 예를 들어, Wikipedia에서 TopicGPT는 30.3%의 잘못 맞춤 주제를 가지고, LDA는 62.4%를 기록.
  - 다양한 프롬프트 및 데이터 설정에서도 주제 품질의 강건성 발휘.

- **해석 가능성**:
  - TopicGPT가 생성한 주제는 자연어 레이블 및 설명 포함.
  - 문서-주제의 연관성을 제공하고, 이해 가능한 증거 인용 제공.
  
- **사용자 맞춤화 가능성**:
  - 주제 모델은 특정 응용 설정에 맞춰야 함.
  - 사용자는 초기 몇 개의 예제 주제를 제공하여 생성된 주제를 안내하고 조정 가능.
  
- **오픈 소스 LLM의 한계**:
  - 많은 실험에서 GPT-4로 주제 생성, GPT-3.5-turbo로 주제 할당 실시.
  - 비싼 API 의존도를 감소시키기 위해 Mistral-7B-Instruct 모델도 실험했으나, 주제 생성에서 한계를 보임.
  
- **미래 연구 방향**:
  - 오픈 소스 LLM에서 주제 생성 개선이 중요한 연구 방향이 될 것.

---

# 2 Related Work

- **TopicGPT의 설계**: 
  - TopicGPT는 자동화된 콘텐츠 분석을 위한 모델로 설계됨.
  - 초기 레이블을 설정하기 위해 문서를 열린 코딩 방식으로 분석.
  - 초기 코드가 신중하게 검토되고 재구성되어 통합된 코딩 시스템으로 조직됨.

- **콘텐츠 분석을 위한 주제 모델링**:
  - 전통적인 주제 모델링 접근법(예: LDA)은 잠재적 주제 구조를 드러내지만 해석이 복잡함.
  - 주관적인 수작업이 신뢰성과 유효성 문제를 초래할 수 있음.
  - 더 해석 가능한 주제를 생성하기 위한 연구를 따라감.
  - 초기 주제 모델링(시드 기반) 및 가지치기된 주제 모델링에 기반.

- **LLM 기반 콘텐츠 분석**: 
  - ChatGPT와 같은 LLM이 새로운 텍스트 분석 접근법을 가능하게 함.
  - 텍스트 군집화, 추상 요약, 유도 질적 코딩 등의 작업에 LLM의 프롬프트 기법 활용.
  - 사전 훈련된 모델의 컨텍스트화된 임베딩을 활용한 주제 모델링의 연구 진행.

- **GoalEx와의 비교**: 
  - TopicGPT는 GoalEx와 유사하지만 주제 모델링에 특화됨.
  - GoalEx는 클러스터링에 집중하지만 전체 주제 집합 조직에는 미치지 못함.
  - TopicGPT는 의미론을 기반으로 주제를 정제하여 일관되고 유익한 주제 유지.
  - GoalEx는 각 문서에 개별적으로 주제를 할당하는 반면, TopicGPT는 효율적인 확장을 위해 모든 프롬프트 주제를 동시에 제공.
  - TopicGPT는 안정성과 진실 주제와의 정렬로 벤치마킹되어 콘텐츠 분석 도구로서의 유용성을 입증.

---

# 3 Methodology

<img width="724" alt="image" src="https://github.com/user-attachments/assets/b39cb206-fdbd-4638-b324-30f972d2f423" />

- TopicGPT는 두 가지 주요 단계로 구성됨:
  - 주제 생성 (§3.1)
  - 주제 할당 (§3.2)
  
- 그림 1은 프레임워크의 개요를 제공함.

---

# 3.1 Stage 1: Topic Generation

- LLM(대형 언어 모델)를 사용하여 입력 데이터셋을 기반으로 주제 집합 생성
- 생성된 주제를 정제하여 사용 빈도가 낮은 주제 제거 및 중복 병합
- 결과물은 선택적으로 TopicGPT에 다시 입력하여 세부 주제 생성 가능

## 새로운 주제 생성
- 문서 d와 예시 주제 S를 기반으로 모델이 d를 S의 기존 주제에 할당하거나 d를 더 잘 설명하는 새로운 주제 생성
- 주제는 간결한 레이블과 포괄적인 설명 쌍으로 정의
  - 예: "Trade: 자본, 상품 및 서비스의 교환"
- 초기 S는 몇 개의 인간 작성 주제(예: 2개)로 구성
- 이 주제들은 “few-shot” 형식의 주제 생성 예시로 사용
- 데이터셋의 특정 주제나 전체 가능 주제를 나타낼 필요는 없음
- 이 과정은 생성된 주제가 다른 주제들과 차별화되고 특수성을 갖도록 유도
- 전체 데이터셋이 아닌 신중하게 구성된 샘플에 대해 주제 생성 수행

## 생성된 주제 정제
- 선택적으로 최종 주제 목록의 일관성과 비중복성 보장
- Sentence-Transformer 임베딩을 사용하여 코사인 유사도가 ≥ 0.5인 주제 쌍 식별
- LLM에 5개의 주제 쌍을 요청하여 적절한 경우 근사 중복 주제 병합
- 이전 단계에서 간과된 소규모 주제를 해결하기 위해 등장 빈도가 낮은 주제 제거
  - 각 주제의 발생 빈도 추적
  - “제거” 임계값 이하로 발생하는 주제는 중요하지 않다고 간주하여 최종 목록에서 제외

---

# 3.2 Stage 2: Topic Assignment

- **목표**: 생성된 주제와 데이터셋의 문서 간의 유효하고 해석 가능한 연관성을 Establish.
- **과정**:
  - LLM에 생성된 주제 목록과 2-3개의 예시, 그리고 관심 있는 문서를 제공.
  - 모델에 주어진 문서에 하나 이상의 주제를 할당하도록 지시.
- **최종 출력**:
  - 할당된 주제 라벨, 문서와 관련된 주제 설명,
  - 이 할당을 뒷받침하는 문서의 인용구 포함.
- **인용구의 중요성**: 
  - TopicGPT의 할당의 검증 가능성을 향상.
  - 전통적인 방법(LDA)의 오랜 우려 사항을 해결.
- **샘플 주제 할당**: 
  - "Agriculture": 농업 수출 요구사항 변경 언급 (“...repeal of the agricultural export requirements...”).
- **자체 수정 단계**: 
  - 잘못된 형식이나 품질이 낮은 주제 할당을 해결하기 위한 단계 포함.
  - 파서를 구현하여 잘못된 또는 불가능한 할당(예: “None”/“Error”) 식별.
  - 오류 유형과 함께 식별된 문서를 LLM에 제공하고 유효한 주제를 재할당하도록 프롬프트.

---

# 4 Experiments

- **목표**: TopicGPT의 출력이 인간이 설정한 주제와 일치하는지 평가하고, 다양한 설정에 대한 견고성을 테스트.
- **구성 요소**:
  - 데이터세트: 실험에 사용된 데이터셋 설명.
  - 기준 방법: 비교를 위한 기존의 방법들 소개.
  - 모델 구성: 실험에 사용된 모델의 설정.
  - 평가 메트릭: 성능 평가를 위한 지표 설명.

---

# 4.1 Datasets

- 평가를 위해 두 가지 영어 데이터셋 사용:
  - **Wiki**:
    - 작성자: Merity et al., 2018
    - Wikipedia 기사: 14,290개
    - 기준: 핵심 편집 기준 충족
    - 라벨: 
      - 15개 고수준 라벨
      - 45개 중수준 라벨
      - 279개 저수준 라벨

  - **Bills**:
    - 작성자: Adler와 Wilkerson, 2018 (Hoyle et al. 2022에 의해 수집됨)
    - 데이터: 미국 110-114회 국회의회에서의 32,661개 법안 요약
    - 라벨:
      - 21개 고수준 라벨
      - 114개 저수준 라벨

---

# 4.2 Baselines

- 세 가지 인기 있는 주제 모델을 고려: LDA, BERTopic, SeededLDA
- 공정한 비교를 위해 TopicGPT가 생성한 주제 수(k)와 동일하게 설정
- 데이터 전처리 파이프라인:
  - LDA, SeededLDA: Hoyle et al. (2021)의 전처리 사용
  - BERTopic: 전처리 없음
- LDA:
  - MALLET 구현 사용 (McCallum, 2002)
  - Gibbs 샘플링 적용 (Griffiths, 2002)
  - 인간 코드와 강한 정렬 및 안정성 보장 (Srivastava and Sutton, 2017; Hoyle et al., 2021; Hoyle et al., 2022)
  - 설정: |V| = 15,000, α = 1.0, β = 0.1
  - 2,000회 반복 실행, 매 10회 간격으로 최적화
- BERTopic:
  - 인기 있는 신경망 주제 모델
  - 문서의 Sentence-Transformer 임베딩을 클러스터링하여 주제 도출 (Grootendorst, 2022)
  - 모든 기본 하이퍼파라미터 유지
- SeededLDA:
  - 사용자 관심 주제를 유도하기 위해 시드 주제 포함 (Jagarlamudi et al., 2012)
  - 모든 기본 하이퍼파라미터 유지

---

# 4.3 Sampling documents for TopicGPT

- TopicGPT의 주제 생성 단계에서 사용되는 문서 수는 중요한 파라미터임.
- 시간과 비용(폐쇄형 LLM API 사용 시)이 충분하다면 전체 훈련 코퍼스를 사용할 수 있지만, 실제로 비현실적이며 비용이 높음.
- 따라서 문서 하위 집합을 무작위로 샘플링하여 사용함.
  - Bills에서 1,000개 문서
  - Wiki에서 1,100개 문서
- 문서 샘플링 수 추천:
  - 예산에 맞는 샘플 크기 선택
  - 또는 점진적으로 주제 생성을 수행하고 새로운 주제가 생성되지 않을 때 중지 (예: 200문서 기준)
- 주제 범위를 평가하기 위해, 이 임계치에 도달한 후 생성된 주제를 검토하고 정제 과정에서 제거된 주제가 있는지 확인함.
- 두 데이터 세트 모두에서 정제 후 남아 있는 새로운 주제 수가 임계치 후에 안정화됨 (약 600문서에서 정체).

---

# 4.4 TopicGPT implementation details

- 기본 설정:
  - OpenAI의 GPT-4를 사용하여 주제를 생성.
  - GPT-3.5-turbo를 사용하여 문서에 주제를 할당.

- 결정적 출력 장려:
  - max_tokens를 300으로 설정.
  - temperature와 top_p를 0으로 설정.

- 문서 처리:
  - 긴 문서는 LLM의 컨텍스트 창 크기에 맞추기 위해 잘라냄.
  - Bills와 Wiki는 문서와 레이블 간의 일대일 맵핑을 가짐.
  - 할당 프로프트를 수정하여 문서당 하나의 주제만 할당.

- 제거 빈도 설정:
  - Bills를 위해 10, Wiki를 위해 5로 설정.

- 자기 수정 기능:
  - 재시도 한도를 10으로 설정.
  - 이 한도 내에서 모든 주제 환각과 형식 문제 해결.

- 평가 용도:
  - Wiki에서 8,024문서, Bills에서 15,242문서를 샘플링하여 주제 생성 샘플에 포함되지 않음.

---

# 4.5 Evaluation Setup

- TopicGPT의 유용성을 평가하기 위해 자동화된 콘텐츠 분석 도구로서의 기능을 검토함.
- 평가 방식: 
  - 생성된 주제의 정렬(topic alignment) 및 안정성(stability)을 평가
  - Hoyle et al. (2022)의 방법을 따름.

---

# 4.5.1 Topical alignment

- TopicGPT는 LDA와 같은 확률 모델이 아니므로, 예측된 주제 할당과 실제 주제 레이블 간의 정렬을 평가하여 기준선과 비교함.
- 문서마다 가장 가능성이 높은 주제에 할당.
- 지상 진실 클래스와 예측된 할당 클러스터 간의 정렬을 세 가지 외부 클러스터링 메트릭스를 사용하여 평가.

## 주요 메트릭

- **Purity (순도)**:
  - 순도와 역순도의 조화 평균을 사용하여 각 지상 진실 카테고리와 가장 높은 정밀도와 재현율을 가진 클러스터를 일치시킴.
  - 순도 점수는 무작위 할당에 대해 0에 가깝고 강한 일관성이 있는 할당에 대해 1에 가까움.

- **Adjusted Rand Index (조정 랜드 지수)**:
  - 두 클러스터 집합 간의 쌍wise 일치를 측정.
  - ARI는 기회를 보정하므로 무작위 할당에 대해 0에 가깝고 일관된 할당에 대해 1에 가까운 점수를 제공함.

- **Normalized Mutual Information (정규화된 상호 정보)**:
  - 상호 정보(MI)는 두 클러스터 집합 간의 공유 정보를 측정.
  - NMI는 MI를 0과 1 사이로 정규화하여 클러스터 수의 변화에 덜 민감하게 만듦.

## 메트릭 비교

- P1, ARI, NMI는 세트 매칭, 쌍 계산, 정보 변화에 대한 상호 보완적인 관점을 제공.
- P1은 클러스터의 순도에 중점을 두며, 지상 진실 레이블의 분포에는 큰 영향이 없음.
- ARI는 기회를 보정하지만, 클래스 불균형을 고려하지 않아 예측된 클러스터 수에 더 민감함.

---

# 4.5.2 Stability

- TopicGPT의 안정성을 평가하기 위해 프롬프트와 생성 샘플의 변동성에 대한 강인성을 측정함.
- **도메인 외 프롬프트**: 
  - 사용자들은 프롬프트의 예제 주제와 몇 가지 샘플을 변경하여 데이터 세트에 맞게 조정 가능.
  - Wiki 프롬프트를 Bills 데이터셋에 적용하여, 한 데이터셋에 맞게 작성된 프롬프트가 다른 데이터셋에서도 작동하는지 탐색.
  
- **추가 예제 주제**: 
  - Bills에서 TopicGPT의 성능에 대한 추가 예제 주제의 영향을 평가.
  - 원래 주제 생성 프롬프트에 두 개의 예제 주제가 있었으나, 세 개의 주제를 추가하여 확장함.
  
- **문서 샘플 셔플링**: 
  - Bills의 생성 샘플에서 문서를 무작위로 섞어 LLM이 문서를 처리하는 순서의 중요성을 이해하고자 함.
  
- **다른 샘플 사용**: 
  - TopicGPT의 데이터 변화에 대한 강인성을 평가하기 위해 다른 Bills 샘플에 TopicGPT를 적용하고 결과의 변동성을 조사.

---

# 5 Results

- **TopicGPT의 주제 정렬성과 강건성**
  - TopicGPT는 데이터와 프롬프트 변형에 대해 강한 주제 정렬성을 보이며, 인간 평가에서 제공된 주제와의 일치도가 높다.
  
- **TopicGPT의 ground truth 레이블과의 일치**
  - 실험 결과, TopicGPT는 모든 데이터셋과 설정, 메트릭에서 기본 모델들에 비해 인간 주석 레이블과 더 잘 일치함.
  - LDA는 SeededLDA에 비해 우수하나 TopicGPT의 성능에는 미치지 못함
    - 예: TopicGPT의 정제 후 조화적 순도 스코어는 Wiki에서 0.74, Bills에서 0.57로, LDA의 0.64 및 0.52, BERTopic의 0.58 및 0.39, SeededLDA의 0.62 및 0.52보다 높음.

- **다중 주제 할당 가능성 탐색**
  - TopicGPT의 출력이 ground truth와 일치하지 않을 경우, 여러 주제로 문서를 할당 가능성에 대해 살펴보았고, 업데이트된 프롬프트를 사용했을 때 3/5의 경우에 성공적으로 적절한 주제를 찾음.

- **TopicGPT의 안정성**
  - TopicGPT는 모든 실험 설정에서 안정적인 주제 할당을 나타내며, 예시 주제를 추가하더라도 성능이 저하됨.
  - LDA와 비교해 TopicGPT는 더 높은 안정성을 보였음.

- **TopicGPT 주제가 ground truth와 의미적 근접성**
  - TopicGPT 출력의 주제는 LDA와 비교할 때 의미적으로 더 높은 일치를 보여줌.
  - TopicGPT의 정제된 주제 출력은 LDA보다 훨씬 적은 수의 잘못된 주제를 포함함.

- **오픈 소스 모델 사용 탐구**
  - Mistral-7B-Instruct를 TopicGPT의 주제 할당에 사용할 수 있는 가능성을 평가했으나, 주제 생성에서는 GPT-4에 비해 복잡함으로 인해 효과적이지 않음.

- **인간 평가 과정**
  - 세 명의 주석자가 생성된 주제를 ground truth 분류와 비교.
  - TopicGPT의 주제는 LDA의 주제들보다 명확하고 더 해석하기 쉬운 것으로 판단됨. 
  - TopicGPT는 LDA에 비해 많은 주요 주제를 포함하고 있으며, 각 적용에서 저조한 비율의 오류를 기록함.

---

# 5.1 TopicGPT is strongly aligned to ground truth labels

<img width="724" alt="image" src="https://github.com/user-attachments/assets/9f26c2d4-30d1-450e-a67e-a1542576f7e5" />

- **강력한 주제 일치**: TopicGPT는 사람의 주석으로 부터 주제와의 일치도가 매우 높음.
- **기초 모델과 비교**: LDA, SeededLDA, BERTopic와 같은 세 가지 기초 모델과 비교했을 때 TopicGPT의 성능이 상회함.
  - 예를 들어, TopicGPT는 Wiki와 Bills 데이터셋에서 각각 0.74 및 0.57의 조화 순도(P1) 점수를 기록하였고, 이와 비교해 LDA는 0.64 및 0.52를 기록함.
- **불일치 분석**: TopicGPT가 지지하는 데이터와 인간 레이블 간의 불일치를 분석하였고, 여러 주제를 합리적으로 분배할 수 있음을 발견함.
  - 예제 문서에서 “Labor” 또는 “Transportation Safety”와 같이 여러 주제를 명시할 수 있음을 보여줌.
- **다중 주제 할당 추천**: TopicGPT가 각 문서에 대해 여러 주제를 할당할 수 있도록 프롬프트를 업데이트한 결과, 많은 적합한 주제를 회수할 수 있음을 확인함.
- **모델 안정성**: TopicGPT는 모든 실험 설정에서 지상 진리와의 일치성을 안정적으로 유지함.
- **수정 및 주제 생성**: TopicGPT은 문서 분석을 위한 수정 단계 전후로 주제를 생성하고, 부정확한 주제를 줄이는 결과를 보여줌.

이러한 결과들은 TopicGPT가 전통적인 주제 모델들보다 뛰어난 성능을 보인다는 점을 강조하며, 다양한 상황에서도 일관되게 높은 성능을 유지한다는 것을 증명한다.

---

# 5.2 TopicGPT is stable

<img width="724" alt="image" src="https://github.com/user-attachments/assets/78d7db56-5a85-427c-955d-43deaf9e8592" />

- **TopicGPT 성능:**
  - Bills 데이터 세트에서 TopicGPT는 모든 수정된 실험 설정에서 Ground truth와 유사한 주제 정렬을 생성.
  - 예시 주제를 추가할 경우 성능 저하 발생; 복잡한 주제로 인해 모델이 주요 주제로 압축되지 않을 수 있음.
  - 최적의 결과를 위해 2-3개의 고품질 주제를 소규모로 유지할 것을 권장.

- **다양한 설정 간 일관성:**
  - 기본 설정과 각 수정된 설정 간의 정렬 점수를 계산하여 TopicGPT의 일관성 평가.
  - LDA를 10회 실행한 평균 내부 정렬 점수와 비교한 결과, 모든 메트릭이 0.05 범위 내에서 매우 일관성 있게 나타남.
  - TopicGPT는 P1 및 ARI 측면에서 LDA보다 안정성이 높았으나 NMI에서는 유사한 성과를 보임.
  - 흥미롭게도 두 번의 동일 설정 실행에서 약간씩 다른 결과 발생, 이는 무작위성 추가 및 LLM API 비결정성 때문일 수 있음.

- **주제의 의미적 정렬:**
  - 위의 메트릭은 주제가 의미적으로 맞는지를 포착하지 못함.
  - LDA 출력과 TopicGPT의 미세 조정된 주제 생성을 질적으로 비교하였고, 각 방법이 생성한 미정렬 주제 비율을 분석함.

- **수작업 주제 일치 프로세스:**
  - 세 명의 주석자가 생성된 주제를 검토하고 각각을 Ground truth 클래스로 할당.
  - 정확한 일치가 불가능할 경우, 세 가지 미정렬 카테고리 중 하나로 레이블을 지정.
    1. 범위 초과: Ground truth에 비해 너무 좁거나 넓은 주제.
    2. 누락 주제: Ground truth에는 존재하지만 생성된 출력에는 없는 주제.
    3. 반복 주제: 다른 주제의 중복.

- **오류 분석 결과:**
  - TopicGPT는 LDA에 비해 미정렬 주제가 훨씬 적음; 정제 후 LDA는 62.4%의 미정렬 주제를 보인 반면 TopicGPT는 각각 38.7% 및 30.3%로 감소.
  - TopicGPT의 출력물이 LDA 출력물보다 작업하기 훨씬 용이하다고 주석자들이 언급. 
  - 정제 과정은 범위 초과 및 반복 주제를 줄이는 데 기여함.
  - 그러나 Bills 데이터 세트에서 정제 과정을 통해 누락된 주제가 1개 증가; 이는 드물게 나타나는 “문화”와 관련된 주제임.

---

<img width="724" alt="image" src="https://github.com/user-attachments/assets/b986a9cf-43a0-4862-92dd-2c9bef9c2b16" />

<img width="724" alt="image" src="https://github.com/user-attachments/assets/f81fbd95-3d84-4819-8390-17e5ba5d2c86" />

# 5.3 TopicGPT topics are semantically close to ground truth

- TopicGPT는 Bills 데이터의 모든 수정된 실험 환경에서 진실 데이터(ground truth)와 비교 가능한 주제 정렬을 제공함.
- 추가 예시 주제가 포함된 환경이 가장 낮은 성능을 보였으며, 이는 더 많은 예시 주제를 추가하는 것이 항상 유익하지 않음을 시사함.
  - 너무 많은 예시 주제가 모델을 압도할 수 있으며, 중요한 주제를 중심으로 통합하는 대신 다양한 주제에 맞추려 할 가능성이 있음.
  - 따라서 최상의 결과를 위해 예시 주제 목록은 2-3의 고품질 주제로 소규모로 유지할 것을 권장함.

- TopicGPT의 주제 할당 일관성을 평가하기 위해, 기본 설정과 각 수정된 설정 간의 정렬 점수를 계산함.
  - LDA 모델을 10회 실행하여 평균 내부 정렬을 측정하였고, TopicGPT의 할당은 설정 간에 매우 안정적임을 나타냄 (모든 지표가 0.05 범위 내에 위치).
  - TopicGPT는 P1 및 ARI에서 LDA보다 더 높은 안정성을 보였고, NMI는 유사한 수준을 나타냄.
  - 흥미롭게도, 동일한 설정의 두 실행 간에 약간 다른 출력이 발생했으며, 이는 자체 수정 프로세스에 무작위성이 추가된 결과일 수 있음.

- **수작업 주제 매칭 과정**:
  - 세 명의 검토자가 생성된 주제를 검토하고 각 주제를 진실 데이터 분류에 할당.
  - 정확한 일치가 불가능할 경우, 다음의 세 가지 비정렬 카테고리 중 하나에 주제를 라벨링함:
    1. 범위 외(out-of-scope): 연관된 진실 데이터에 비해 너무 좁거나 넓은 주제.
    2. 누락된 주제(missing): 진실 데이터에 존재하지만 생성된 출력에 없는 주제.
    3. 중복 주제(repeated): 다른 주제와 중복되는 주제.

- 각 검토자가 수행한 매핑에 따라 비정렬 주제의 비율을 계산하였으며, TopicGPT는 특히 정제 후 LDA보다 훨씬 적은 비정렬 주제를 포함함.
  - TopicGPT의 초기 및 정제된 주제는 전체적으로 LDA보다 비정렬 가능성이 낮음 (LDA: 62.4%, TopicGPT: 38.7% 초기, 30.3% 정제 후).
  - 검토자들은 TopicGPT의 출력이 모호한 LDA 출력에 비해 훨씬 더 작업하기 쉽다고 언급함.
  - 정제 과정은 일관되게 범위 외 주제와 중복 주제를 줄였으나, Bills 데이터셋에서는 누락된 주제가 증가함 (23개 문서 중 단 1개 문서에 등장하는 "문화" 주제). 

- TopicGPT는 LDA에 비해 사용성과 해석 가능성이 우수하며, 다양한 정제 임계값을 시도하여 연구 질문에 중요한 주제를 걸러내지 않도록 해야 함.

---

# 5.4 Implementing TopicGPT with open-source LLMs

- TopicGPT는 주제 할당 및 생성에서 대안 LLM을 평가함.
- 주제 할당에는 open-source LLM이 사용 가능하나, 주제 생성은 GPT-4 외의 모든 LLM에서 복잡함.
- **Mistral-7B-Instruct** 사용:
  - 주제 할당에 적합한 저비용 대안으로 실험.
  - Mistral의 할당은 인간 기준과 잘 맞지만, GPT-3.5-turbo와 비교해 약 6점 낮은 수치 기록.
- **주제 생성 테스트**:
  - Mistral 및 GPT-3.5-turbo로 주제 생성 시도, 둘 다 포맷 지침을 따르기 어려움.
  - Mistral: 1,418개, GPT-3.5-turbo: 151개 주제 생성, 대부분 매우 세분화되어 있어 정제 과정에서 삭제될 가능성이 큼.
- 복잡한 주제 생성 지침이 필요하므로, **GPT-4** 혹은 유사 능력을 가진 모델을 추천.
- 향후 연구:
  - TopicGPT는 데이터 탐색 시 업적 분석에 효율적일 수 있음.
  - Zero-shot prompting을 통해 탐색적 사용 사례를 위한 잠재력 탐구 가능성 연구 필요.
- **결론**:
  - TopicGPT는 주제 모델링에 최적화된 프레임워크로, 기존 모델 대비 주제 일치성 및 해석 가능성을 높임.
  - 연구자들이 이 프레임워크를 시도해 볼 수 있도록 파이프라인 공개.

---

# 6 Future Work

- TopicGPT는 사용자가 데이터셋의 내용에 대해 어느 정도 익숙하다고 가정하여 유도적 콘텐츠 분석을 위해 설계됨.
- 그러나 이 프레임워크는 사용자가 데이터셋에 익숙하지 않은 데이터 탐색 시나리오에도 적용 가능.
- 탐색적 용도로 TopicGPT에서 예제 주제나 문서를 제공하지 않고 제로샷 프롬프팅의 잠재성을 탐구할 수 있음.
- 부록 A에는 이 프레임워크의 계층적 확장 내용이 제시됨.
- 이 확장의 성능을 기존 계층적 주제 모델과 비교하여 철저한 비교 연구를 통해 평가하는 것이 중요한 다음 단계가 될 것임.

---

# 7 Conclusion

- TopicGPT는 주제 모델링을 위해 특별히 설계된 프롬프트 기반 프레임워크이다.
- 전통적인 주제 모델의 해석 가능성과 적응성의 한계를 해결하고, 고품질의 설명적인 주제를 생성해낸다.
- 연구 결과, TopicGPT는 기본 주제 모델보다 실제 레이블과의 일치성에서 뛰어난 성과를 보이며, 다양한 프롬프트와 데이터 하위 집합에서도 강한 강인성을 보여준다.
- 이 프레임워크는 관심 있는 연구자와 실무자들에게 제공될 예정이다.

---

# 8 Limitations

- **투명성 문제**
  - TopicGPT는 주제 생성을 위한 GPT-4와 주제 할당을 위한 GPT-3.5-turbo를 사용하는데, 이들은 모두 폐쇄형 LLM이다.
  - 이들 모델의 사전 훈련과 지침 조정 데이터셋, 아키텍처 세부정보에 대한 투명성이 부족하다.
  - 향후 연구에서는 더 강력한 개방형 모델을 사용하는 것을 탐구할 수 있다.

- **비용 문제**
  - TopicGPT의 폐쇄형 모델 사용으로 인해 실행당 비용이 발생한다.
  - 사용자들은 개방형 대안 탐색과 함께 프레임워크의 모듈형 설계를 활용하여 비용을 절감할 수 있다.
  - 예를 들어, 사용자가 데이터셋과 관련된 주제 목록만 원할 경우 주제 정제 및 할당 단계를 건너뛰고 실행 시간을 줄일 수 있다.

- **문맥 제한 처리**
  - TopicGPT의 컨텍스트 길이 제한으로 인해 문서를 잘라내야 하는 제한이 있다.
  - 문서의 일부만 제공됨으로써 중요한 맥락을 잃고 전체 문서 내용을 잘못 표현할 위험이 있다.
  - 향후 연구에서는 전체 문서를 길이 제한 내에서 표현할 전략을 탐구할 수 있다.

- **다국어 지원 부족**
  - TopicGPT는 비영어 데이터셋에서 평가되지 않았다.
  - OpenAI의 LLM은 주로 영어 데이터로 사전 훈련되었고, 비영어에 대한 능력이 현저히 떨어진다.
  - 미래의 다국어 LLM의 발전이 TopicGPT의 접근성을 향상시키길 기대한다.

- **윤리적 고려사항**
  - 프레임워크가 제기하는 위험은 이를 지원하는 대형 언어 모델의 본질적인 위험과 다르지 않다.
  - 연구를 위한 인간 평가가 기관의 검토 위원회 승인을 받았다.

---

# 9 Ethical Considerations

- TopicGPT의 위험은 이 시스템을 지원하는 대규모 언어 모델에서 내재된 위험과 유사함.
- 인적 평가 과정은 기관 윤리 위원회의 승인을 받았음.
- 모든 주석자는 자발적으로 참여하였으며 보상을 받지 않았음.
- 연구의 윤리성을 강조하며, 저자들은 주제 모델링 및 평가 결과에 대해 신뢰할 수 있는 정보를 제공하고자 함.
