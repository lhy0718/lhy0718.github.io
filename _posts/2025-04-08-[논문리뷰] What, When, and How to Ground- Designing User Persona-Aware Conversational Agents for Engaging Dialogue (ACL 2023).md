---
title: "[논문리뷰] What, When, and How to Ground- Designing User Persona-Aware Conversational Agents for Engaging Dialogue (ACL 2023)"
date: 2025-04-08 12:00:00 +0900
categories:
  - Paper Review
tags:
  - ACL 2023
---

이 연구에서는 상업적 설정에서 자연스러운 응답 생성을 위한 개인화된 개방형 대화 시스템 구축 방법을 제안하며, WWH 문제를 해결하기 위해 데이터셋 혼합, 부정적 페르소나 정보 증강, 개인화된 대화 데이터셋 설계를 적용하여 대화 유창성과 근거의 균형을 맞추는 방법을 소개합니다. 이러한 접근 방식은 응답 유형 레이블을 도입하여 통제 가능성과 설명 가능성을 개선하며, 주관적 및 객관적 평가에서 더 유창한 대화를 입증합니다.

---

# 1 Introduction

- 맞춤형 대화 시스템(PD 시스템)은 사용자의 페르소나에 대한 장기 기억을 기반으로 사용자 맞춤형 응답을 생성할 수 있음.
- 페르소나 속성은 개인의 성격, 행동, 선호, 경험 등 포괄적인 사용자 관련 정보를 포함.
- PD 시스템의 사용자 참여를 증가시키는 핵심은 맥락에 적합하고 관련 있는 페르소나를 찾아내는 것.
- 각 발화에 대해 결과에 포함될 페르소나 속성은 검색 모델을 통해 조회됨.
- 모델 추론 중 어떤 페르소나 속성을 선택할지 결정하는 것은 어려운 문제(“WHAT to ground” 문제).
- 특정 대화 맥락에서는 페르소나 정보를 사용하지 않는 것이 더 자연스러울 수 있음(“WHEN to ground” 문제).
- PD 시스템 설계를 위해 다음과 같은 세 가지 질문을 다루어야 함:
  1. 대화 맥락에 따라 어떤 개인 정보를 기반에 두어야 하는가?
  2. 개인 정보를 사용하여 언제 응답을 생성해야 하는가?
  3. 자연스럽고 사람 같은 맞춤형 응답을 어떻게 만들어야 하는가?

- 기존 연구는 이상적인 개인화 설정에서 자연스러운 응답 생성을 주로 다루었으나, 실제 대화 시스템에서는 중간 대화 턴과 개인화 응답 간의 간섭 문제도 고려해야 함.
- 대규모 언어 모델(LLMs)은 다양한 자연어 이해(NLU) 작업에서 탁월한 능력을 보여줌.
- 그러나 LLM의 고유 능력만으로는 WWH 문제 해결에 충분하지 않음.
- 제안하는 방법은 페르소나 정보를 효과적으로 활용하도록 모델을 제어하는 것을 목표로 함.
- 제안된 접근 방식:
  1. 다중 세션 맞춤형 대화(MSPC) 데이터셋 생성.
  2. 대화 데이터셋의 혼합 가중치를 조정하여 모델의 페르소나 기반 수준 제어.
  3. 회전 레이블을 활용하여 개인화된 턴과 일반 턴을 구분.
- 18억 개의 파라미터를 갖춘 대형 언어 모델을 통해 개인화된 대화 시스템 구축 및 성능 분석.

---

# 2 Related Work

- **개인화 응답 생성**: PersonaChat 데이터셋 (Zhang et al., 2018) 출시 이후 개인화된 응답 생성을 위한 연구가 활발히 진행됨 (Lee et al., 2021; Liu et al., 2020; Xu et al., 2021, 2022).
  
- **주요 연구 초점**: 연구의 대부분은 응답의 "WHAT"과 "HOW" 문제 해결에 집중, 다양한 모델 아키텍처와 훈련 프레임워크를 활용함 (Fu et al., 2022; Dong et al., 2022).

- **보상 기반 접근법**: Liu et al. (2020)은 상호 개인 인식에 대한 보상을 통해 개인화된 대화 생성을 위한 RL 기반 접근법을 제안.

- **변분 방법 활용**: Wu et al. (2019)와 Fu et al. (2022)는 변분 방법을 통해 개인화되고 지식 기반 응답 생성을 수행.

- **메모리 기반 응답**: et al. (2019)은 선택된 페르소나를 이용한 CV AE를 통해 개인화된 응답을 생성, "WHAT"과 "HOW" 문제를 동시에 해결.

- **페르소나 검색 모듈**: Xu et al. (2022) 및 Bae et al. (2022)는 페르소나 검색 모듈과 생성기 모듈을 통해 같은 문제를 다룸.

- **WWH 질문 해결 필요성**: 현재까지 PD 시스템에서 모든 세 가지 WWH 질문을 동시에 해결한 연구는 없음. 따라서 상업적 시스템에서 이러한 문제를 다루는 방법을 제안하는 것이 중요함.

---

# 3 Dataset

- **목적**: WWH 문제를 해결하기 위한 PD 시스템 개발을 위한 한국어 다중 세션 개인화 대화 데이터셋(MSPD) 구축
- **특징**:
  - 에이전트는 여러 고유한 역할 수행
  - 사용자 성격 정보를 기억하고, 대화 중 도입된 속성도 포함
  - 개인화된 응답 제공: 성격에 기반하여 적절하고 시의적절해야 함
- **목표**: 모델이 어떻게 (HOW) 그리고 언제 (WHEN) 개인화된 응답을 접지하는지를 학습하기 위함
- **구성**:
  - 평균적으로 1회 에피소드당 4세션 포함
  - 각 세션은 사용자와 에이전트 간 10-12 턴으로 구성
  - 자연스러운 대화 흐름 유지 학습 가능
- **주석**: 
  - 개인 정보 포함하는 사용자 발화와 응답에서 사용된 성격 주석화
  - 각 세션당 개인화된 응답 수는 2개 이하로 제한
  - 전반적인 품질 및 적합성 검토 수행
- **총 에피소드 수**: 13,469회
- **부록**: MSPD 데이터셋 통계 및 샘플은 부록 A 및 B에서 확인 가능
- **Dcasual 데이터셋**:
  - MSPD와 함께 비공식 대화 데이터셋 Dcasual 포함
  - 보다 균형 잡힌 모델 훈련을 위한 약 1,250만 발화 수집
  - NIA에서 개발한 한국어 대화 데이터셋 및 크라우드소싱 대화 데이터셋 사용
  - 포함된 데이터셋: PersonaChat, EmpatheticDialogues, Wizard of Wikipedia의 한국어 버전

---

# 4 Methodology

- 모델 훈련 단계에서 WHAT 및 WHEN 질문을 해결하기 위해 다양한 방법을 사용
  - **부정적 페르소나 증강**: 709가지 유형의 부정적 페르소나 활용
  - **데이터셋 혼합**: 다양한 데이터셋의 조합 사용
  - **응답 유형 생성**: 개인화된 응답 생성 방법
  
- 추론 단계에서는 대화 맥락과 페르소나 속성의 하위 집합을 통해 적절한 개인화된 응답 생성 가능
  - 페르소나 속성은 대화의 맥락에 의해 결정됨

- 모델의 결정에 대한 설명 제공
  - **응답 유형 레이블 (RTL)**: 개인화된 응답 생성을 명확히 제어하는 데 사용
  - RTL에 따른 조건부 생성 방식 도입

---

# 4.1 Persona-Grounded Generation

- **입력 데이터 구성**:
  - 훈련 데이터셋의 각 입력은 다음으로 구성됨:
    - 사용자 인구통계 정보 $$d$$ (예: 성별, 나이)
    - 사용자 성격 $$\rho_m$$의 하위 집합 (성격 속성 포함)
    - 대화 맥락 $$c_m = [u_1, a_1, u_2, a_2, \ldots, u_{m-1}, a_{m-1}, u_m]$$ 
      - 여기서 $$u$$는 사용자, $$a$$는 에이전트를 나타냄
    - 목표 응답 $$y_m = [y_{m1}, \ldots, y_{m\ell}]$$는 에이전트 응답 $$a_m$$에 인덱싱됨

- **모델 최적화**:
  - 입력 포맷: $$(d, \rho_m, c_m)$$
  - 개인화된 응답 $$y_m$$에 대한 조건부 확률을 기반으로 모델을 최적화
  - 손실 함수: Negative Log-Likelihood (NLL)
  
- **수식**:
  - 조건부 확률:
    $$ P(y_m \vert d, \rho_m, c_m) = \prod_{t=1}^{\ell} P(y_{m_t} \vert d, \rho_m, c_m, y_m < t) \quad (1) $$
  - NLL 손실:
    $$ LNLL = - \sum_{t=1}^{\ell} \log P(y_{m_t} \vert d, \rho_m, c_m, y_m < t) \quad (2) $$
  - 여기서 $$\ell$$은 목표 응답의 길이를 나타냄.

---

# 4.2 Dataset Blending

- 다양한 대화 데이터셋의 블렌딩은 대화 시스템의 다양성, 공감 및 지식을 향상시키는 데 효과적임을 보여줌 (Smith et al., 2020).
- 개인화된 대화에 맞춘 MSPD를 여러 유형의 캐주얼 대화 데이터셋인 Dcasual와 결합하여:
  - 모델이 더 균형 잡히고 응집력 있으며 자연스러운 대화를 할 수 있게 됨.
  
- 데이터 인스턴스는 (c, r)로 정의되며, 여기서 c는 대화 맥락, r은 목표 응답을 의미함 (섹션 4.1 참조).
- 데이터셋 블렌딩은 각 데이터셋의 블렌딩 가중치에 따라 인스턴스 단위로 수행됨.
  
- WWH 문제를 세밀하게 조정하기 위해 MSPD 데이터셋은 다음과 같이 나뉨:
  - 개인화된 응답: DMSPD-PR
  - 비개인화된 응답: DMSPD-NPR
  - (예: 그림 3의 에이전트의 빨간색 및 검은색 응답)
  
- 최종 훈련 데이터셋은 개별 데이터셋을 과표본화 또는 미표본화하여 구성됨.
- 개별 데이터셋의 훈련 데이터 크기는 다음 식으로 결정됨:
  $$\|D_i \text{(train)}\| = w_i \sum_{j=1}^{N} w_j \times \|D\|$$
  - 여기서 $$N$$은 대화 데이터셋의 집합 $$D = \{D_{\text{casual}1}, \ldots, D_{\text{casual}k}, D_{\text{MSPD-PR}}, D_{\text{MSPD-NPR}}\}$$을 나타냄. 
  - $$D_i$$는 $$D$$ 내 i번째 데이터셋.

---

# 4.3 Control of WHEN & WHAT by Negative Samples

- **WHEN 문제 해결**
  - 모델이 인물 기반 응답을 생성하는 경향 조절 중요.
  - 인물 정보를 바탕으로 적절한 시점에 개인화된 응답 생성 필요.
  - 너무 자주 개인화된 응답을 생성하면 부자연스러운 대화 발생.
  - 너무 드물게 생성할 경우 사용자 참여 증대 부족.
  - 모델이 특정 상황에서 개인화된 응답 대신 일반 응답 생성 필요.
  - 비개인화 응답을 위해 맥락과 관련 없는 인물 속성의 하위 집합 포함.
  - 이를 '부정적 인물 하위 집합 증강(negative persona subset augmentation)'이라고 함.
  - 이 증강이 모델의 과도한 인물 기반 응답 생성을 "억제"함.
  - 하지만 지나친 증강은 모델의 인물 기반 응답 가능성 저하 우려.
  - 따라서 DMSPD-NPR 데이터에 대해서만 증강 수행.

- **WHAT 문제 조절**
  - 모델이 인물 기반 응답을 생성할 때, 어떤 인물 속성을 사용할지 결정 필요.
  - 응답과 관련된 실제 인물 속성 ρpos와 관련이 없는 "부정적" 인물 속성 ρneg1, ..., ρnegk−1 제공.
  - 이를 통해 모델이 대화 맥락에 맞는 적합한 인물 속성 선택 학습.
  - 이를 '부정적 인물 속성 증강(negative persona attribute augmentation)'이라고 함.
  - 응답 유형에 따라 인물 집합 ρin의 하위 집합을 다르게 설정:
    - ρ = { ρnpr (비개인화 응답, DMSPD-NPR) 
             ρpr (개인화 응답, DMSPD-PR) 
             ρc (일반 응답, Dcasual) }
  - 여기서 각 인물 속성 집합 정의:
    - ρnpr = {ρneg1, …, ρnegk}
    - ρpr = {ρpos, ρneg1, …, ρnegk−1}
    - ρc = ϕ.

---

# 4.4 Controllability & Explainability via Response Type Label

- **상업적 환경에서의 제어 가능성**  
  - 개인화된 응답을 생성할 필요성을 판단해야 함  
  - Agent가 사용자에게 메시지를 사전 전송할 시점을 결정  

- **응답 유형 레이블(Response Type Labels, RTL)**  
  - 모델의 결정에 대한 명시적 제어 가능하게 함  
  - 응답 유형 레이블은 <RTL>로 표기  
  - 모델 훈련 시, 응답 및 RTL 토큰 생성: $$P(<RTL>,y|d,ρ,c)$$  
  - 개인화 응답 레이블은 <PRTL>, 일반 응답 레이블은 <CRTL>로 정의  

- **추론 시 RTL 활용**  
  - 삽입된 RTL에 따라 응답 생성  
    - 개인화 응답: $$y \sim P_\theta(·|d,ρ,c, <PRTL>)$$  
    - 일반 응답: $$y \sim P_\theta(·|d,ρ,c, <CRTL>)$$  

- **설명 가능성**  
  - 오류 분석은 상업 시스템에서 중요하여 빠른 디버깅과 문제 해결 필요  
  - 수작업으로 로그 데이터를 검토하는 시간 소모적 과정 발생  
  - RTL을 통해 설명 가능성을 강화하여 오류 분석을 용이하게 함  
  - 결과적으로 서비스 운영 개선에 기여함

---

# 5.1 Experimental Setup

- 제안한 방법의 효과를 검증하기 위해 개인화 대화(PD) 시스템의 성능 비교
  - WWH 문제를 다루는 방식
- 여러 모델의 성능 비교
  - 세밀하게 조정된 기본 모델 사용
  - 데이터셋 혼합 및 부정 샘플링 방법 포함
- 혼합 가중치에 따라 훈련된 모델 비교
  - 모델의 기초 확률 및 유창성에 대한 혼합 가중치의 영향 평가
- 기본 모델:
  - 우리의 내부 18B 매개변수 사전 훈련 언어 모델에서 파생 
  - GPT-3 (Brown et al., 2020)와 동일한 아키텍처 공유
- 모든 실험:
  - SKT의 독점 슈퍼컴퓨터 Titan에서 수행
  - NVIDIA A100 SXM4 80GB GPU 탑재

---

# 5.2 Evaluation

- **목표 평가 (Objective Evaluation)**
  - PPL(Perplexity)를 사용하여 모델이 생성한 응답의 유창성을 측정.
  - 개인 속성과 생성된 응답 간의 F1 점수가 모델의 기초 자료를 grounding하는 능력을 평가하기 위한 대리 지표 역할.
  - P-coverage 점수를 계산하여 사용자 개인 정보가 생성된 응답에서 얼마나 잘 반영되는지를 측정 (Song et al., 2019).

- **주관적 평가 (Subjective Evaluation)**
  - 객관적 평가 지표를 보완하기 위해 세션 및 회전 수준에서 주관적 인간 평가를 수행.
  - Sensibleness and Specificity (SS) 점수를 사용하며, 이는 회전 수준에서 0 또는 1로 평가 (Adiwardana et al., 2020).
  
- **회전 수준에서의 응답 분석**
  - 응답(y)의 개인화 여부 평가.
  - 두 가지 기준에 따라 y를 분류:
    1. **Grounding 수준 (Grounding Level)**
       - **Hard Grounding**: y와 개인 속성(ρ_pos) 간의 직접적이고 명시적인 연관, 높은 표현적 유사성.
       - **Soft Grounding**: y와 ρ_pos 간의 간접적이고 암묵적인 연관, 낮은 표현적 유사성.
    2. **일관성 (Consistency)**
       - **Consistent Grounding**: y와 주어진 ρ_pos 간의 일관성.
       - **Inconsistent Grounding**: y와 주어진 ρ_pos 간의 불일치.

- **모델 성능 비교 표**
  - Table 1: 목표 평가 결과.
  - Table 2: 다양한 혼합 가중치에 따른 평가.

---

# 5.3 Results

- **5.3.1 부정적 페르소나 속성의 영향**
  - Model1: 긍정적인 페르소나 속성으로만 훈련 → F1 점수 11.4, PPL 0.28
  - Model2: 부정적 페르소나 속성 포함 → F1 점수 10.15, PPL 10.97
  - Model2는 적절한 페르소나 속성을 선택해 응답 생성 품질 개선
  - 응답 생성 품질 향상에도 불구하고, 그라운딩 빈도 감소

- **5.3.2 부정적 페르소나 하위 집합의 효과**
  - Model3: 부정적 페르소나 하위 집합 적용 → 응답 생성 시 적절치 않은 페르소나 속성 회피
  - PPL 9.37, F1 점수 01, P-Cover 점수 0.05로 유창성 향상
  - 부적절한 페르소나에 대한 그라운딩 감소로 자연스러운 응답 증가

- **5.3.3 데이터셋 혼합 효과: 모델 유창성과 그라운딩 간의 균형**
  - MSPDNPR 데이터셋의 비중 증가 → F1 점수 0.14에서 0.06으로 감소
  - PPL은 10.46에서 9.33로 감소하였으며, 응답 품질은 향상됨
  - 최적의 대화 시스템은 유창성과 그라운딩 간의 신중한 균형 필요

- **5.3.4 RTL 생성의 효과: 설명 가능성 및 유창성 향상**
  - Model4: RTL과 개인화된 응답 모두 생성 → PPL 8.88로 감소
  - 동시 생성이 품질 향상에 기여한다는 이전 연구와 일치

- **5.3.5 주관적 그라운딩 평가**
  - Model3, Model4의 평균 점수 0.88 이상으로 적절한 응답 생성
  - 하드 그라운딩 사례가 소프트 그라운딩보다 약 4배 많음
  - "bad-sensible" 응답 비율이 10% 미만으로 고품질 개인화 응답 생성 확인

- **5.3.6 객관적 및 주관적 평가 간 상관관계**
  - PPL과 인간의 적절함 판단 간 긍정적 상관관계 확인
  - Model4는 "bad-sensible" 비율이 낮아 응답 품질 향상 입증
  - 주관적 평가 비용 고려 시 객관적 평가가 신뢰할 수 있는 유창성과 그라운딩 경향 평가 가능하다는 결과 도출