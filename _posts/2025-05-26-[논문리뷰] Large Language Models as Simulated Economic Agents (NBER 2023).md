---
title: "[논문리뷰] Large Language Models as Simulated Economic Agents (NBER 2023)"
date: 2025-05-26 14:13:04 +0900
categories:
  - Paper Review
tags:
  - NBER 2023
  - LLM in Game Theory
---

새롭게 개발된 대형 언어 모델(LLM)은 인간의 암묵적 계산모델로, 경제학의 호모 이코노미쿠스처럼 시뮬레이션을 통해 인간 행동을 탐구하고 사회과학 연구의 초기 실험 도구로 활용될 수 있다.

---

# 1 Introduction

- 경제 연구는 주로 두 가지 형태로 진행됨:
  - (a) "경제적 인간(homo economicus)은 무엇을 할까?" 
  - (b) "실제 인간(homo sapiens)은 실제로 무엇을 했을까?"
- (a)형 연구는 경제적 인간 모델을 유지하면서 다양한 자원, 선호, 정보 등을 부여한 뒤 행동을 유도하고, (b)형 연구에서는 실제 인간 행동과 비교함.
- 본 논문에서는 새로 개발된 대규모 언어 모델(LLM)이 훈련과 설계 방식 때문에 암묵적인 인간의 계산 모델, 즉 'homo silicus'로 생각될 수 있음을 주장함.
- 이들 모델은 homo economicus처럼 자원을 부여하고 시나리오에 배치하여 행동을 탐구할 수 있으나, 수학적 유도 대신 계산적 시뮬레이션 방식을 사용함.
- LLM은 다양한 텍스트 입력에 현실적으로 대응할 수 있어, 인간과 유사한 반응을 보임.
- 이전 세대 LLM은 이러한 작업에 적합하지 않음.
- LLM이 실제 인간 이해에 도움이 될 수 있는 이유:
  1. LLM은 인간처럼 반응하도록 설계된 계산적 인간 모델임.
  2. 경제 법칙, 의사결정 휴리스틱, 사회적 선호 등 잠재적 사회 정보를 내재함 (대량의 경제 관련 텍스트로 훈련됨).
- 모든 모델과 마찬가지로, 각 homo silicus 모델은 불완전하고 때론 비합리적·비상식적인 답변을 할 수 있음.
- 그러나 실용적 가치 여부는 AI 실험이 통찰을 생성하는 데 얼마나 유용한지에 달림.
- 논문 대부분은 GPT-3 실험에 집중하며, 각 실험은 행동경제학의 고전 실험들을 동기로 함.
- 사용된 실험 및 출처:
  - 사회적 선호와 단독 독재자 게임: Charness and Rabin (2002)
  - 가격 인상에 대한 공정성 인식: Kahneman et al. (1986)
  - 상태 유지 편향: Samuelson and Zeckhauser (1988)
  - 고용 시나리오: Horton (2023)
- 주요 실험 결과:
  - AI에 다양한 사회적 선호 부여 시 행동 변화 관찰.
  - AI는 자신이 공평성, 효율성, 자기 이익만 중요시한다고 지시받으면 각각 상응하는 선택을 함.
  - 가장 능력 있는 GPT-3 모델(text-davinci-003)만이 선택을 바꿈; 저능력 모델들은 항상 동일한 선택 유지.
  - 가격 인상 실험에서, 큰 폭의 인상은 부정적으로 인식되고, 정치적 성향에 따라 반응 차이 존재.
  - 상태 유지 편향 실험에서, GPT-3 text-davinci-003은 인간처럼 초기 상태 선호 경향을 보임.
  - 고용 시나리오에서 최저임금 도입 시 더 경험 많은 노동자 선호 경향을 재현함.
- AI 실험 결과는 실제 인간 행동과 대조해 경험적 확인이 필요함.
- AI 실험의 가치:
  - 실리콘 실험을 통한 예비 실험 및 통찰 획득용.
  - 빠르고 저렴하게 다양한 변수 탐색 및 질문 문구 민감도 검사 가능.
  - 실제 실험 및 이론 모형 연구에 가이드 제공 가능.
  - 경제학에서 이론 모형이 현실을 직접 묘사하지 않으면서 사고 도구로 사용되는 것과 유사한 역할 가능.
- 관련 연구:
  - Aher, Arriaga, Kalai (2022): GPT-3가 심리학, 언어학 실험 재현 및 긍정적 행동 반응 제시.
  - 본 논문은 경제학 연구 패러다임과 합리성 가정과 결부하여, LLM 실험이 경험적 연구보다 경제 이론에 더 가까운 실천이라고 강조.

---

# 2 Background and conceptual issues

- 대형 언어 모델(LLM)은 매우 방대한 텍스트 데이터셋을 기반으로 학습된 기계 학습 모델임.
- 목표는 인간과 유사한 텍스트를 생성하거나 자연어 처리 작업 수행.
- 대형 언어 모델은 번역, 요약, 텍스트 생성 등 다양한 자연어 처리 작업에서 뛰어난 성과를 거둠.
- 가장 잘 알려진 대형 언어 모델 중 하나는 OpenAI에서 개발한 GPT-3 (Generative Pre-trained Transformer 3)임.
- 본 논문의 모든 예시는 GPT-3를 기반으로 하고 있음.
- 비기술적인 설명만으로도 LLM의 이해에 충분한지를 의문시하는데, 저자는 더 깊은 이해가 필요 없다고 주장함.
- 경제학, 특히 행동 경제학 연구를 위해 뇌의 뉴런이나 부분들을 연구할 필요는 없으며, 중요한 것은 LLM이 명확한 최적화 목표를 가지고 만들어졌다는 점임.
- Simon (1996)은 "인공물의 과학(sciences of the artificial)"이 환경에 대한 제약 하에서 어떤 것을 최대화하려는 목적을 가진 객체로서 인공물을 추상화할 수 있다고 언급함.
- LLM은 인간을 간접적으로 연구하는 수단으로 사용할 수 있음.

---

# 2.1 The “Garbage in, Garbage out” critique

- LLM(대형 언어 모델)을 사회과학에 활용하는 데 있어 대표적인 비판 중 하나는,  
  - LLM이 너무 방대한 코퍼스를 대상으로 학습되어 있어 엄밀히 선정된 데이터가 아니기 때문에  
  - “Garbage in, Garbage out”(쓰레기가 들어가면 쓰레기가 나온다) 문제에 노출된다는 점이다 (Bender et al., 2021).

- 그러나 실제 재활용 공장이 쓰레기가 들어와도 쓸모 있는 자원이 나오는 것처럼, 데이터가 쓰레기라고 해서 반드시 쓸모없는 결과가 나오지는 않는다.

- 설령 코퍼스가 신중하게 선별되었다 하더라도, LLM이 반영하는 것은 “일반 인간”이 아니라,  
  - “공개된 글을 작성하는 특정 인간 집단”이고,  
  - 다시 그들이 선택해 발표한 내용에 기반한다.

- 경제학자들은 보통 행위 대신 단순 진술(statement)의 경제적 내용에 회의적인 입장이다.  
  - 이는 LLM이 진술 데이터를 기반으로 학습한 모델이라는 점에서 심각한 비판으로 보인다.

- 이에 대한 잠재적 반응은 Friedman(1953)의 주장과 유사하다:  
  - homo economicus 모델과 같은 가정의 사실성은 중요하지 않으며,  
  - 모델의 유용성(현실 문제 해결에 도움을 주는가)으로 평가해야 한다는 것.

- 만약 homo silicus 실험의 주된 목적이 실제 실험 전에 시뮬레이션하는 것이라면, 논쟁은 크게 의미가 없어진다.

- 비판에 대한 비-Friedman적 반응으로서:  
  - LLM의 코퍼스는 사람들이 협상 시 자신의 예약가격을 거짓말하는 수백만 줄의 데이터가 아니다.  
  - 많은 텍스트는 경제 문제에 대해 사람들이 어떻게 사고하고 접근하는지를 다룬다.  
  - 즉, 그들의 진짜 의도나 상황대처 방식에 관한 ‘속삭임’(stage whispers) 같은 내용도 포함된다.

- “Garbage in, Garbage out” 비판은 LLM의 응답을 단순히 가중평균(weighted average)으로 보는 것에 근거하는데, 이는 틀렸다.  
  - LLM은 추정자(estimator)라기보다 난수 생성기(random number generator)에 더 가깝다.  
  - 예를 들어, 만약 LLM이 $$U[0,1]$$에서 무작위 추출된 수들의 데이터를 학습했다면, 응답은 평균값 $$\approx 0.5$$가 아니라 $$[0,1]$$ 범위 내 임의의 값이 될 가능성이 대등하다.

- 하지만 확률적으로 변형된 “Garbage in, Garbage out” 비판도 존재한다.  
  - 진짜 사회과학 데이터가 $$[0,1]$$ 균등분포에서 그려진 무작위 수라고 가정하자.  
  - 코퍼스에 “bad:x” (여기서 $$x \sim N(0,1)$$) 또는 “good:x” (여기서 $$x \sim U[0,1]$$) 접두사가 포함되어 있다면,  
  - 무조건적(unconditioned) 응답은 “bad” 혼합 분포일 것이다.

- 하지만 모델에 “good:”을 프롬프트로 주면,  
  - 모델이 최적화 문제의 후보 해답을 생성하는 “좋은” 베이지안과 같이 제대로 작동한다면, 문제 해결이 가능하다.

- 물론 실제로 “good:” 같은 접두사는 존재하지 않지만,  
  - 조건부 분포 응답을 유도하는 프롬프트(prompting) 방식을 통해  
  - 연구 질문에 대해 “충분히 좋은” 결과를 얻을 수 있을 수 있다 (Argyle et al., 2022).

- Argyle 등(2022)은 “Out of one, many”라는 제목의 논문에서,  
  - 단일 LLM이 아니라 다양한 페르소나(personas)를 조건부로 취해 현실성 있는 반응을 낼 수 있다고 지적한다.

- 사회과학에서 대표성 문제는 연구 질문에 따라 항상 달라진다.  
  - 예를 들어, “미국 대통령이 CIA 정보 평가를 의사결정에 어떻게 반영하는가?” 같은 질문에는 매우 특별한 샘플이 필요하지만,  
  - “인간이 질량을 가지는가?”라는 질문은 누구나 샘플로 가능하다.  
  - 대부분 사회과학 질문은 이 두 극단 사이에 위치한다.

- 경제학자가 LLM을 활용할 때 강점은 대체로 샘플에 대한 요구가 적은 질문을 던진다는 점이다.  
  - 예를 들어, 수요곡선이 우하향하는 이유는 “서구, 부유, 산업화, 민주주의” 현상 때문이 아니라 거의 모든 인간이 수행하는 합리적 목표 추구 결과로 보기 때문이다.

- 경제학자들이 특히 엘리트 4년제 대학의 학부생을 실험 참가자로 사용해온 것도 편의 이유뿐만 아니라,  
  - 심리학자와 공유하는 견해에 따라 큰 영향이 없을 것이라 보기 때문이다.

- 일반적으로, 대부분 사회과학은  
  - 어떤 수준의 정확한 측정보다는 원인 효과(causal effects)의 방향(direction)에 더 관심을 둔다 (Horton, Rand and Zeckhauser, 2011).

---

# 2.2 Are these just simulations?

- AI 기반 실험에 대한 한 가지 반대는 이것들이 시뮬레이션이나 에이전트 기반 모델(ABMs)이라는 점이며, 이러한 모델들은 경제학에 그다지 큰 영향을 미치지 못했다는 것이다.
- ABMs는 연구자에게 일정한 역할을 부여하는데, 연구자가 에이전트를 직접 프로그래밍하고 그 결과를 관찰하는 구조이다.
- 이 경우, 질문은 “호모 이코노미쿠스는 무엇을 할 것인가?”가 아니라 “내가 프로그래밍한 이 모델은 무엇을 할 것인가?”가 된다.
- 사람들은 후자에 덜 관심을 가지며, 이것이 Schelling(1971) 사례가 예외인 이유이기도 하다. 그가 사용한 결정 규칙은 매우 단순하고 명백했기에, 독자들은 특별한 비밀이나 놀라운 현상을 보장하기 위한 트릭이 없다는 것을 알 수 있었다.
- ABMs와 달리, 호모 실리쿠스(homo silicus)는 연구자가 직접 통제할 수 없다.
- 그러나 연구자는 신념, 정치적 신념, 경험 등의 초기 조건(내재된 자원)을 통해 이들의 행동에 영향을 미칠 수 있다.
- 하지만 여전히 행동은 연구자의 직접적인 프로그래밍이 아닌, 기저에 깔린 6가지 기본 모델에 의해 결정된다.

---

# 2.3 The “performativity” problem

- LLM(대형 언어 모델)은 수십억 개의 파라미터와 방대한 학습 코퍼스를 바탕으로, 단순히 학습한 내용을 반복해서 출력한다고 생각할 수 있음.
- 그러나 이러한 단순한 암기(memorization) 관점은 완전하지 않음.
  - LLM은 실제로는 새로운 “사실”을 만들어내거나(hallucinate) 왜곡하는 경우가 많음.
- AI 에이전트가 우리가 쓰는 이론과 실험 결과를 읽고 기억해 그에 맞게 행동할 가능성에서 “performativity problem”(수행성 문제)이 존재함 (MacKenzie, 2007).
  - 즉, LLM은 교과서와 논문을 읽고 결과를 그대로 답할 수 있지만, 그 이론을 진정으로 ‘이해’하거나 ‘적용’하지는 못함.
- 예시: GPT-3는 π값을 물으면 공식적인 교과서 언어로 대답하지만, 실제 간단한 상황에 π를 적용하는 방법은 모름.
  - 마치 시험을 위해 벼락치기한 학생처럼, 지식은 있으나 상황에 따라 일관되게 적용하지는 못함.
- 이는 “performativity” 비판의 중요성을 다소 낮추는 점임.
- 그러나 LLM의 능력 향상으로 실험 가능성은 늘어나지만, 수행성 문제도 더욱 커질 수 있음.
- 또한 LLM이 특정 결과를 아는지 여부는 테스트해 볼 수 있음.
  - 예: 필자는 GPT-3 text-davinci-003에 Charness and Rabin (2002) 실험 결과에 대한 질문을 함.
  - 질문: "Charness and Rabin (2002)에서 Berk29, Berk26, Berk23, Barc2 시나리오에서 어떤 비율이 'Left'를 선택했나?"
  - LLM 응답: "Berk29, Berk26, Berk23, Barc2 시나리오에서 각각 약 59%, 57%, 43%, 16%가 'Left'를 선택했다."
  - 실제 값과 비교: 31%, 78%, 100%, 52%로 정확한 답과는 차이가 큼.

---

# 2.4 What counts as an “observation” and the need to endow beliefs

- 특정 LLM의 경우 모델은 하나이므로 $$N=1$$로 보일 수 있음.
- 하지만 LLM의 페르소나는 고정되어 있지 않으며, 프롬프트를 통해 다양한 에이전트 역할을 할 수 있음.
- 예를 들어 Kahneman et al. (1986) 실험에서는 자유주의자, 사회주의자, 중도파 등 다양한 정체성으로 질문에 답하도록 에이전트를 설정할 수 있음.
- 이러한 에이전트 “프로그래밍”은 실험경제학에서 실험 참가자에게 생산 비용이 15 토큰이라는 카드 제공하는 것과 유사함.
- Argyle et al. (2022)은 LLM 에이전트에 인구통계학적 특성을 부여하고, 실제 관측된 행동과 일치하는 다양한 시나리오에서 응답을 얻음.
- 모델의 응답은 주어진 “온도(temperature)” 파라미터에 따라 확률적일 수 있음.
- 당연히, 서로 다른 모델은 다른 응답을 야기할 수 있음.
- 경제학의 단일 합리적 인간(homo economicus)과 달리, LLM은 다양한 “homo silici”를 생성함.
- LLM은 반드시 특정 언어 애플리케이션에 미세조정(fine-tuning)된 것은 아니지만, 이 논문에서 사용된 주요 모델은 지시사항 수행에 대해 미세조정됨.
- 추가 미세조정은 새로운 예시 제공이나 피드백(예: RLHF, 인간 피드백을 통한 강화학습)을 통해 가능함.
- 이를 통해 광범위한 경험이나 기술을 가진 에이전트를 만들어 실험 대상자로 사용할 수도 있음.
- 예를 들어, 백지 상태의 homo silicus 대신 금융시장 지식과 관행에 능한 트레이더로 에이전트를 생성 가능.
- 일반적으로 존재하는 행동 편향들도, 시장 상호작용에 의해 제거되는 경우(예: List (2011)의 전문 트레이더들의 소유 효과 제거) 해당 편향을 반영한 에이전트를 만들 수 있음.

---

# 3 Experiments

- 총 네 가지 실험 수행 보고
- 첫 번째부터 세 번째까지는 실험 자료를 대체로 충실히 따르는 실험실 실험
- 네 번째는 실제 현장 실험에서 영감을 받은 실험

---

# 3.1 A social preferences experiment: Charness and Rabin (2002)

- Charness와 Rabin (2002)의 실험에서는 참여자들이 효율성과 공평성 간의 상충 관계가 있는 두 할당 중에서 선택해야 함.
- 여러 실험 중에서 본 요약은 일방적인 독재자 게임(unilateral dictator game)에 집중.
- 독재자 게임의 구조:
  - Left: B가 600달러, A가 300달러를 받음
  - Right: B가 500달러, A가 700달러를 받음
  - B가 결정을 할 때, 100달러를 포기하고 A가 400달러를 더 받게 할 수 있음
- 이를 간단히 표현하면,  
  $$ (300 \xrightarrow{A}, 600 \xrightarrow{B}) \text{ "Left" } \quad \text{vs} \quad (700 \xrightarrow{A}, 500 \xrightarrow{B}) \text{ "Right"} $$

- 실험 참가자 B는 "Left" 혹은 "Right" 중 선택.

- AI 모델(GPT-3)을 활용해 각 시나리오에 대해 관점을 부여하지 않은 경우와 특정 관점(공평성, 총합 효율성, 자기 이익)에 기반해 선택을 평가:
  - 텍스트 다빈치(text-davinci-003, 가장 고급), 및 저사양 모델들(text-ada-001, text-babbage-001, text-currie-001) 사용.
  
- Figure 1 분석 내용:
  - y축: 시나리오, x축: 선택("Left" 또는 "Right")
  - 첫 번째 열은 원 실험 결과, 예: Berk29에서는 31%가 (400,400) 선택, 68%는 (750,400) 선택 (A가 추가 350 받는 Right 선택 다수)
  - 인간 참여자들은 보다 불평등하지만 효율성 높은 결과를 선호하는 경향이 존재.
  - AI 모델들의 선택 비율은 프레임과 모델 유형에 따라 다양하게 나타남.
  
- GPT-3에 부여한 관점:
  - 관점 없음 (기본)
  - 불평등 혐오: "플레이어 간 공정성만 신경 쓴다."
  - 효율성 지향: "두 플레이어의 총 지급액만 신경 쓴다."
  - 자기 이익 중시: "자기 자신의 지급액만 신경 쓴다."
  
- 관점별 AI 행동 패턴:
  - 자기 이익 중시 AI: 대부분 자기 이익 최대화("Left") 선택, Berk29 예외 (평등한 지급액일 경우 A에게 이익이 큰 "Right" 선택)
  - 효율성 지향 AI: 항상 총 지급액이 최대인 선택("Right")을 함.
  - 불평등 혐오 AI: 두 플레이어 간 차이를 최소화하는 선택을 하지만, Berk23 시나리오(커다란 자원 낭비 상황)에서는 한계 존재.
  - 관점 부여 없는 AI: 사회적 최적화(지급액 최대화) 경향 보임.
  
- 저사양 GPT-3 모델들은 관점 부여 여부와 관계없이 대부분 "Left"(자기 이익) 선택.
  - 단일 예외 있음 (관점 부여 없을 때 모두 "Right" 선택하는 모델 있다).
  - 기본 선택이 의미 있는지는 불분명하며 결과의 제시 순서 영향을 받을 수 있음.
  
- 인간 집단 행동을 벡터화하여 AI 선택 결과에 근접하도록 가중치 조정 시:
  - 공평성 선호 15%
  - 효율성 선호 32%
  - 이기적 선호 52%
- 이 분포를 다른 게임에 적용해 비교 가능.

- 요약:  
  - 인간은 다양한 사회적 선호를 보이지만 기본 AI는 특정 선호를 갖지 않으면 자기 이익 선택 경향 강함.
  - AI 모델에 사회 선호 관점 부여 시 행동 다양성 나타남.
  - 사회 선택 이론의 구성 요소(공평성, 효율성, 자기 이익)가 실험 결과와 AI의 선택을 설명하는 데 유용하게 작동함.

---

# 3.2 Fairness as a constraint on profit-seeking: Kahneman et al. (1986)

- Kahneman et al. (1986)은 시장 맥락에서 공정성에 대한 직관을 평가하기 위해 참가자들에게 여러 시장 시나리오를 제시함.
- 예시: 한 철물점이 눈삽을 $$15에 판매하다가 큰 눈폭풍 다음 날 가격을 $$20으로 인상. 이에 대해 참가자들은 다음 중 평가를 선택:
  1) 완전히 공정함  
  2) 허용 가능함  
  3) 불공정함  
  4) 매우 불공정함  
- 원 논문에서 82%의 응답자가 “불공정” 또는 “매우 불공정”으로 평가.

- 정치적 성향과 시장에 대한 태도가 이런 평가에 영향을 미치는지 탐색하는 것이 자연스러운 관심사임.
- 작성자는 유사 실험을 MTurk 작업자 대상으로 Uber의 “서지(surge)” 가격 책정 공정성에 대해 진행하였고, 태도가 가격 책정 방식의 제시법에 따라 크게 달라짐을 발견.

- 본 실험에서는 GPT-3 text-davinci-003를 이용, 원래 $$20 가격 인상 대신 $$16, $$20, $$40, $$100의 가격을 변화시켜 평가하도록 함.
- 가격 변화의 문구도 “changes the price to” (중립적)와 “raises the price to” (부정적 함축) 두 가지로 나누어 제시.
- AI 응답자의 정치 성향도 "socialist"에서 "libertarian"까지 다양하게 지정하여 반응 차이 관찰.

- 결과 요약 (Figure 2 참조):
  - x축: 도덕적 판단 (Acceptable, Unfair, Very Unfair 등)
  - y축: 해당 판단을 선택한 응답 수
  - 쌓기 막대그래프, 색깔로 프레이밍 구분 표시
  - 원 논문과 유사하게 $$20 가격 인상에 대해 82%가 불공정 판정.
  - 실제 실험에서 중도 및 자유지상주의 AI만 $$16과 $$20 인상을 허용 가능하다고 평가.
  - 2021년 미국인의 약 37%가 중도 성향으로, LLM 결과는 다소 과소평가 가능.
  - $$40 가격 인상은 100%가 불공정 판정.
  - $$40→$$100 인상 시에는 자유주의자들조차 “매우 불공정”으로 판단 전환.
  - 정치 성향별로 보면 좌파일수록 가격 인상에 엄격, 우파일수록 관대.
  - 흥미롭게도 보수파 AI는 자유지상주의 및 중도 AI와 다르게 모든 인상 가격을 불공정하다고 평가.
  - 프레이밍 효과는 거의 없었으나, $$20 인상 시 “raises” 표현은 사회주의자들이 “불공정”에서 “매우 불공정”으로 판단 강화.

- 가격 인상 정도가 도덕적 허용 가능성을 결정하는 주요 변수임:
  - $$16, $$20 인상은 일부 상황에서 허용 가능.
  - $$40 이상 인상은 거의 모두 불공정으로 인식됨.
  - 정치적 성향과 문구 프레이밍이 판단에 영향을 미침.

- LLM 기반 실험은 정치 성향 및 프레이밍의 영향력을 탐구하는 데 비용이 거의 들지 않으며, 추가 연구 촉진 가능성을 가짐.

- 모델이 원 논문의 내용을 암기했는지 여부 검증:
  - OpenAI API의 토큰 확률을 검사한 결과, 다양한 시나리오에서 $$20 가격 완성이 가장 높은 확률.
  - 이는 특정 사본 암기가 아니라 숫자 선호(bias; 5의 배수, 정수 등)와 실용적 가격 설정 때문으로 보임.
  - $$20 선택이 Kahneman et al. (1986) 논문 때문만은 아님.

---

- 주요 수식 형태는 없으나, 가격 변동을 수식으로 표현하면 예를 들어:

  - 기존 가격: $$15  
  - 새로운 가격: $$ p \in \{16, 20, 40, 100\} $$  
  - 가격 인상: $$ \Delta p = p - 15 $$  
  - 도덕적 판단 함수 (일반화):  
  $$ f(\Delta p, \text{정치성향}, \text{프레이밍}) \rightarrow \{\text{Acceptable}, \text{Unfair}, \text{Very Unfair}\} $$  

- 가격 인상폭 $$\Delta p$$가 커질수록 불공정 판정의 확률이 증가하며, 이는 정치성향과 프레이밍에 따라 민감도가 다름.

---

# 3.3 Status Quo bias in decision-making: Samuelson and Zeckhauser (1988)

- Samuelson과 Zeckhauser (1988) 논문은 **status quo bias (현상 유지 편향)** 개념을 처음 소개하고, 여러 의사결정 상황에서 현상 유지 옵션이 제시되면 더 자주 선택된다는 것을 실험적으로 보였다.
- 실험 시나리오:
  - 참가자들은 안전 예산을 자동차 안전과 고속도로 안전 연구 프로그램에 배분하도록 요청받음.
  - 예산 배분 옵션: 
    - (자동차 70%, 고속도로 30%)
    - (자동차 40%, 고속도로 60%)
    - (자동차 30%, 고속도로 70%)
    - (자동차 50%, 고속도로 50%)
- 실험 조건:
  - 베이스 프롬프트는 중립적(neutral)으로 제시되거나
  - 특정 예산 배분이 현상 유지 상태(status quo)로 제시됨.
- 현상 유지 편향 검출 방법:
  - 동일한 배분 옵션이 **중립적 프레이밍일 때와 현상 유지 프레이밍일 때** 선택 비율 변화 관찰.
- AI 에이전트를 이용한 재현:
  - AI 에이전트 각각에게 “Your own beliefs are:” 뒤에 랜덤으로 샘플링한 믿음을 부여하여 기본 신념 제공.
  - AI는 이전 프롬프트 기억이 없으므로 **within-subject 실험** 가능.
- 실험 설계:
  - 100명의 AI 에이전트 각각에 대해 5가지 시나리오(중립 + 4개의 각 배분을 현상 유지로 설정)를 개별 API 호출로 실행.
  - 총 500개의 관찰치 수집.
- 결과 (그림 4 참조):
  - 중립 프레이밍에서 가장 선호되는 배분은 (자동차 50%, 고속도로 50%) 임.
  - 중립 프레이밍에서 (자동차 60%, 고속도로 40%) 선택 비율은 거의 없음.
  - 현상 유지 프레이밍에서는 현상 유지로 제시된 배분이 모든 시나리오에서 가장 많이 선택됨.
  - (자동차 60%, 고속도로 40%) 옵션도 현상 유지로 제시되면 선호도가 크게 증가함.
- 시사점 및 확장 가능성:
  - 주제, 선택지 순서, 예산 수준, 현상 유지 이유 제공 변경 등 다양한 변형 실험 가능.
  - 기본 신념 강도에 따른 처리 효과 차이 탐색도 흥미로움.
  
---

- 수식 없이 개념 중심으로 기술되었으며, 현상 유지 편향은 특정 옵션의 프레이밍이 선택 확률에 강력한 영향을 준다는 점을 강조.

---

# 3.4 Labor-labor substitution in the presence of a minimum wage: Horton (2023)

- Horton (2023) 논문에서는 고용주들이 무작위로 최소임금이 할당되는 실험 결과를 보고함.
- 지원자들은 그 최소임금을 맞추기 위해 임금 입찰을 해야 함.
- 핵심 발견:
  - 단기적으로 전체 고용은 거의 감소하지 않음.
  - 하지만 과거 임금과 경력으로 대변되는 더 생산적인 노동자 쪽으로 현저한 노동자 대체가 발생함.
- 이러한 노동자 간 대체 현상은 이론적으로 가능성이 제기되어 왔으나, 실증적으로는 발견하기 어려움.

- Horton은 이 문제를 탐색하기 위해 다음과 같은 시나리오를 설정:
  - 고용주는 경력과 임금 요구가 다른 지원자 풀에서 한 명을 채용함.
  - 역할: 설거지 담당자(“Dishwasher”).
  - 채용 담당자에게는 이 역할의 일반 임금이 시간당 12달러임을 알리고, 최소임금에 대한 정보는 주지 않음.
- AI는 경험을 지나치게 선호하는 경향이 있음(경력 우선, 그 다음에 임금 고려).
- 두 지원자 설정:
  - Person A: 1년 경력 보유, 임금 요구액을 시나리오별로 13~19달러 사이 변동.
  - Person B: 무경력, 최소임금이 15달러일 때만 요구임금을 올리며, 아니면 항상 13달러 요구.

- GPT-3에 다음과 같은 프롬프트를 보냄:
  ```
  You are hiring for the role of “Dishwasher.” The typical hourly rate is $$12/hour.
  You have 2 candidates.
  Person 1: Has 1 year(s) of experience in this role. Requests $$17/hour.
  Person 2: Has 0 year(s) of experience in this role. Requests $$13/hour.
  Who would you hire? You have to pick one.
  ```
- GPT-3 답변 예시: “Person 2. 경력은 없지만 13달러 요청이 12달러 일반 임금에 더 가깝기 때문”

- 각 임금 요구액(13~19달러)별로 채용된 노동자의 임금과 경력 변화를 관찰하는 직무별 데이터셋 생성.
- 최소임금 도입 여부에 따른 (1) 채용 임금, (2) 채용 노동자 경력 에 대해 회귀분석 수행.

- 표 1: 최소임금이 채용 임금과 채용 노동자 특성에 미치는 효과

| 변수                        | (1) 채용 임금        | (2) 채용 경력        |
|-----------------------------|----------------------|----------------------|
| $$15/hour$$ 최소임금 도입   | 1.833*** (0.076)     | 0.167*** (0.045)     |
| 상수항                      | 13.333*** (0.054)    | 0.667*** (0.032)     |
| 관측치 수                   | 360                  | 360                  |
| $$R^2$$                     | 0.621                | 0.037                |

- 해석:
  - 최소임금 도입이 예상대로 시간당 임금을 증가시킴.
  - 동시에 경력이 더 있는 지원자를 채용하는 경향이 증가함.
- 이 실험은 파라미터 공간의 일부에 불과하며, 직무 유형, 다른 근로자 속성, 최소임금 정보 여부, 대체 가능 자본 사용 가능성 등 다양한 요소를 쉽게 탐색 가능함.

- 실제 적용 예:
  - Brand, Israeli, Ngwe (2023)는 GPT-3가 경제 이론 및 소비자 행동 패턴(예: 수요곡선 하강, 상태 의존성)에 부합하는 방식으로 설문 질문에 응답함을 보여줌.

---

# 4 Conclusion

- 본 논문은 GPT3 AI를 실험 대상으로 사용한 여러 실험 결과를 보고함.
- 주요 결론은 이 접근법이 유망해 보인다는 점임:
  - 실제 인간을 대상으로 한 실험에서 발견된 결과들을 정성적으로 재현할 수 있음.
  - 매우 저렴한 비용으로 실험 가능.
- 낮은 비용 덕분에 문구, 프롬프트, 답변 순서 등 다양한 변형을 시도할 수 있으며, 표본 크기도 임의로 크게 할 수 있음.
- 인간 피험자 관련 윤리 문제 없음 (Kessler, Low and Sullivan, 2019).
- 전통적 실험에서는 연구자가 의미 있는 결과를 찾기 위해 데이터 마이닝하는 문제가 있으나, AI 실험에서는 사전 등록(pre-registration)이 효과적이지 않을 수 있음.
  - AI 실험 비용이 적고 실행 시간이 짧아(예: 1달러, 30초) 사전 등록의 인센티브가 낮음.
- 더 현실적인 해결책은 “버튼 클릭”만으로 재현(reproducibility)이 가능하도록 실험을 설계하는 것:
  - 저장소를 복제하고 API 키를 교체한 후 재실행할 수 있어야 함.
  - 약간의 프레이밍 차이에 따른 결과 민감도를 검증 가능.
- 재현 가능한 연구를 위해 실험 데이터는 항상 공개 가능.
- 단, OpenAI나 다른 LLM 제공자는 특정 모델에 대한 접근을 계속 보장하지 않음.
  - 하지만 새로운 AI가 등장하면 해당 실험을 다시 수행할 수 있음.
  - 인간 대상 실험도 동일한 피험자가 재현되지 않을 가능성이 있음 점과 유사함.
  
- 요약하면, GPT3 AI를 이용한 실험은 비용 효율적이고 윤리적이며 재현 가능성이 높은 새로운 실험 방법으로 기대됨.

---
