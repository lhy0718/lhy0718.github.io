---
title: "[논문리뷰] Building a Personalized Dialogue System with Prompt-Tuning (NAACL 2022)"
date: 2025-04-07 17:00:00 +0900
categories:
  - Paper Review
tags:
  - NAACL 2022
  - Persona-based Dialogue
---

이 연구에서는 캐릭터 설정에 기반한 일관된 응답을 제공하는 대화 시스템을 구축하며, 사전 훈련된 대형 언어 모델에 저비용의 프롬프트 튜닝 방식을 적용하여 자연스럽고 개인화된 응답을 생성할 수 있음을 보여준다.

---

# 1 Introduction

- 대화 시스템을 훈련하는 데 사용되는 대규모 대화 말뭉치는 다양한 발화자의 발화를 포함
  - 이로 인해 훈련된 시스템이 생성하는 발화가 일관성이 결여됨 (Li et al., 2016b)
  - 예: "나는 도쿄에서 왔어요." 다음에 "나는 교토에서 왔어요."라고 말할 수 있음

- 일관된 발화를 위해, 페르소나에 기반한 응답을 생성하는 대화 시스템 구축 목표
  - 모델에 페르소나를 부여하는 간단한 방법: 자연어로 페르소나를 모델 입력에 연결 (Zhang et al., 2018)
  - 하지만 이 방법은 추가되는 페르소나 정보로 인해 입력 텍스트가 길어지는 단점 존재

- 제안하는 방법:
  - 사전 훈련된 언어 모델의 모든 매개변수를 동결하고 입력 토큰 시퀀스 전에 고정 길이의 프롬프트 추가
  - 페르소나 정보를 포함하도록 임베딩된 벡터만 최적화
  
- 실험 언어: 영어와 일본어
  - 자동 및 수동 평가 결과, 제안한 방법으로 페르소나 기반의 자연스러운 응답이 가능한 대화 시스템 구축 가능성 확인

- 장점:
  - 사전 훈련된 모델의 매개변수를 업데이트하지 않음으로써 훈련에 필요한 계산 비용 절감
  - 수백~수천 개의 발화-응답 쌍으로 이루어진 작은 데이터셋으로도 개인화된 대화 시스템 구축 가능

---

# 2 Related Work

## 2.1 프롬프트 조정 (Prompt-Tuning)
- **사전학습 모델**: BERT 및 T5와 같은 모델의 등장으로 사전학습 모델을 특정 작업에 적합하게 조정하는 방법이 주류로 자리잡음.
- **파라미터 업데이트 필요 없음**: 모델의 규모가 커짐에 따라 파라미터를 업데이트하지 않고 작업에 적응하는 방법이 주목받음.
- **제로/소수-shot 학습**: Brown 등(2020)이 수동으로 작성된 작업 설명과 예제를 기반으로 한 방법 제안.
- **정확도 문제**: 기존 연구(Reynolds and McDonell, 2021; Zhao et al., 2021)는 정확도에서 파인튜닝에 미치지 못함.
- **자동화된 프롬프트 최적화**: 프롬프트 조정은 수작업 없이 프롬프트를 최적화하는 방식.
  - **두 가지 방법**:
    - 이산 어휘에서 최상의 단어 선택 (Shin et al., 2020)
    - 연속 임베딩 벡터 최적화 (Qin and Eisner, 2021; Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021; Vu et al., 2021)
  - **프리픽스 조정**: 입력의 시작 부분에 프리픽스 토큰을 추가하고 해당 임베딩 벡터만 최적화 (Lester et al., 2021; Li and Liang, 2021).
  - **다중 모달 프롬프트 조정**: 이미지와 자연어를 위한 연구 (Tsimpoukelli et al., 2021).

## 2.2 페르소나 기반 대화 시스템 (Persona-Based Dialogue Systems)
- **자연스러운 상호작용**: Roller et al. (2021)에 따르면, 일관된 성격, 지식, 감정 및 공감을 고려하는 것이 중요.
- **페르소나 중심 강조**: 일관된 응답 생성을 위해 성격을 가장 중요하게 삼음.
- **페르소나-챗 데이터셋**: Zhang et al. (2018)이 성격 추가를 목표로 만든 데이터셋.
  - **구성**: 두 명의 작업자가 각기 5개의 페르소나 문장을 기반으로 한 다중 회차 대화.
  - **총 1,155개 페르소나**: 각 페르소나에 대해 원본 문장과 수정된 문장 제공.
- **모델 훈련 방식**: Zhang et al. (2018)의 연구에서는 다양한 페르소나 기반 발화를 사용하여 모델을 훈련한 반면, 본 방법은 단일 페르소나 기반 발화로 모델을 훈련.
- **일본어 버전**: JPersonaChat (Sugiyama et al., 2021).
- **다른 대화 데이터**: PersonalDialog (Zheng et al., 2019) 및 Reddit 대화 데이터 코퍼스 (Mazaré et al., 2018).
  - **인코딩된 페르소나 정보 추가**: Zheng et al. (2019)이 seq2seq 모델의 입력 전 단계에서 방법 제안.

---

# 3 Method

- 제안된 방법 소개
- 실험에 대한 자세한 설정은 섹션 4.1과 4.2에서 설명됨

---

# 3.1 Proposed Model

- 변환기(Transformer) 기반 모델 제안
- 추가 임베딩 레이어:
  - 개인 정보 토큰(pesona info tokens) 위한
- 개인 정보 토큰 설명:
  - 사용자의 개인 정보를 담고 있는 토큰
- 모델의 아키텍처 및 입출력 관계:
  - 그림 1에 표시됨

---

# 3.2 Datasets

- 일상 대화는 항상 개인 정보와 관련되지 않음 (Song et al., 2021).
- 제안된 모델은 다음의 두 가지 종류의 대화 데이터셋으로 구성됨:
  - **개인 정보 기반 대화 데이터셋**: 각 발화가 개인 정보에 기반함.
  - **비개인 정보 대화 데이터셋**: 개인 정보와 관련이 없는 발화로 구성됨.
- 모델은 개인 정보와 관련된 발화 뿐만 아니라 비관련 발화도 생성할 수 있도록 설계됨.

---

# 3.3 Training

- 신규 추가된 임베딩 레이어는 개인 정보 토큰을 임베딩함.
- 사전 훈련된 언어 모델의 임베딩 레이어는 발화와 응답 쌍을 임베딩함.
- 이 임베딩 벡터들은 결합되어 모델에 입력됨.
- 훈련 중 응답 문장의 출력 토큰에 대해 교차 엔트로피 손실이 계산됨.
- 개인 정보 토큰의 임베딩 레이어의 파라미터만 업데이트됨.
- 개인 정보 토큰의 임베딩 레이어는 Persona-Chat 데이터셋에 포함된 개인 문장으로 초기화됨.
- 해당 문장들은 사전 훈련된 언어 모델의 임베딩 레이어에 의해 벡터로 임베딩되며 초기화에 사용됨.
- 개인 문장의 토큰 수가 개인 정보 토큰의 길이보다 적으면, 개인 문장은 필요할 때까지 반복 배치됨.

---

# 4 Experiments

- **대화 시스템 구축**:
  - 섹션 3의 방법을 기반으로 개인화된 대화 시스템을 구축.
  - Hugging Face의 Transformers를 사용.

- **하드웨어**:
  - NVIDIA A100 SXM4 GPU 사용 (GPU 메모리: 40 GB).

- **실험 언어**:
  - 주요 실험은 영어로 수행.
  - 일본어 추가 실험 결과도 섹션 마지막에 포함.

---

# 4.1 Datasets Setup

- 실험에 사용된 데이터셋:
  - **Persona-Chat Dataset**: 1
  - **DailyDialog Dataset**: Li et al. (2017) 2
- 두 데이터셋 모두 영어로 진행됨.

---

# 4.1.1 Training Datasets

- **다중 회전 대화**: Persona-Chat 데이터셋의 다중 회전 대화는 한 라운드 왕복의 두 발화로 분할됨.
- **대화 쌍**: 두 발화의 쌍을 대화 쌍이라고 지칭.
- **개인화 유형**: 대화 쌍은 응답자에게 주어진 개인화 유형에 따라 집계됨.
- **사용된 페르소나**: 
  - Persona-Chat 데이터셋에는 1,155개의 페르소나가 존재.
  - 실험에서는 대화 쌍이 많은 세 가지 페르소나를 선택하여 실험 진행.
  - 선택된 페르소나의 대화 쌍 개수: 
    - 185, 167, 166.
- **모델 훈련 및 평가**: 
  - 각 실험 설정에 대해 세 가지 페르소나에 해당하는 모델을 훈련 및 평가.
- **훈련 및 평가 비율**: 
  - 집계된 대화 쌍은 9:1 비율로 훈련 쌍과 평가 쌍으로 분할됨.
- **짧은 발화 추가**: 
  - Persona-Chat 데이터셋에는 짧고 개인화와 관련 없는 발화가 부족.
  - 이를 보완하기 위해 DailyDialog에서 '관계' 주제의 대화 쌍을 추가.
  - 짧은 발화는 발화와 응답 모두 50자는 미만일 때 포함.
- **혼합 비율**: 
  - Preliminary 실험 결과를 바탕으로 DailyDialog에서 추가된 대화 쌍과 Persona-Chat에서 얻은 대화 쌍의 비율을 1:1로 설정함.
- **훈련 데이터셋 비율**: 이 비율을 훈련 데이터셋 비율이라고 정의함.

---

# 4.1.2 Evaluation Datasets

- 평가를 위해 두 가지 데이터셋을 생성함:
  - **Persona Eval Dataset**: 
    - 전체 9:1 데이터셋의 10%를 차지함.
  - **General Eval Dataset**: 
    - DailyDialog에서 수집한 대화 쌍으로 구성됨.

- 일반 평가 데이터셋의 주제:
  - 태도 및 감정
  - 문화 및 교육
  - 금융
  - 건강
  - 일상생활
  - 정치
  - 관계
  - 학교 생활
  - 관광
  - 업무

- **훈련 방법 및 모델 성능**:
  - Dist-1과 Dist-2에 따른 자동 평가 결과
    - Fine-Tuning (added) 모델:
      - GPT2-XL: Dist-1 = 0.199, Dist-2 = 0.526
    - Fine-Tuning (none) 모델:
      - GPT2-XL: Dist-1 = 0.210, Dist-2 = 0.568
    - Prompt-Tuning 모델:
      - GPT-J-6B: Dist-1 = 0.213, Dist-2 = 0.595

- **결과 해석**:
  - prompt-tuned GPT-J-6B 모델이 가장 다양한 응답을 생성.
  - "Added"와 "none"은 개성 문장의 추가 여부를 나타냄.

---

# 4.2 Model Setup

- 모델 비교:
  - 프롬프트 튜닝 모델과 파인튜닝 비교.
  - 데이터셋: 섹션 4.1의 데이터셋 사용.
  - 사용 모델: 
    - GPT2-XL (1.5B 파라미터)
    - GPTJ-6B (Wang and Komatsuzaki, 2021).
  - GPT-J-6B 모델은 GPU 메모리 부족으로 파인튜닝 미실험.

- 하이퍼파라미터 설정:
  - 프롬프트 튜닝의 하이퍼파라미터는 Lester et al. (2021)의 설정에 기반.
  - 퍼소나 정보 토큰 길이는 200으로 설정, 초기 실험 결과 반영.
  
- 응답 문장 생성 전략:
  - 탐욕 검색(greedy search) 전략 사용.

- 에폭 수 설정:
  - 학습 중 손실이 수렴할 때까지 에폭 수 설정.

- 파인튜닝 실험:
  - 두 가지 방법 실험:
    1. 대화 쌍만 입력.
    2. 대화 쌍 발언 전에 퍼소나 문장 추가 후 모델에 입력.

- 기타 하이퍼파라미터 값은 부록 B에 제공.

---

# 4.3 Results

- 평가 데이터셋에서 대화 쌍의 발화를 모델에 입력함.
- 생성된 응답의 다양성을 자동으로 평가함.
- 응답이 자연스러운지, 그리고 개인화된 정보(pesona)를 기반으로 하는지를 수동으로 평가함.

---

# 4.3.1 Automatic Evaluation

- 생성된 응답의 다양성을 평가하기 위해 distinct-N 방법(Li et al., 2016a)을 사용함.
- distinct-1 및 distinct-2 값은 Table 1에 제시됨.
  - 각 모델의 결과는 세 가지 유형의 페르소나에 대한 일반 평가 데이터셋으로부터 수집됨.
- 결과:
  - prompt-tuning으로 훈련된 GPT-J-6B 모델이 가장 높은 점수(98)를 기록함.

### Table 2 요약
- 생성된 응답은 유창성, 몰입도, 적합성에 대해 5점 척도로 평가됨.
- 총 5명의 작업자가 각각의 질문에 대해 답변, 평균 및 표준편차 제공.
- prompt-tuning된 GPT-J-6B 모델이 페르소나 평가 데이터셋에서 모든 분야에서 최고 점수 기록.
- 일반 평가 데이터셋에서는 유의미한 차이 없음.

### Table 3 요약
- 생성된 응답의 페르소나 고려 점수 분포를 5점 척도로 평가함.
  - 1은 페르소나와 일치하지 않음, 3은 페르소나와 관련 없음, 5는 페르소나에 부합함.
  - 각 설정에서 페르소나 평가 데이터셋 샘플 수는 50, 일반 평가 데이터셋 샘플 수는 150임.

- 추가 발견:
  - fine-tuning 과정에서 페르소나 문장을 입력에 추가하지 않을 때 결과가 더 좋음을 발견함.
  - Zhang et al. (2018)의 seq2seq 모델 실험 결과와 유사한 결과임.

---

# 4.3.2 Manual Evaluation

- **평가 플랫폼**  
  - Amazon Mechanical Turk 사용
  - Zhang 외 (2018)의 방법론 따름

- **평가 항목**  
  - 유창성, 몰입도, 적절성, 페르소나 고려
  - 5점 척도로 평가

- **평가 대상 및 샘플 수**  
  - 페르소나 평가 데이터셋에서 50개, 일반 평가 데이터셋에서 150개

- **실험 설계**  
  - 작업 예시는 부록 C에 제시
  - 인간 평가 점수는 Zhang 외 (2018) 실험에서 인용

- **모델 성능**  
  - 페르소나 문장을 입력할 경우, 자동 평가 결과가 낮아지므로 페르소나 문장 없이 평가
  - prompt-tuned GPT-J-6B 모델이 모든 평가 항목에서 최고 점수 기록

- **결론**  
  - 모델 크기가 클수록 더 많은 지식 저장 및 자연스러운 응답 생성
  
- **일반 평가 데이터셋 결과**  
  - 큰 차이는 없지만, 일반적인 짧은 문장으로 이루어져 있어 간단한 응답 대부분
 
- **페르소나 고려 결과**  
  - 평가 결과 분포: 1(불일치)에서 5(일치)까지
  - 생성된 응답의 평균 점수는 페르소나 기반 응답 대부분이 동응일함을 나타냄

- **모델 훈련 방법 비교**  
  - prompt-tuning이 자연스러운 응답 생성을 더 잘함

- **응답 예시**  
  - 탁월한 점수를 받은 prompt-tuned GPT-J-6B 모델의 생성된 응답 예시 제시
  - 페르소나 기반과 비기반 응답 모두 나타남

- **훈련 데이터셋**  
  - 소규모 데이터셋에서도 자연스럽고 일관된 개성을 생성하는 응답 가능함.

---

# 4.4 Experiments in Japanese

- 일본어 실험을 위해 두 개의 데이터셋 사용:
  - JPersonaChat
  - JEmpatheticDialogues (Sugiyama et al., 2021)
  
- 영어 실험과 유사하게 세 개의 페르소나 사용:
  - JPersonaChat의 대화 쌍 수: 527, 525, 525
  
- 훈련 데이터셋 생성 과정은 영어 실험과 동일함:
  - JEmpatheticDialogues의 발화가 더 짧고 일반적이어서, 추가 발화 조건 없음
  
- 훈련 데이터셋 비율은 1:10으로 설정:
  - 초기 실험 결과 기반
  
- 모델:
  - GPT2-XL5 (1.3B 매개변수)
  - HyperCLOV A (Kim et al., 2021; 6.9B 매개변수)
  
- 자동 평가 결과 (표 6):
  - HyperCLOV A는 매개변수가 많음에도 불구하고 낮은 점수 경향
  - 응답이 백채널링으로 시작하는 경우가 많음
  
- 수동 평가 결과 평균 점수 (표 7):
  - Persona eval 데이터셋 및 일반 eval 데이터셋 모두에서 HyperCLOV A 모델이 가장 높은 점수 기록
  
- 페르소나 고려 사항 분포 (표 8):
  - 영어 실험과 유사하게 많은 응답이 페르소나를 기반으로 함
  - 페르소나와 불일치하는 응답은 적음
  
- 생성된 응답 예시는 부록 A에 제공됨

---

# 5 Conclusion

- **제안된 방법**: 단일 페르소나를 기반으로 하는 대화 데이터를 이용한 사전 훈련된 언어 모델의 프롬프트 튜닝 방법.
  
- **자동 및 수동 평가 결과**:
  - 대화 시스템이 더 자연스럽고 개인화된 응답을 생성할 수 있음.
  - 재조정(finetuning)보다 적은 계산 자원으로도 가능.

- **언어 비교**:
  - 일본어로 생성된 응답이 영어보다 자연스러움.
  
- **평가 결과**:
  - 프롬프트 튜닝을 통한 HyperCLOV A 모델이 모든 평가 항목에서 가장 높은 점수.
  - 평가 항목: 유창성, 흥미도, 관련성.

- **미래 활용 방향**:
  - 대화 시스템에 개인성을 추가하는 데 사용할 수 있음.
  - 감정을 표현하는 응답 생성을 위한 프롬프트 설계 가능성.

---
