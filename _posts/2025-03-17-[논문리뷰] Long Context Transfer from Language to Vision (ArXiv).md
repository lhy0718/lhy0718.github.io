---
title: "[논문리뷰] Long Context Transfer from Language to Vision (ArXiv)"
date: 2025-03-17 00:00:00 +0900
categories:
  - Paper Review
tags:
  - ArXiv
  - Large Multimodal Models
---

요약: 이 논문은 언어 모델의 컨텍스트 길이를 확장하여 기존의 대형 멀티모달 모델이 긴 비디오를 이해할 수 있도록 하는 방법을 제안하며, 이를 통해 LongVA라는 새로운 비디오 어시스턴트를 개발하여 2000프레임 이상의 비주얼 토큰을 처리할 수 있음을 보여줍니다.

---

# 1 Introduction

- 대형 언어 모델(LLMs)의 발전에 따라, 이미지와 비디오를 이해하는 능력을 확장하기 위한 여러 연구가 진행됨.
- 다중 모달 모델(LMMs)은 캡셔닝과 시각적 질문-응답과 같은 뛰어난 능력을 보여줌.
- 현재 LMMs는 단일 이미지와 짧은 비디오 작업에서 유망한 성능을 발휘하지만, 매우 긴 비디오를 효과적으로 처리하고 이해하는 것은 큰 도전 과제임.
- 이 도전의 주요 이유는 비전 인코더가 생성하는 시각적 토큰의 과도한 수치 때문임.
  - 예를 들어, LLaVA-1.6는 단일 이미지에 대해 576에서 2880개의 시각적 토큰을 생성함.
- 시각적 토큰 수를 줄이기 위한 많은 방법이 제안되었고, 주로 비전 인코더와 LLM을 연결하는 비주얼 리샘플러를 조정하여 더 적은 토큰을 추출하려고 함.
- 현존하는 고성능의 긴 비디오 LMMs의 개발을 저해하는 또 다른 문제는 고품질 긴 비디오 데이터 세트의 부족임.
- 기존 데이터 세트는 대부분 1분 이내의 비디오 클립으로 구성되어 있으며, 긴 비디오가 포함된 데이터 세트도 몇 개의 프레임에 대해서만 주석이 달려있어 긴 감독 신호가 부족함.
  
- 본 논문에서는 시각적 토큰을 줄이는 대신, 기존 LMMs에서 시각적 맥락 길이를 제한하는 보다 중요한 문제를 식별함:
  - 언어 모델 백본의 맥락 길이.
- 언어 모델의 맥락 길이를 연장하기 위해 더 긴 텍스트 데이터로 훈련함.
- 이러한 과정을 통해 언어 모델의 맥락 길이가 LMMs로 직접 전이됨.
- UniRes라는 통합 인코딩 방식 제안하여, 비디오를 확장된 이미지로 표현하고 이미지와 비디오 간의 역량 융합을 강화함.
  
- V-NIAH라는 합성 비주얼 벤치마크를 생성하여, 긴 맥락에서 시각적 정보를 찾고 검색하는 LMMs의 능력을 평가함.
- Long Video Assistant는 2000 프레임 또는 200K 이상의 시각적 토큰에서 시각적 정보를 정확하게 검색할 수 있는 능력을 보임.
- 실험 결과 추가적인 프레임이 긴 비디오 질문-응답 벤치마크의 성능을 향상시키는 데 기여함.
- LongVA는 Video-MME 및 MLVU 데이터 세트에서 7B 모델 중에서 최첨단 성능을 달성함.

- 요약:
  1. 긴 맥락 전이: 언어 모델의 맥락이 다중 모달 모델에 직접 전이되는 현상을 발견.
  2. V-NIAH: LMMs의 시각적 정보 위치 찾기 및 검색 능력을 시험하기 위한 벤치마크.
  3. LongVA: 긴 맥락 전이와 UniRes를 결합하여 200K 이상의 시각적 토큰을 인식할 수 있는 모델 개발.

---

# 2 Related Work

- **비전 언어 커넥터 및 대규모 다중모드 모델**
  - 기존 연구들은 LLMs(대규모 언어 모델)에 시각적 특징을 추출하고 주입하기 위한 다양한 아키텍처를 탐구함.
  - **Flamingo** [2]에 의해 시작된 한 작업은 영상 기능을 압축하기 위해 리샘플러를 채택하고 LLM에 크로스 게이티드 어텐션 레이어를 삽입함.
  - 다른 연구들은 리샘플러를 사용하면서 이미지 특징을 언어 모델의 입력 레이어에 직접 공급함.
  - **LLaVA 시리즈** [47, 46, 48]는 풀링이나 리샘플링 없이 이미지 특징을 언어 모델로 직접 투영하는 간단하고 확장 가능한 디자인 사용.
  - 이미지 전용 모델에서 다중 이미지 및 비디오 입력을 포함하는 방향으로 발전하면서 시각 언어 커넥터에 대한 더 많은 수정안이 제안됨.
  - [89]와 [9]는 간단한 평균 풀링을 사용하고, [31]은 시각 토큰을 동적으로 버리며, [15]는 비디오 데이터의 동적 특성을 더 잘 포착하기 위해 공간-시간 합성을 채택함.

- **Transformer에서의 맥락 외삽**
  - Transformer는 훈련 길이보다 긴 시퀀스에서 직접 작동하지 않으며, 이를 완화하기 위해 다양한 RoPE 기반의 확장 기법이 제안됨.
  - 긴 맥락 훈련 동안 데이터 관리와 시스템 최적화에 대한 노력도 이루어짐.
  - LMM(대규모 다중모델) 도메인에서의 맥락 외삽 탐사는 제한적이며, [44]는 LMM을 긴 맥락 언어 모델로 훈련하지만 시각적 맥락 길이에 대한 benchmark는 없음.

- **비디오 언어 벤치마크**
  - 최근 몇 년 동안 비디오 질문-답변 분야에서 큰 발전이 있었음.
  - 비디오 LMM의 성능을 정확히 측정하기 위해 연구자들은 다양한 벤치마크 개발, 기본적인 시각 인식 작업의 전반적인 스펙트럼 포함.
  - 대부분의 벤치마크는 짧은 비디오에 집중하고, 긴 맥락에 대한 LMM의 능력을 평가할 데이터와 메트릭이 부족함.
  - **V-NIAH**는 NIAH 테스트 [23]에 영감을 받아 LMM의 긴 시각 입력에서의 능력을 평가하기 위해 제안됨.
  - 여러 동시 작업이 Needle-in-a-haystack 테스트의 다중모달 버전을 개발하였으나, 몇 백 프레임에서만 측정하고 시각적 맥락 길이에 대한 강력한 기준 없이 분석에 한계가 있음.

---

# 3 Long Video Assistant

<img width="715" alt="image" src="https://github.com/user-attachments/assets/dfb9a215-1e42-4395-b0c0-5d614df39301" />

- **논문 가설**: 비전과 언어의 모달리티가 진정으로 정렬되면, 긴 컨텍스트를 처리하는 능력이 텍스트에서 비전으로 전이될 수 있다.
- **방법론 개요**: 
  - 언어 모델을 기반으로 긴 컨텍스트 훈련 후, 짧은 이미지 데이터로 비주얼 능력 강화.
  
## 3.1 긴 언어 모델 훈련
- **모델 선택**: Qwen2-7B-Instruct 사용.
- **훈련 세부사항**:
  - 컨텍스트 길이: 224K, 총 900M 토큰 사용.
  - RoPE 기본 주파수 설정 및 고정 학습률 1e-5 유지.
  - 데이터셋은 Slimpajama에서 문서 길이를 늘려 구성.
  - 여러 문서를 BOS 토큰으로 구분하여 하나의 시퀀스로 묶음.
  
- **최적화 전략**:
  - FlashAttention-2, 순환 주의(Ring Attention), 활성화 체크포인트, 파라미터 오프로드 등 사용.
  - GPU 부하를 균형 있게 분산하기 위해 지그재그 방식으로 시퀀스 나누기.
  - Qwen2로 최대 1224K 시퀀스 길이 가능.
  
- **성능 평가**:
  - NIAH 테스트에서 훈련 컨텍스트 길이(224K) 내에서 완벽한 결과 달성.
  
## 3.2 짧은 비주얼 데이터로 긴 언어 모델 조정

<img width="715" alt="image" src="https://github.com/user-attachments/assets/3d84821b-9fcd-4e70-98e7-6b8486b3555c" />

- **기반 아이디어**: LLaVA-NeXT의 AnyRes 인코딩 방식 영감.
- **UniRes 설계**:
  - 이미지와 비디오에 대해 통합 인코딩 방식 제공.
  - 고해상도 이미지를 작은 그리드로 나누고, 각 그리드는 336 × 336 픽셀로 설정.
  
- **비디오 표현**:
  - 비디오는 각 프레임을 그리드로 취급하여 인코딩.
  - N프레임 비디오를 144N 개의 비주얼 토큰으로 인코딩.
  
- **실험 설계**:
  - 짧은 훈련, 긴 테스트 프로토콜 도입.
  - UniRes는 AnyRes에 비해 영상 벤치마크에서 약간 낮은 점수를 보이나 V-NIAH 및 Video-MME에서 더 우수한 성능.
  
- **총 훈련 시간**: 이미지-텍스트 정렬 및 긴 컨텍스트 훈련 포함, 3.5일 소요.
- **설계 영감**: 이전 작업에서 영감을받아 LongVA의 설계 선택을 진행.

---

# 4 V-NIAH

<img width="715" alt="image" src="https://github.com/user-attachments/assets/a02842ff-302f-4faa-9088-24f74fa0e97c" />

- **컨텍스트 길이 측정**: 기존의 언어 모델 컨텍스트 길이 측정 방법인 언어 모델의 혼란도(perplexity) 점수를 긴 문서에 대해 계산.
- **NIAH 테스트**: 최근 LLM의 긴 컨텍스트 정보를 정확히 검색하는 능력을 평가하기 위해 Needle-in-a-Haystack(NIAH) 테스트 사용.
- **시각적 컨텍스트**: 현재 LMM의 시각적 컨텍스트 길이를 측정하는 벤치마크가 없음.
- **V-NIAH 제안**: LongVA의 긴 범위 시각 정보를 검색할 수 있는 능력을 평가하기 위해 NIAH 테스트를 텍스트에서 비디오로 확장.
- **비디오 질문-답변 문제**: 
  - 5개의 비디오 질문-답변 문제를 설계하여 각각을 수시간 분량의 비디오에 프레임으로 삽입.
  - 비디오는 1 FPS로 샘플링됨.
  - "니들(needle)" 이미지는 기존 VQA 벤치마크 또는 AI 생성 이미지 사용하여 오염 방지.
  - 질문에 "위치 식별 프롬프트" 포함, 이를 통해 시스템이나 사람이 비디오에서 니들 프레임을 찾을 수 있도록 함.
- **처리 난이도**: LongVA를 3000 프레임의 시각적 입력으로 테스트할 때, 200K 토큰 입력 처리에 100GB GPU 메모리 필요.
- **메모리와 배치 크기 문제**: vLLM 같은 고급 LM 서비스 시스템을 사용하더라도 샘플링 과정이 느림.
- **혼란도 기반 평가**: 모델 출력의 정 correctness를 측정하기 위해 "혼란도 기반" 평가 사용.
  - 모든 프레임을 인코딩하고 해당 시각적 임베딩을 저장.
  - 평가 시 LangV A의 언어 모델만 로드하고 시각적 임베딩, 질문 토큰 및 답변 토큰을 단일 포워드 패스에 연결.
  - 이 방법으로 KV 상태 캐시 없이 오버헤드 감소.
- **모델 정답 기준**: 모델의 출력은 답변 스팬의 모든 토큰 중 최고 출력 로짓 인덱스가 정답과 동일할 경우에만 올바른 것으로 간주됨.
- **질문 예시**: "결혼식에서 커플의 프레임을 찾아라. 프레임 안에 신랑 머리 위 풍선이 있다. 그 풍선 색상은?" - 답변: 노란색.

---

# 5 Experiments

<img width="715" alt="image" src="https://github.com/user-attachments/assets/48e7282d-89ed-424f-b6fd-fbb0ee38a22e" />

- LongVA의 장기 비주얼 능력을 두 가지 벤치마크인 V-NIAH와 Video-MME에서 평가
  - V-NIAH: 정보 검색 능력을 테스트하지만 다른 실제 비디오 보조 기능은 포함하지 않음
  - Video-MME: 다양한 데이터 유형과 질적 주석으로 구성된 종합 평가 세트. 평균 비디오 길이는 1017초
  - MLVU 기준 결과는 부록 C에 포함됨

- LongVA와 다른 이미지 및 비디오 LMMs 비교
  - LLaVA-Next-Qwen2: Qwen2-7B-Instruct를 기반으로 한 기준 모델
  - LongVA (AnyRes): UniRes 인코딩 기법의 장점 보여줌
  - LongVA와 기준 모델 간의 차이는 Table 5에서 확인 가능

- V-NIAH 결과
  - 장기 맥락 전이의 현상 확인
  - LLaVA-NeXT-Video-32K의 비주얼 컨텍스트 길이는 32K로 제한되어 약 200 프레임에 해당
  - LongVA는 224K 컨텍스트 길이로 훈련되었으나 3000 프레임에서도 만족스러운 성능 유지

- 통합 인코딩이 비주얼 컨텍스트 외삽에 미치는 효과 검토
  - LongVA는 UniRes로 훈련되었을 때 더 나은 검색 능력 발휘
  - UniRes의 통합 표현이 언어에서 비전으로의 장기 맥락 전이를 향상시킴

- 비디오 평가에서 LongVA의 성능
  - 10B 매개변수 미만의 LMM들 중에서 최첨단 성능 달성
  - 비디오 데이터 없이 훈련되어 성능은 제로-샷으로 간주됨
  - 샘플링된 프레임 수가 증가함에 따라 긴 서브셋에서 성능 개선

- 짧은 비디오 벤치마크에 대한 평가
  - LongVA는 NeXTQA와 ActivityNetQA에서 높은 점수 기록
  - Short duration datasets에서 성능 향상을 위한 Direct Preference Optimization(DPO) 사용 시 큰 개선 관찰

- 이미지 평가 결과
  - LongVA의 이미지 성능 검토: 다양한 이미지 벤치마크에서 평가
  - UniRes를 사용한 LongVA는 AAAI에서 높은 성능 발휘

- 질적 결과
  - LongVA-DPO는 짧고 긴 비디오에 대한 이해 능력을 입증하기 위한 사례 제시
  - 특정 비디오 세부 사항을 정확히 식별하고 해석하는 능력 강조

- 결론
  - LongVA는 장기 비디오 이해의 도전에 대한 해결책 제시
  - 장기 맥락 전이 현상을 통해 비디오 처리 능력을 대폭 개선
  - V-NIAH라는 새로운 벤치마크 소개

---

# 6 Conclusion

<img width="608" alt="image" src="https://github.com/user-attachments/assets/48cadc20-3ee3-4261-b6f0-0557c8180d36" />

- 본 연구는 대형 다중모달 모델에서 긴 비디오 이해의 어려움을 해결함.
- 텍스트에서 언어 모델을 확장한 후 이를 시각 입력과 정렬하여 긴 비디오를 처리하는 능력을 크게 향상시킴.
- LongVA 모델은 입력 프레임 수가 많아질수록 성능이 향상되고, Video-MME에서 최첨단 성능을 기록.
- V-NIAH라는 합성 벤치마크를 도입하여 비디오 LMM의 시각적 맥락 길이를 측정함.
- 이 연구는 긴 비디오 LMM 및 다중모달 에이전트 분야에 대한 추가 연구를 촉진하기를 희망함.
