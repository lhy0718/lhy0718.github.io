---
title: "ONNX Tutorial"
date: 2021-10-21T15:34:30+09:00
categories:
  - machine learning
tags:
  - machine learning
  - deep learning
  - onnx
  - pytorch
  - tensorflow
  - huggingface
---

- ë³¸ íŠœí† ë¦¬ì–¼ì€ pytorchì™€ huggingface modelì— ì´ˆì ì´ ë§ì¶”ì–´ì ¸ ìˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.

## ONNX ê°œìš”

- ONNX [Ëˆo:nÊks] - Open Neural Network eXchange
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í‘œí˜„í•˜ëŠ” ì˜¤í”ˆ í‘œì¤€ í¬ë§·
- ì œê³µ ê¸°ëŠ¥
  - Build model / Export to ONNX format
    - **Tensorflow, Pytorch, Scikit Learn** ë“± 20ì—¬ ê°€ì§€ì˜ Framework & Converter ì§€ì›
    - Pre-Trained model (**ONNX Model Zoo**) ì œê³µ
  - Deploy model
    - Intel, Qualcomm, Windows, Nvidia, TensorFlow ë“±ì˜ ë‹¤ì–‘í•œ í”Œë«í¼ ì§€ì›
  - [ONNX Optimizer](https://github.com/onnx/optimizer)
  - Model Visualize - [Netron](https://github.com/lutzroeder/Netron)

## ONNX Runtime

- ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¶”ë¡  ë° í•™ìŠµì„ ìµœì í™”í•˜ëŠ” ê¸°ëŠ¥

## ì‚¬ìš© ì¤€ë¹„

- prerequisite : pytorch (or tensorflow)
- Install ONNX and ONNX runtime.

```sh
conda install -c conda-forge onnx # no need when using pytorch.
pip install onnxruntime # when using CPU
pip install onnxruntime-gpu # when using GPU
```

## ONNX ë³€í™˜ íŠœí† ë¦¬ì–¼ (PyTorch model)

### ê°œìš”

HuggingFaceì—ì„œ ì œê³µí•˜ëŠ” PyTorch ê¸°ë°˜ BERT ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

### transformers.onnx ì„ ì‚¬ìš©í•œ ë³€í™˜

- hub ë˜ëŠ” local pathì— ìˆëŠ” ëª¨ë¸ì„ ONNX graphë¡œ ë³€í™˜í•œë‹¤.
- ì˜ˆì‹œ: bert-base-cased ëª¨ë¸ì„ onnx/bert-base-cased/ ìœ„ì¹˜ì— ë³€í™˜

```sh
python -m transformers.onnx --model=bert-base-cased onnx/bert-base-cased/
```

- `transformers.onnx` ì˜ ë§¤ê°œë³€ìˆ˜ ëª©ë¡

```txt
usage: Hugging Face ONNX Exporter tool [-h] -m MODEL -f {pytorch} [--features {default}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating where to store generated ONNX model.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Model's name of path on disk to load.
  --features {default}  Export the model with some additional features.
  --opset OPSET         ONNX opset version to export the model with (default 12).
  --atol ATOL           Absolute difference tolerance when validating the model.
```

### Python API ë¥¼ ì‚¬ìš©í•œ ë³€í™˜

```py
import torch

torch.onnx.export(model,
                  dummy_input,
                  "onnx/bert-base-cased/model.onnx",
                  input_names=input_names,
                  output_names=output_names,
                  dynamic_axes=dynamic_axes)
```

#### ë§¤ê°œë³€ìˆ˜ ì„¤ëª…

- model: exportí•  pytorch model
- dummy_input: modelì— ë“¤ì–´ê°€ëŠ” ë‹¨ì¼ input ë˜ëŠ” íŠœí”Œ í˜•íƒœì˜ ë‹¤ì¤‘ input
  - **ë‹¤ì¤‘ input ì´ë¼ë©´ íŠœí”Œ ì†ì— PreTrainedModel.forward()ì— ë“¤ì–´ê°€ëŠ” ì¸ìˆ˜ì˜ ìˆœì„œëŒ€ë¡œ ë„£ì–´ì£¼ì–´ì•¼ í•¨**
- ëª¨ë¸ì„ exportí•  ê²½ë¡œ (type : str)
- input_names: nputìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë³€ìˆ˜ì˜ ì´ë¦„ë“¤ (type : list[str])
- output_names: outputìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ë³€ìˆ˜ì˜ ì´ë¦„ë“¤ (type : list[str])
- dynamic_axes: ê°€ë³€ ê¸¸ì´ì˜ input/output ì°¨ì›ì´ ìˆì„ ë•Œ ì•Œë ¤ì£¼ëŠ” ë³€ìˆ˜ (type : dict[str, dict[int, str]])

### ONNX ëª¨ë¸ì„ ONNX Runtimeì—ì„œ ì‚¬ìš©í•˜ê¸°

```py
import onnxruntime as ort

from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased")

ort_session = ort.InferenceSession("onnx/bert-base-cased/model.onnx")

inputs = tokenizer("Using BERT in ONNX!", return_tensors="np")
outputs = ort_session.run(["last_hidden_state", "pooler_output"], dict(inputs))
```

`ort_session.run`ì— ë“¤ì–´ê°€ëŠ” output keyë“¤ì€ ëª¨ë¸ì˜ onnx configë¥¼ í†µí•´ í™•ì¸í•˜ê±°ë‚˜, ì§ì ‘ inputì„ modelì— ë„£ì–´ì„œ ì–´ë–¤ ê°’ì´ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•˜ë©´ ëœë‹¤. ğŸ‘‡

```py
from transformers.models.bert import BertOnnxConfig, BertConfig

config = BertConfig()
onnx_config = BertOnnxConfig(config)
output_keys = list(onnx_config.outputs.keys())
```

### ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•„í‚¤í…ì³ì— ëŒ€í•œ custom config

- custom configë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
- ë‹¨ `inputs`, `outputs` propertyëŠ” `OrderedDict`ì´ë¯€ë¡œ ìˆœì„œë¥¼ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
  - `inputs`ëŠ” `PreTrainedModel.forward()`ì˜ í”„ë¡œí† íƒ€ì…ê³¼,<br>`outputs`ëŠ” `BaseModelOutputX` ì¸ìŠ¤í„´ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ ìœ„ì¹˜ê°€ ë§ì•„ì•¼ í•œë‹¤.

```py
class CustomOnnxConfig(OnnxConfig):
    @property
    def inputs(self) -> Mapping[str, Mapping[int, str]]:
        return OrderedDict([
          ("input_ids", {0: "batch", 1: "sequence"}),
          ("attention_mask", {0: "batch", 1: "sequence"}),
          ("token_type_ids", {0: "batch", 1: "sequence"}),
        ])

    @property
    def outputs(self) -> Mapping[str, Mapping[int, str]]:
        return OrderedDict([
          ("last_hidden_state", {0: "batch", 1: "sequence"}),
          ("pooler_output", {0: "batch"})
        ])
```

### PyTorch â†’ ONNX ë³€í™˜ ì˜ˆì‹œ ì½”ë“œ

<https://github.com/lhy0718/huggingface-study/blob/main/onnx.ipynb>

## references

> - <https://huggingface.co/transformers/serialization.html>
