---
title: "[논문리뷰] ESC-Eval- Evaluating Emotion Support Conversations in Large Language Models (EMNLP 2024)"
date: 2025-03-12 15:00:00 +0900
categories:
  - Paper Review
tags:
  - EMNLP 2024
  - Empathetic Dialogue Systems
---

요약: 감정 지원 대화(ESC)는 인간의 스트레스를 줄이고 정서적 지도를 제공하여 정신적 및 신체적 웰빙을 향상시키는 데 중요한 응용 프로그램이다. 본 연구에서는 역할 놀이 에이전트를 활용하여 ESC 모델의 평가 프레임워크인 ESC-Eval을 제안하고, 이를 통해 여러 LLM의 ESC 능력을 평가했다.

---

# 1 Introduction

- 대규모 언어 모델(LLM)의 빠른 발전으로 많은 개인이 LLM과 대화에 참여하고 있음.
- 감정 지원 대화(ESC)는 개인이 자신의 경험이나 고민을 나누고 감정적 지원 및 실질적인 조언을 받는 분야로 주목받고 있음.
- 이러한 상호작용은 인간의 스트레스를 줄이고 전반적인 웰빙을 개선하는데 도움을 줌.

- 현재 ESC 모델의 효과적이고 포괄적인 평가가 어려움:
  - (1) 텍스트 기반 통계적 지표 사용:
    - 모델의 응답이 실제 데이터와 유사한지 평가하지만, 주관적인 한계가 있음.
  - (2) 수동 평가:
    - 인간 평가자가 모델과 사용자 간의 대화를 시뮬레이션하여 평가하나, 비용과 효율성 문제 발생.

- ESC-Eval 제안:
  - 인간의 노동을 역할 놀이 모델로 대체하여 ESC 모델의 평가 효율성을 높임.
  - 평가를 위한 역할 카드를 다양하게 구성하고, 실제 인간 행동을 반영하는 역할 놀이 에이전트를 개발하여 데이터 수집.

- 주요 목표:
  - 다양한 역할 카드를 이용하여 모델의 전반적인 평가 확보.
  - 실제 인간 상호작용을 반영하여 평가 결과의 객관성과 공정성 보장.

- ESC-Eval의 개발 및 결과:
  - 14개의 LLM을 평가하고, 59,654개의 수동 평가 결과를 수집하여 다양한 차원에서 분석.
  - ESC 모델은 일반적인 AI 모델보다 뛰어난 성능이나, 감정 지원 지식과 인간 선호에 대한 취약함 발견.

- ESC-RANK 개발:
  - 미래의 ESC 모델을 위한 자동 평가 시스템으로, GPT-4보다 35점 높은 정확도를 달성함.

- 주요 기여:
  - ESC 모델을 역할 놀이를 통해 평가하는 첫 번째 프레임워크 제안.
  - 14개 LLM을 테스트하고, 정밀하게 설계된 차원에 따라 평가 결과를 수동으로 주석 처리.
  - ESC 모델의 감정 지원 지식과 인간 선호의 우수성을 요구하는 필요성이 강조됨.

---

# 2 ESC-Eval

<img width="801" alt="image" src="https://github.com/user-attachments/assets/2531bd00-1d4c-4541-993c-8319c6691439" />

<img width="801" alt="image" src="https://github.com/user-attachments/assets/41c4d85c-5744-4b18-8403-b3948b9b7b34" />

- ESC-Eval은 다양한 평가기준을 사용하여 언어 모델의 성능을 측정하는 방법론이다.
- 여러 평가 지표들은 모델의 질과 사용자의 요구를 반영하기 위해 설계되었다.
- 평가의 주요 항목들:
  - 정확성 (Accuracy): 모델의 정답성과 관련된 지표.
  - 창의성 (Creativity): 응답의 독창성과 다양성.
  - 관련성 (Relevance): 질문과 응답 간의 적합성.
- 수식적 표현:
  - 정확도를 수식으로 나타낼 때 $$Accuracy = \frac{TP}{TP + FP}$$ 형태로 표현.
    - 여기서 $TP$는 참 긍정의 수, $FP$는 거짓 긍정의 수를 의미.
- 평가 기준으로는 머신 러닝 모델의 훈련 및 테스트 데이터에 대한 성능을 반영.
- ESC-Eval의 목표는 언어 모델이 실제 환경에서 얼마나 효과적인지를 평가하는 것.

---

# 2.1 Framework Overview

- ESC-Eval의 워크플로우는 그림 2에 설명되어 있음
- ESC-Eval은 평가하고 있는 ESC 모델과 상호작용하기 위해 역할 놀이 모델과 역할 카드 집합을 사용함
- 얻어진 대화 데이터는 이후 수동으로 주석이 달림
- ESC-Eval에서는 다양한 역할 카드의 풍부한 수와 더 신뢰할 수 있는 역할 놀이 에이전트의 가용성이 매우 중요함
- 다음 섹션에서는 이 두 가지 주요 기본 구성 요소의 신뢰성을 보장하기 위해 취한 조치를 설명할 것임

---

# 2.2 Role Card Acquisition

- 캐릭터 카드의 다양성을 보장하기 위해 다음을 참조하여 분류 시스템을 구축:
  - ESConv (Liu et al., 2021)
  - ExTES (Zheng et al., 2023b)
  - Life Events Scale (Wethington, 2016)
  
- 세 가지 계층으로 구성된 분류 시스템을 생성하며 37개 범주 포함.
  
- 역할 카드를 재구성하고 각 범주 내에서 식별하는 절차 제안.
  
- 이 절차는 세 가지 주요 단계로 구성:
  1. **데이터 수집**: 7개의 오픈 소스 데이터셋 수집, 다양한 잠재 사용자 역할 포함.
  2. **역할 추출 및 필터링**:
     - GPT-4를 사용하여 데이터셋에서 역할 추출.
     - 저품질 역할 카드 필터링 후 인간 필터링 진행.
  3. **수동 주석 및 분류**:
     - 역할 카드의 품질 보장을 위한 수동 주석 프로세스 수행.
     - 역할 카드를 각각의 3차 범주로 분류.

- 각 단계는 다음에서 자세히 소개되며, 추가 세부정보는 부록 A에서 확인 가능.

---

# 2.2.1 Dataset collection

- 다양한 캐릭터 카드 세트를 얻기 위해, 
  - 감정 지원 및 정신 건강 관련 기존 데이터셋에 대한 종합적인 조사를 실시함.
  
- 조사 결과, 
  - 이 연구의 원천 데이터셋으로 7개의 데이터셋을 선택함.
  
- 사용된 오픈소스 데이터셋 목록은 
  - 부록 A에 기재되어 있음.

---

# 2.2.2 User cards extraction and filtering

- 다양한 데이터셋에서 사용자 프로필을 추출하기 위해 Multi-turn Dialogue (MD) 데이터셋과 단일 턴 질문-응답 (QA) 데이터셋을 사용.
- 초기 추출에는 GPT-4를 활용함.
- 이 과정에서 발생한 비용: 약 $120.
- 초기 캐릭터 카드를 얻은 후, GPT-4를 사용하여 역할 카드에 대한 초기 필터링 과정 수행.
  - 이 과정에서는 감정만 포함되며 관련 사건이 없는 역할 카드를 제거.
  - 발생한 비용: 약 $70.
- 이 섹션에서 사용된 프롬프트는 부록 A에 포함됨.
- GPT-4의 필터링 과정 이후, 카드의 품질을 보장하기 위해 인간 필터를 적용.

---

# 2.2.3 Manual annotation and correction

- GPT-4와 인간 필터를 거친 역할 카드 확보 후, 
  - 2단계 접근법을 사용하여 크라우드소싱 주석 및 수동 수정 과정 진행
- **크라우드 작업자 주석** 
  - 37개의 실제 질문 카테고리 포함된 3단계 분류 시스템 개발 (표 9 참조)
  - 필터링된 캐릭터 카드를 해당 3차 분류로 주석 달기
  - 역할 놀이의 질과 편리한 평가 필요에 기반
  - 고품질 및 중품질 캐릭터 카드 라벨링 요구
  - 주석 규칙 및 분류 시스템은 부록 A에서 확인 가능
  - 주석 단계 예상 소요 시간: 약 10일, 10명의 크라우드소싱 작업자 필요
- **인간 수정**
  - 첫 번째 단계 크라우드소싱 주석 완료 후 수동 수정 진행
  - 논문의 저자들이 각 역할 카드의 주석 확인 및 수정
  - 잘못된 분류 및 역할 카드 품질 관련 문제 해결
- 2단계 크라우드소싱 주석 및 수동 수정 과정을 거쳐 
  - 다양한 문제를 가진 실제 인물의 역할 카드 성공적으로 재구성
- 역할 카드의 데이터 분석 내용은 부록 A에 기재됨

---

# 2.3 ESC-Role

- ESC-Role은 보다 견고한 역할 기반 모델을 구축하기 위해 설계됨.
- ESC-Role은 일반 데이터와 ESC 시나리오에 특화된 데이터 모두를 사용하여 훈련됨.
- 주요 목적은 ESC-Eval을 위한 특정 역할 수행 에이전트를 훈련하는 것임.
- 모델 훈련 및 평가에 관련된 단계들이 이어지는 섹션에서 설명될 예정임.

---

# 2.3.1 Data Collection

- 이전 섹션 2.2와 동일한 절차를 사용하여 Smile, ESConv, ExTES 데이터셋을 선택
- ESC 시나리오 데이터를 수집하기 위해 다음 방법 사용:
  - GPT-4를 통한 추출
  - GPT-4로 필터링
  - 수동 필터링
- 총 3,390개의 역할 카드 및 해당 대화로 구성된 역할 연기 데이터 수집
- 역할 카드는 모델 훈련을 위한 시스템 프롬프트로 사용됨
- 모델의 강력한 역할 연기 능력을 강화하기 위해 Huggingface에서 다중 턴 대화 데이터로 구성된 다섯 개의 역할 연기 데이터셋 필터링
- 처리 후 14,000개의 역할 연기 데이터 인스턴스 확보
  - 일반 역할 연기 지침 데이터와 ESC 역할 연기 데이터 포함

---

# 2.3.2 Implementation and Evaluation Metric

- **모델 선택**: 
  - 영어와 중국어가 포함된 캐릭터 카드 때문에 Qwen1.5-14B-Chat을 기본 모델로 선택.

- **파라미터 효율적인 Fine-tuning**:
  - LoRA(Hu et al., 2021) 방식 적용.

- **비교 모델**: 
  - ESC-Role과 GPT-4, BaichuanNPC 등의 최신 역할 수행 에이전트 비교.
  - 이들 모델은 API 기반 LLMs임.

- **프롬프트 종류**: 
  - Chain-of-Thought (CoT)(Wei et al., 2022) 및 In-Context-Learning (ICL)(Min et al., 2022) 사용.
  - 자세한 내용은 부록 B 참조.

- **효과성 비교**:
  - 역할 수행 모델의 효과성을 비교하기 위해 역할 수행 연구와 감정 지원 분야의 특징 활용.
  - 다음 여섯 가지 카테고리의 메트릭 제안:
    - 일반 메트릭: 일관성(Coherence), 유창성(Fluency)
    - 도메인 특화 메트릭: 주제 일관성(Thematic consistency), 완전성(Completeness), 감정 일치(Emotional Congruence), 인간성(Humanoid)

- **평가 방법**:
  - 수동 평가 방법을 통해 각 차원을 3포인트 척도로 평가.
  - 쌍 비교를 통해 자연어 대화와 유사한 대화를 누구의 모델이 더 잘 생성했는지 평가.

- **인간 평가 결과**: 
  - 표 1에 다양한 역할 수행 에이전트의 인간 판단 결과 제시.
  - 일반 API 모델 비교에서 영어에서는 GPT-4가 더 우수하고, 중국어에서는 Baichuan-NPC가 우수함.
  - GPT-4는 다양한 프롬프트 최적화를 통해 성능 향상 가능, 반면 Baichuan-NPC는 최적화 시 성능 저하.

- **ESC-Role 성능**:
  - ESC-Role은 강력한 인간 유사 특성을 보여주며, 일반 메트릭에서도 인상적인 결과를 보임.
  - ESC-Role과 원본 데이터 간의 차이를 수동으로 평가한 결과, 두 모델 모두 GPT-4와 Baichuan-NPC보다 우수함.

- **결론**:
  - ESC-Role을 사용한 역할 수행이 ESC-Eval에서 효과적임을 증명.

---

# 3 Evaluation

- 14개의 일반 LLM 및 도메인 특화 LLM에 대한 ESC-Eval 평가 수행
- 평가할 모델 소개
- 실험 결과 표시
- 점수 모델 ESC-RANK의 세부 사항 제공

---

# 3.1 Evaluating models

- **모델 선정**: 
  - 총 14개의 모델을 평가 대상으로 선정
    - **폐쇄형 모델**:
      1. GPT-4 (Achiam et al., 2023)
      2. ChatGPT
    - **오픈소스 모델**:
      1. Vicuna (Zheng et al., 2023a)
      2. llama3 (Touvron et al., 2023)
      3. WizardLM (Xu et al., 2023)
      4. Qwen1.5 (Bai et al., 2023)
      5. Chatglm3 (Zeng et al., 2022)
      6. Yi (AI et al., 2024)
    - **도메인 특정 모델**:
      1. ExTES-llama (Zheng et al., 2023b)
      2. ChatCounselor (Liu et al., 2023)
      3. MindChat (Xin Yan, 2023)
      4. SoulChat (Chen et al., 2023b)
      5. EmoLLM (EmoLLM, 2024)
      6. MeChat (Qiu et al., 2023)

- **평가 기준 및 결과**:
  - 모델의 유창성, 표현력, 공감, 정보 제공, 능숙함, 인간미, 전반적인 평균 등을 기준으로 평가
  - 예시 결과(모델명: Fluency, Expression, Empathy, Information, Skillful, Humanoid, Overall Average):
    - **GPT-4**: 74.32, 71.68, 71.22, 73.72, 74.92, 36.40, 44.18, 63.78
    - **ChatGPT**: 74.70, 71.22, 72.12, 73.19, 74.92, 37.08, 45.24, 64.07
    - **Vicuna-7B-1.5**: 63.37, 67.07, 71.00, 71.53, 71.68, 41.31, 38.67, 60.66

- **모델 크기 비교**:
  - 비슷한 규모의 모델들(예: 6B/7B/8B 파라미터 크기)을 선택하여 비교함으로써 더 정확한 평가를 가능하게 함.

---

# 3.2 Evaluation Results

- 명확하게 정의된 차원을 기반으로 포괄적인 수동 평가를 수행하였으며, 결과는 Table 2에 제시됨.
- 영어 및 중국어 ESC 조건에서 도메인 특화 LLMs인 ChatCounselor와 EmoLLM이 각각 최고의 성과를 이룸.
- 일반 모델과 도메인 특화 모델 비교 결과:
  - 일반 모델은 유창성, 표현 다양성, 감정적 편안함 기술에서 더 뛰어난 성능을 보임.
  - 예를 들어, "I understand you very well, it is very normal to feel ..., here are some possible suggestions:"와 같은 구조적인 출력을 생성.
  - 일반 모델은 많은 양의 텍스트를 생성하여 조언 효과성과 표현 다양성에서 높은 점수를 기록.
- API 기반 모델은 파라미터 규모가 커서 감정적 편안함에 대한 지식이 뛰어나며, GPT-4 및 ChatGPT가 가장 높은 전문성을 보여줌.
- 그러나 이러한 모델은 인간적이고 인간 중심의 반응에서 성과가 낮음. 사용자는 보다 인간화되고 인간적인 특성을 갖춘 응답을 기대.
- 도메인 특화 모델 비교 결과:
  - MindChat, SoulChat, EmoLLM은 영어로 미세 조정되지 않아 유창성에서 열세를 보임.
  - 반면 ExTES-llama와 ChatCounselor는 좋은 성과를 나타냄.
  - ExTES는 ChatGPT가 생성한 데이터로 조정되었고, ChatCounselor는 실제 심리 상담 데이터를 사용하여 조정되어 우수한 성과를 보임.
- Table 2 하단에서, 일반 모델은 표현 다양성과 효과적인 제안 제공 측면에서 좋은 성과를 기록.
- 다양한 풍부한 데이터로 훈련된 EmoLLM은 여러 차원에서 기타 도메인 특화 모델들 중 뛰어난 성과를 보임.
- 다른 도메인 특화 모델은 뛰어난 인간적 특성과 인간 편의성으로 인해 일반 모델을 초과 성과.
- 그러나 감정적 지원 지식 측면에서 개선의 여지가 있으며 인간 편의성을 향상시킬 잠재력이 큼.
- MindChat은 이중 언어 데이터로 훈련되어 강력한 중국어 능력을 보여줄 뿐만 아니라 영어 능력도 칭찬할 만함.

---

# 3.3 Correlation Analysis

- ESC-Eval의 효과성을 검증하기 위해 ESConv 데이터셋에서 20개의 인스턴스를 무작위로 선택
- 타겟 모델에서 3개의 카테고리를 선택하고, 상관 분석을 위해 5개의 다른 모델 포함
- 모델은 도움을 요청하는 탐색자 역할의 인간 평가자와 상호작용
- 상호작용 완료 후, 다른 논문에서 제시된 인간 평가 방법에 따라 평가 점수 제공
- 인간 평가 점수는 최적의 평가 방법으로 간주됨
- 다양한 자동 평가 방법과 ESC-Eval 방법 간의 상관 분석 실시
- 결과는 표 3에 제시됨
  - ESC-Eval은 평가 지표와 가장 높은 상관관계를 보임 (Fluency 및 Empathy 지표 제외)
- Fluency 측면에서 자동화된 지표가 ESC-Eval보다 우수
  - 인간 평가자들은 일반 모델이 생성한 세분화된 문장에 대한 유창성에 편견을 가질 가능성
  - 일반 모델이 생성한 내용이 ESConv 데이터셋과 크게 다름
  - 자동화된 지표는 낮은 점수를 보이지만 인간 평가자들은 자연스럽게 표현된 내용을 선호함
- ESC-Eval에서는 모든 모델이 LLM의 능력 덕분에 Fluency에서 우수한 성능을 보이며, 이로 인해 상관관계가 낮음
- Empathy 지표에서도 비슷한 현상이 관찰됨
  - 상관관계가 있음에도 불구하고 LLM의 정렬 과정 덕분에 위로 능력과 분석 기술을 잘 보여줌
- 전체 평균 지표 측면에서 ESC-Eval이 자동화된 지표보다 가장 큰 상관관계를 보임
- ESC-Eval의 효과성을 추가적으로 강조함
- 더 많은 상관 관계 실험 결과는 부록 C에 수록됨

---

# 3.4 ESC-RANK

- 후속 연구를 위한 기반으로, InternLM2-7B-Chat(Cai et al., 2024)와 수동으로 주석이 달린 데이터를 사용하여 ESC-RANK를 훈련함.
- ESC-RANK는 여러 모델의 대화 결과를 잘 설계된 차원에 따라 점수화할 수 있음.
- 주석 데이터는 훈련 세트, 검증 세트, 테스트 세트로 무작위로 나누며 비율은 7:1:2임.
- ESC-RANK는 기본 모델과 GPT-4와 비교했을 때, 정확도 면에서 35점 더 높은 성능을 보여줌 (Table 4 참조).
- 사람의 점수 매기기가 항상 명확한 경계를 갖는 것은 아니므로, 1점의 오류 허용이 있으며, 이를 통해 $$ACC_{soft}$$를 정의함.
- $$ACC_{soft}$$를 고려했을 때, ESC-RANK는 99% 이상의 정확도를 달성하여 후속 자동화 프로세스에 대한 해결책을 제공함.
- 흥미롭게도, GPT-4는 인간형 및 인간 선호 점수 측면에서 성능이 저조함.
- 분석 결과, GPT-4는 자신이 생성한 콘텐츠나 유사한 콘텐츠에 더 높은 점수를 부여하는 경향이 있으며, 이는 인간 평가에서 쉽게 식별됨. 
- 특히 글머리 기호 형태의 출력에서는 기계 생성 콘텐츠가 명확히 드러나고, 결과적으로 인간형 및 인간 선호 점수에서 낮은 점수를 초래함.
- InternLM2도 인간 선호 행동에서 유사한 문제를 보이지만, 인간형 점수에서는 더 나은 성과를 보여 GPT-4보다 높은 $$ACC_{soft}$$를 달성함.

---

# 4 Related Work

- 여러 연구들이 다변량 시계열 데이터의 예측을 위한 다양한 접근방식을 제안함
  - 예를 들어, 시계열 데이터의 동적 특성을 반영하는 모형들이 존재
- 기존의 일부 연구는 인공신경망(ANN)을 활용하여 성능을 향상시킴
  - 특히 LSTM(Long Short-Term Memory) 네트워크가 인기
- 최근에는 그래프 신경망(GNN) 등의 새로운 모델들이 도입되며 주목받고 있음
  - GNN은 데이터의 구조적 관계를 효과적으로 반영하여 시계열 예측에 기여
- 특정 적용 분야에서의 연구가 두드러짐
  - 예를 들어, 금융, 헬스케어 등 고유한 속성을 지닌 데이터셋에 대한 연구
- 그러나, 많은 기존 연구들이 특정한 구조나 가정을 기반으로 하여 일반화에 한계를 가짐
- 알고리즘의 성능 개선을 위한 다양한 접근이 필요하며, 이는 제안된 방법론의 필요성을 강조함
- 현재의 연구들 사이에서의 성능 비교 및 데이터셋의 중요성이 시사됨
- 데이터의 특성과 요구 사항에 따라 맞춤형 접근이 필요하다는 점도 강조됨

---

# 4.1 Emotion Support Conversation

- 전통적인 연구
  - 초기 감정 지원 시스템은 단순한 단일 턴 감정 대화 시스템에 집중
  - ESConv (Liu et al., 2021) 데이터셋 출현으로 다중 턴 대화로 발전

- 연구 방향
  - 다양한 최적화 전략 제안
  - Peng et al. (2022): 계층적 그래프 네트워크 도입, 감정 원인과 사용자의 의도 효과적으로 활용
  - Tu et al. (2022): 단일 전략에서 벗어나 상식 지식 및 혼합 응답 전략 통합

- LLMs의 발전
  - 생성 아키텍처 덕분에 챗봇 시나리오에 적합
  - Zheng et al. (2023b), Qiu et al. (2023), Liu et al. (2023): LLMs 활용

- 평가 메트릭 
  - Fluency, Suggestion, Skillful, Empathy 등 다양한 메트릭으로 성능 평가
  - Spearman 및 Pearson 상관계수를 이용한 결과 제공
  
- 모델 성능 비교
  - 모델: InternLM2, GPT-4, ESC-RANK의 정확도 및 흐름 평가
  - 다양한 평가 지표에서 ESC-RANK 모델이 우수한 성능 기록

- 연구 목표
  - LLM 기반의 ESC 모델에 대한 포괄적이고 엄격한 평가 제공

---

# 4.2 Role Play Agents

- 최신 LLMs의 발전이 Role-Playing Language Agents (RPLAs)의 출현을 크게 증가시킴.
- 다양한 연구들이 역할 놀이를 위한 여러 평가 데이터셋을 제안함.
  - 주요 연구: Chen et al. (2024), Wang et al. (2024b), Tu et al. (2024), Shen et al. (2024), Wang et al. (2024a).
- RPLAs 평가 방법으로는 다음과 같은 접근법이 사용됨:
  - In-Context-Learning (ICL) $$ \rightarrow \text{Min et al. (2022)} $$
  - Chain-of-Thought (CoT) $$ \rightarrow \text{Wei et al. (2022)} $$
  - Supervised Fine-Tuning (SFT)
- 다양한 역할 놀이 제품들이 산업에서 등장함:
  - Character AI
  - Reflection AI
- RPLAs는 특정 역할을 맡고, 복합 캐릭터 설정과 역할 배경 지식, 대화 스타일을 통해 인간과 유사한 상호작용 가능.
- RPLAs는 인간적인 속성을 나타내며, 일상 대화 맥락에서도 역할 수행 가능. 
- 본 논문은 RPLAs를 통해 ESC 모델을 평가하는 주제를 다룸.

---

# 5 Conclusion

- 본 논문은 대규모 언어 모델(LLM)에서 감정 지원 대화(ESC)의 효과성과 지속 가능성을 평가하기 위한 새로운 접근 방식을 제안.
  
- 역할 놀이 모델을 활용하여 다중 턴 대화 데이터를 확보함으로써 실험 결과 제안된 방법의 효능과 실행 가능성을 입증.

- 평가 결과:
  - 일부 ESC 모델은 일반 모델보다 우수한 성능을 보임.
  - 그러나 ESC 모델의 지식 능력 및 인간 선호 능력 개선 여지가 크게 존재.

- 연구자들에게 ESC 연구에 참여하고 더 견고한 ESC 모델 개발에 기여할 것을 권장.

- 한계:
  - 사용할 수 있는 크라우드소싱 주석자는 원어민이 아님.
  - 총 14명의 주석자 중 1명은 컴퓨터 과학 배경, 2명은 정신 건강/심리학 배경을 지님.
  - 나머지는 법학, 사회학 및 역사와 같은 인문사회과학 배경을 가짐.
  - 영어 주석에서 발생할 수 있는 한계 피할 수 없음.

- 역할 카드 제작:
  - 본 논문에서 사용된 역할 카드는 GPT4를 통해 추출됨.
  - 다양한 필터링 방법 사용에도 불구하고 모델의 잠재적 편향은 여전히 보장할 수 없음.
  - 향후 프로젝트의 개선 희망.

- 윤리적 고려사항:
  - 본 연구는 심리학과 관련이 있어 데이터 세트 형식이 변환되고, 각 데이터 인스턴스는 수작업으로 검토됨.
  - 데이터의 윤리적 및 개인 정보 보호 문제와 법적 요구사항을 준수함을 확인.

---

# 독자 의견

- 본 논문은 롤플레잉 방법을 결합하여 감정지원대화에서의 다중 턴 대화를 실험하였음.
- 또한 자동 및 수동 평가 방법인 ESC-Eval을 통해 ESC 모델의 성능을 평가하였음.
- 다양한 자동 Metrices와 ESC-Eval의 인간평가와의 상관관계를 분석하여 ESC-Eval의 효과성을 검증하였음.
- InternLM2-7B-Chat 기반의 모델을 수동으로 주석이 달린 데이터를 사용하여 ESC-RANK를 훈련하였음.
- 본 논문의 후속 연구로는 다음과 같은 것들을 생각해볼 수 있음
  - 다국어 모델로서의 확장.
  - 사전입력되는 프롬프트의 축소.
  - 강화학습을 통한 모델 성능 향상.
  - 수동 작업의 최소화.