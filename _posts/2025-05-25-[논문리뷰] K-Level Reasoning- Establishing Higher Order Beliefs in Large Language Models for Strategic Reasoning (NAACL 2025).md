---
title: "[논문리뷰] K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning (NAACL 2025)"
date: 2025-05-25 18:54:04 +0900
categories:
  - Paper Review
tags:
  - NAACL 2025
  - LLM in Game Theory
---

본 논문은 게임 이론의 Level-K 프레임워크를 기반으로, 대규모 언어 모델이 다른 에이전트의 관점과 행동을 재귀적으로 추론하며 전략적 깊이를 확장할 수 있는 K-R 프레임워크를 제안하여, 다중 에이전트 환경에서의 전략적 추론 성능을 향상시켰다.

---

# 1 Introduction

- 전략적 추론(여러 참가자가 있는 환경에서의 의사결정)은 대형 언어 모델(LLMs)과 LLM 에이전트에게 독특한 도전 과제를 제시함.
- 이 환경에서는 에이전트가 다른 참가자들의 행동에 대응하고, 동적인 환경에 적응하며, 자신의 목표와 일치하는 결정을 내려야 함.
- 전략적 추론은 투자, 비즈니스 전략 수립, 협상, 정책 결정 등 다양한 실제 과제에서 필수적임.
- 효과적인 전략적 추론은 타인의 관점을 이해하고 그들의 전략을 예측하는 데 의존함.
- 기존 연구들은 대부분 정적인 프롬프트 기반 방법으로 타인의 신념과 결정을 모델에 반영하도록 지시함. 그러나 이는 타인의 신념에 대한 신념(고차원 신념)을 형성하거나 깊은 전략적 추론을 수행하는 데 한계가 있음.

- K-레벨 사고(Level-k thinking) 개념(그림 1 참조)은 행동경제학과 게임 이론에서 다양한 깊이의 전략적 사고를 분류함.
  - 1단계: 에이전트가 환경에 직접 반응.
  - 2단계: 타인의 1단계 사고를 고려.
  - 이 과정을 반복하여 더 높은 차원의 신념을 형성함.

- 본 논문에서는 K-레벨 사고에서 영감을 받아 "K-레벨 추론 프레임워크(K-R)"를 제안함.
  - 계층적 수준으로 추론을 조직하며, 재귀적 메커니즘을 사용해 다양한 전략적 깊이를 통합함.
  - 구체적으로, 
    1) 환경 문맥과 과거 공개 정보를 바탕으로 타인의 행동을 다양한 전략 수준에서 재귀적으로 예측,
    2) 이를 기반으로 최적의 행동을 추론함.

- K-R은 LLM에서 다양한 전략적 깊이를 재귀 메커니즘으로 구현한 최초의 접근법이며, 알고리즘적 프레임워크를 통해 LLM 에이전트의 깊은 추론을 가능하게 함.

- 네 가지 테스트베드로 프레임워크를 검증:
  - 두 가지 고전적 게임이론 문제: "평균의 0.8 맞추기"(그림 2 왼쪽), "생존 경매 게임"(그림 2 중간).
  - 두 가지 사회 지능 과제: "협상"(그림 2 오른쪽), SOTOPIA 벤치마크.

- 실험 결과:
  - K-R이 기존 추론 방법들보다 유의미하게 우수하며, 다양한 전략적 깊이를 유연하게 달성함.
  
- 이론적 분석:
  - LLM의 컨텍스트 내 학습 능력을 활용하여 공개 정보와 상대방 정보를 누적적으로 사용해 상대 행동을 효과적으로 모델링함.
  - 인간 참가자와 전략적 깊이를 정렬시켜 평가(Nagel, 1995; Bosch-Domenech et al., 2002).
  - K-R 도입 후 LLM의 전략적 깊이가 0.25에서 1.89로 증가.
  - 특히 $$K=3$$일 때 LLM 전략적 깊이(1.89)가 금융 신문 독자(1.91)에 근접, 이는 K-R이 LLM에 고차원 신념을 확립함을 시사함.

- 본 연구의 주요 기여:
  - K-R: k-레벨 사고를 LLM에 확장하여 재귀적 메커니즘으로 다양한 깊이의 전략적 추론 가능.
  - 게임이론 및 사회 지능 문제에서 광범위한 평가를 통해, 폐쇄형 및 오픈 소스 모델 모두에서 기존 기법 대비 유연성과 성능 우수성 입증.
  - K-R의 고차원 신념 형성 및 전략적 추론 향상 능력에 대한 심층 분석 제공, 이는 LLM의 마음 이론 및 전략적 추론 연구의 토대 마련.

---

# 2 K-Level Reasoning with Large Language Models

- 전략적 추론은 의사결정 상황과 다른 참가자들의 가능한 행동을 모두 고려해야 함.
- 다중 참가자 멀티라운드 정규형 게임을 활용하여 제안된 방법을 소개.

## 2.1 방법론

- 각 에이전트 $$i$$는 시점 $$t$$에서 행동 집합 $$A_t^i$$ 중 하나의 행동 $$a_t^i$$를 선택.
- 에이전트 $$i$$의 보상은 환경 $$E_t$$와 행동 프로필 $$A_t = (a_t^1, a_t^2, ..., a_t^N)$$에 의해 결정되며, $$U_i(E_t, A_t)$$로 표기.

- **Level 1 사고($$k=1$$):** 에이전트는 전략적 예측 없이 환경 $$E_t$$만을 고려하여 결정:
  $$
  a_{t,1}^i = \arg\max_{a^i \in A_t^i} \mathbb{E}[U_i(E_t, a^i)]
  $$
  
- **Level $$k \geq 2$$ 사고:** 에이전트 $$i$$는 다른 에이전트들의 $$k-1$$ 수준 사고를 시뮬레이션하고, 이에 따라 전략을 수정:
  $$
  a_{t,k}^i = \arg\max_{a^i \in A_t^i} \mathbb{E}[U_i(E_t, a^i, \hat{a}_{t,k-1}^{-i})]
  $$
  여기서 $$\hat{a}_{t,k-1}^{-i}$$는 다른 에이전트들의 $$k-1$$ 수준 행동 예측.

- 제안하는 프레임워크는 "K-Level Reasoning with Large Language Models (K-R)"로 명명됨.

- K-R은 다음 두 단계로 구성:
  1. **예측 (Anticipation):** 
     $$
     \hat{a}_{t,m}^j = 
     \begin{cases}
     LLM(E_t, H_t^j) & \text{if } m=1 \\
     LLM(E_t, H_t^j, \hat{a}_{t,m-1}^{-j}) & \text{if } m>1
     \end{cases}
     $$
     여기서 $$H_t^j = \{(E_1, a_1^j), (E_2, a_2^j), ..., (E_{t-1}, a_{t-1}^j)\}$$는 에이전트 $$j$$의 공개된 과거 정보, $$m$$은 사고 수준.
  
  2. **추론 (Reasoning):**
     $$
     a_{t,k}^i = LLM(E_t, H_t^i, \hat{a}_{t,k-1}^{-i})
     $$

- 알고리즘 1 (K-R 구현):

  ```
  함수 K_REASONING(i, k):
      if k == 1 then
          return LLM(E_t, H_t^i)
      else
          for 각 에이전트 j ≠ i do
              hat{a}_{t,k-1}^j = K_REASONING(j, k-1)
          end for
          return LLM(E_t, H_t^i, {hat{a}_{t,k-1}^j | j ≠ i})
      end if
  
  a_{t,K}^i = K_REASONING(i, K)
  return a_{t,K}^i
  ```

- 이 재귀적 방법은 점진적으로 깊은 전략적 사고 단계($$1, 2, ..., k, k+1, ...$$)를 가능하게 하여 LLM 에이전트의 고차원 신념(믿음)을 강화함.

- 단순화를 위해 모든 상대가 동일 사고 수준에 있다고 가정했으며, 실제로는 다양한 수준도 구현 가능.

## 2.2 이론적 분석

- 본 절에서는 K-R의 이론적 이점을 논의.

- 각 에이전트 $$j$$의 의사결정은 숨겨진 전략 $$\theta_j^*$$를 따른다고 가정:
  $$
  P(a_t^j | E_t, \theta_j^*)
  $$

- LLM의 In-context learning은 암묵적 베이지안 추론으로 해석 가능(Xie et al., 2021):
  $$
  P(a_t^j | E_t, H_t^j) = \int P(a_t^j | E_t, \theta_j) P(\theta_j | H_t^j) d\theta_j
  $$

- 상호작용 횟수 $$t \to \infty$$ 일 때, 베이지안 업데이트와 대수의 법칙에 의해 사후 분포가 실제 전략에 수렴:
  $$
  P(\theta_j | H_t^j) \to \delta(\theta_j - \theta_j^*)
  $$
  여기서 $$\delta$$는 디락 델타 함수.

- 따라서 다음 예측은 실제 전략과 일치하게 됨:
  $$
  \int P(a_t^j | E_t, \theta_j) P(\theta_j | H_t^j) d\theta_j \to P(a_t^j | E_t, \theta_j^*)
  $$

- 이 결과는 상호작용 데이터가 충분히 많을수록 K-R이 상대 행동을 더욱 정확하게 예측할 수 있음을 시사.

- 다만, 상호작용 데이터는 무한하지 않으며, In-context learning 성능은 LLM의 성능에 의존함.

- 이러한 가설과 추론 결과는 5.2절에서 실험적으로 검증함.

---

# 3 Experiments: Game Theory

- **목적**  
  - LLM들의 전략적 추론 능력을 공정하게 비교하기 위해, 두 가지 널리 사용되는 게임 이론 설정을 채택  
  - 통제되고 명확히 정의된 게임 이론 문제들은 LLM의 성능을 견고하게 평가함 (자세한 설정은 Appendix B 참고)

## 3.1 과제 정의 및 평가 지표

### 3.1.1 평균의 0.8배 맞히기 (G0.8A)

- Alain Ledoux(1981)가 제시한 고전적 게임 이론 문제  
- 1부터 100 사이의 숫자를 선택하는 10라운드 게임  
- 그룹 전체 평균 선택값의 80%에 가장 가까운 수를 고르는 것이 목표  
- 참가자는 다른 사람들이 평균을 어떻게 추산할지 예측하여 수를 제출  
- 케인즈의 미인대회(1936) 개념과 유사하며, 금융 시장에서 집단 행동 예측하는 상황과 닮음  
- 평가는 개별 라운드 당 승률(Win Rate)로 수행

### 3.1.2 서바이벌 경매 게임(SAG)

- Mao et al.(2023)의 수자원 할당 문제에서 파생  
- 10일간 가뭄을 견디는 것이 목표이며, 플레이어는 물 자원을 입찰하여 건강 포인트(Health Points)를 유지  
- 물을 입찰 성공 시 건강 포인트 획득, 실패 시 손실  
- 경매 시스템과 건강 포인트 메커니즘이 결합된 동적 환경  
- 평균 생존 라운드(Average Survival Round)를 평가지표로 사용

## 3.2 기본 기법

- 다양한 기존 추론 및 에이전트 벤치마크 방법들을 적용  

1. **Standard Prompting (Direct)**  
   - 주어진 게임 설정 프롬프트에 대해 LLM이 바로 최종 행동(Action) 생성

2. **Chain-of-Thought (CoT)** (Wei et al., 2022)  
   - 제로샷 연쇄 추론 기법 적용 (Kojima et al., 2022)

3. **Persona Prompting (Persona)** (Deshpande et al., 2023)  
   - “게임 전문가” 페르소나를 도입하여 추론 능력 강화

4. **Reflexion (Reflect)** (Shinn et al., 2023)  
   - 언어 에이전트에 구두 강화 학습을 적용한 방법, 동적 작업에 적합하도록 변형(자세한 내용 Appendix K)

5. **Self-Refine (Refine)** (Madaan et al., 2023)  
   - 다중 라운드 반복 추론, 추가 LLM이 의견 및 수정을 제공한 뒤 최종 결정  
   - Reflect와의 차이점은 Appendix I에 설명

6. **Prediction Chain of Thought (PCoT)**  
   - CoT와 달리, 결정 전에 상대 행동을 명시적으로 예측하는 강력한 기준선  
   - K-Level Reasoning과 달리 재귀적 접근 대신 직접 예측에 집중  
   - 구현 및 예제는 Appendix K 참고

## 3.3 실험 설정

- 제어 가능한 환경 구축, 두 역할 구분: 플레이어(중심)와 상대  
- 플레이어는 특정 기법 사용, 상대들은 다른 한 가지 기법 사용  
- G0.8A와 SAG 모두에 플레이어 1명과 상대 4명 배치  
- 각 실험은 10회 반복, 유의성 검정 통과(Appendix H)  
- 각 실험은 10라운드 게임 구성  
- 주요 실험의 모든 방법은 GPT-4 (gpt432k)로 구현, temperature=0.7, top-p=0.9  
- 오픈 소스 LLM 실험은 Appendix E 참고  
- 특별한 명시가 없으면 K-Level Reasoning의 사고 수준은 K=2로 설정

## 3.4 결과

- 표에서 플레이어는 **볼드체**, 상대는 *이탤릭체*로 표기  

### 표 1: G0.8A 게임에서 플레이어의 상대별 승률 (Win Rate)

| Player \ Opponent | Direct | CoT  | Persona | Reflect | Refine | PCoT | K-R   |
|-------------------|--------|------|---------|---------|--------|------|-------|
| **Direct**        | 0.43   | 0.67 | 0.62    | 0.53    | 0.43   | 0.61 | 0.82  |
| **CoT**           | 0.07   | 0.32 | 0.35    | 0.14    | 0.22   | 0.45 | 0.63  |
| **Persona**       | 0.05   | 0.37 | 0.29    | 0.05    | 0.37   | 0.11 | 0.46  |
| **Reflect**       | 0.42   | 0.68 | 0.63    | 0.39    | 0.64   | 0.74 | 0.78  |
| **Refine**        | 0.10   | 0.34 | 0.32    | 0.31    | 0.23   | 0.22 | 0.46  |
| **PCoT**          | 0.03   | 0.44 | 0.52    | 0.21    | 0.51   | 0.54 | 0.85  |
| **K-R**           | 0.04   | 0.15 | 0.14    | 0.04    | 0.17   | 0.14 | 0.52  |
| **평균 ± 표준편차**| 0.16±0.18| 0.32±0.19| 0.41±0.18 | 0.24±0.18 | 0.37±0.17 | 0.40±0.25 | 0.65±0.17 |

- K-레벨 추론(K-R) 방식이 월등히 높은 0.65의 승률로 다른 전략들보다 크게 우수함

### 표 2: SAG 게임에서 플레이어의 상대별 평균 생존 라운드 (Average Survival Round)

| Player \ Opponent | Direct | CoT  | Persona | Reflect | Refine | PCoT | K-R   |
|-------------------|--------|------|---------|---------|--------|------|-------|
| **Direct**        | 5.90   | 7.00 | 7.50    | 4.70    | 8.70   | 6.60 | 9.40  |
| **CoT**           | 5.70   | 6.50 | 5.30    | 4.00    | 8.10   | 5.30 | 10.00 |
| **Persona**       | 5.70   | 7.70 | 7.40    | 5.20    | 6.30   | 7.20 | 9.30  |
| **Reflect**       | 9.40   | 9.40 | 9.90    | 5.20    | 8.60   | 8.20 | 10.00 |
| **Refine**        | 6.30   | 6.40 | 8.10    | 4.30    | 8.20   | 5.30 | 7.90  |
| **PCoT**          | 8.50   | 9.60 | 9.90    | 6.30    | 8.50   | 6.20 | 9.70  |
| **K-R**           | 4.10   | 5.50 | 5.00    | 4.04    | 5.70   | 4.40 | 6.80  |
| **평균 ± 표준편차**| 6.51±1.82 | 7.44±1.55 | 7.59±1.95 | 4.82±0.82 | 7.73±1.21 | 6.17±1.29 | 9.01±1.21 |

- K-R 방법이 평균 9.01 라운드로 가장 오래 생존, 다른 방법 대비 압도적 우수성 나타냄

## 주요 해석 및 결론

- K-레벨 추론 방법이 상대 행동을 예측하는 능력을 갖추어 다른 추론 방식들보다 전략적으로 우수함을 증명  
- Reflect 방법은 동적 환경에서 이전 라운드 경험에만 의존하기 때문에 성능이 저조, 차라리 다음 라운드 적용에 부적합한 것으로 추측  
- Refine 방법은 자신의 전략에 기반한 조정만 수행하여 상대의 숨겨진 전략을 고려하지 않아 K-R보다 낮은 성능
- 전반적으로 K-R이 상대 전략을 명시적으로 예측하며, 게임 내 전략적 우위를 크게 확보함을 확인

---

# 4 Experiments: Social Intelligence

- **목적**  
  K-Level Reasoning (K-R)의 성능을 보다 현실적이고 개방형 사회적 상호작용 시나리오에서 평가하기 위해 두 가지 사회적 지능 벤치마크를 사용함.  
  - 게임 이론의 추상적이고 이론적인 설정과 달리, 해당 시나리오들은 풍부한 맥락과 복잡한 목표 추구를 포함하여 실제 애플리케이션(예: 챗봇, 전략적 의사결정)에서 LLM 기반 에이전트의 가치를 잘 보여줌.

## 4.1 Task Definition and Metrics

### 4.1.1 Negotiation (NEG)

- NEG 과제는 오픈엔디드하고 현실적인 과제임 (참조: Cao et al., 2018; Duan et al., 2024).  
- 두 에이전트가 고추(peppers), 체리(cherries), 딸기(strawberries) 세 가지 아이템을 두고 협상함.  
- 각 에이전트는 아이템마다 개인 유틸리티 값을 가짐.  
- 공개 아이템 풀을 합의에 따라 할당하며, 더 높은 유틸리티를 확보한 에이전트가 승리.  
- 성능 지표로 Win Rate(승률)를 사용.

### 4.1.2 SOTOPIA Benchmark

- SOTOPIA는 인공 에이전트 간 복잡한 사회적 상호작용을 시뮬레이션하고 사회적 지능을 평가하는 오픈엔디드 환경임.  
- 다양한 사회적 시나리오를 포함하며, 각 시나리오는 맥락, 각 에이전트의 사적 사회 목표, 성격, 이름, 성별, 직업 등의 캐릭터 프로필을 가짐.  
- 상호작용 종료 시 7개 차원에 따라 점수가 평가됨:  
  - Goal Completion (GOAL)  
  - Believability (BEL)  
  - Knowledge (KNO)  
  - Secret (SEC)  
  - Relationship (REL)  
  - Social Rules (SOC)  
  - Financial and Material Benefits (FIN)

## 4.2 Experimental Settings

- 3.2절에서 소개한 다양한 추론 방법들을 베이스라인 모델로 활용하여 비교 실험 진행.  
- NEG: 기존 연구 설정(Cao et al., 2018; Duan et al., 2024)과 동일하게, 각 게임마다 플레이어와 상대 1명씩 총 100번 독립 반복 게임 수행.  
  - 위치 이점 제거를 위해 선수와 상대 위치를 교환하면서 평가.  
  - 신뢰성 확보를 위해 3회 실험 반복, 평균 및 표준편차 산출.  
- SOTOPIA-hard: 총 100개 에피소드로 구성된 어려운 설정(Zhou et al., 2024) 사용.  
  - 고정된 GPT-4o 기반 에이전트를 파트너로 활용.  
  - 평가 모델로 GPT-4 사용 (SOTOPIA 벤치마크 기준 GPT-4가 인간 판단을 대체할 신뢰할 만한 평가자임 확인됨).

## 4.3 Results

### Negotiation (NEG) 결과 (표 3 요약)

| 모델      | Direct | CoT   | Persona | Reflect | Refine | PCoT  | K-R    |
|-----------|---------|--------|---------|---------|--------|-------|--------|
| Direct    | 50.00   | 61.34  | 49.58   | 66.67   | 65.83  | 63.03 | 70.83  |
| CoT       | 38.66   | 50.00  | 36.67   | 45.83   | 45.76  | 47.27 | 55.36  |
| Persona   | 50.42   | 63.33  | 50.00   | 70.00   | 67.50  | 62.50 | 70.83  |
| Reflection| 33.33   | 54.17  | 30.00   | 50.00   | 57.14  | 55.00 | 55.00  |
| Refine    | 34.17   | 54.24  | 32.50   | 42.86   | 50.00  | 55.77 | 54.55  |
| PCoT      | 36.97   | 52.73  | 37.50   | 45.00   | 44.23  | 50.00 | 57.00  |
| K-R       | 29.17   | 44.64  | 29.17   | 45.00   | 45.45  | 43.00 | 50.00  |
| **평균 ± 표준편차** | 38.96 ± 2.53 | 54.35 ± 0.50 | 37.92 ± 5.84 | 52.19 ± 1.73 | 53.70 ± 4.41 | 53.80 ± 4.34 | **59.08 ± 2.20** |

- K-R의 평균 승률 59.08%로 다른 방법 대비 높은 성과 기록.  
- K-Level Reasoning을 통해 생성된 제안들이 자신에게 더 유리하며, 상대의 제안을 수용할 때도 유리한 경우가 많음.

### SOTOPIA-hard 결과 (표 4 요약)

| 메트릭 / 모델         | Direct | CoT  | Refine | K-R   | (GPT-4o 기준)               |
|-----------------------|--------|-------|--------|-------|-----------------------------|
| BEL [0–10]            | 8.97   | 9.00  | 9.00   | 8.97  |                              |
| REL [-5–5]            | 2.38   | 2.40  | 2.27   | 2.67  |                              |
| KNO [0–10]            | 6.05   | 6.05  | 6.25   | 6.25  |                              |
| SEC [-10–0]           | 0.00   | -0.05 | 0.00   | 0.00  |                              |
| SOC [-10–0]           | -0.05  | 0.00  | -0.05  | 0.00  |                              |
| FIN [-5–5]            | 0.90   | 0.78  | 0.80   | 0.72  |                              |
| GOAL [0–10]           | 6.35   | 6.60  | 6.15   | 6.47  |                              |
| **전체 점수 (평균 ± 표준편차)** | 3.51 ± 0.09 | 3.54 ± 0.08 | 3.49 ± 0.08 | 3.59 ± 0.09 |                              |

- K-R는 일부 메트릭에서 약간의 향상을 보이나 통계적으로 유의미하지는 않음.  
- GPT-4 기반 모델들이 일반적으로 GPT-4 결과에 더 높은 점수를 부여하는 경향이 있음.  
- LLaMA 3.1 70B 기반 K-R 에이전트는 성능 향상 폭이 큼.  
- 전반적으로 K-R은 GPT-4 모델과 비교 가능한 사회적 지능 성능을 보이며 사회적 지능 영역에서 가능성을 보여줌.

---

# 5 Discussions

- ## 5.1 K-Level Reasoning(K-R)이 LLM에서 상위 수준의 신념을 효율적으로 형성하는가?
  - G0.8A 문제는 고전 게임 이론에서 중요한 연구 주제이며, 인간 참가자 실험 결과(Nagel, 1995; Bosch-Domenech et al., 2002)를 기준점으로 사용함.
  - 표 5와 6은 인간과 LLM(GPT-4)의 전략적 깊이와 선택 평균을 보여줌.
  - 직접 Prompt(Direct Prompt) 방식에서 GPT-4의 전략 깊이(0.25)는 실험실 환경의 대학생(0.87)보다 낮음.
  - K-Level Reasoning 적용 시 GPT-4의 전략 깊이가 0.25에서 1.89로 크게 향상됨.
  - K=3일 때 GPT-4의 전략 깊이는 금융 신문 독자(1.91) 수준에 근접함.
  - 전략 깊이 계산 방법은 부록 C에 기술되어 있음.

- ## 5.2 K-Level Reasoning은 상대방에 대한 더 정확한 예측을 이끈다
  - K-R은 상대방 행동 모델링의 중간 단계를 포함하여 예측 정확도의 진행 추이를 분석함.
  - 그림 3에서 K-R은 1라운드부터 PCoT보다 낮은 편차와 더 정밀한 예측을 보여줌.
  - 게임 후반부에서 예측이 빠르게 수렴하며 매우 높은 정확도를 기록함.
  - 이는 게임 플레이 컨텍스트가 증가함에 따라 LLM이 상위 신념을 점점 더 잘 이해함을 반영.
  - K-R은 상대방의 미래 행동을 계산하기 위해 새로운 세션을 생성, LLM의 인컨텍스트 학습 능력을 PCoT보다 효과적으로 활용함(2.2절 이론적 논의 참고).
  - 결과적으로 K-R은 더 높은 예측 정확도를 달성함.

- ## 5.3 더 나은 추론 방법론 vs. 더 강력한 기반 모델
  - 대용량 데이터 및 파라미터를 가진 LLM이 더 강한 추론 능력을 가진다는 일반적 합의가 존재함.
  - 상대적으로 약한 LLM(예: GPT-3.5)에서도 K-R이 전략적 추론 능력을 향상시킬 수 있는지 실험함.
  - GPT-3.5 기반 K-R(K-R[GPT-3.5])과 GPT-4 기반 다양한 추론법 비교(실험 각 10회 반복).
  - 표 7 결과:
    - K-R[GPT-3.5]는 평균 성능에서 GPT-4의 직접 prompting(Direct[GPT-4])을 능가함.
    - GPT-4 기반 추론법을 사용하는 상대와 경쟁 시에도 K-R[GPT-3.5]는 뛰어난 성능을 보임.
  - K-R은 상대 관점을 잘 복원하여 경쟁 환경에서 LLM 능력을 강화함.
  - 오픈소스 모델 LLAMA27B와 GPT-3.5/4 비교도 수행(부록 E), 다양한 LLM에서 K-R이 인터랙티브 맥락에서의 추론을 크게 향상시킴을 확인함.

- ## 5.4 깊은 사고 수준일수록 더 나은 전략적 성과를 내는가?
  - K=2와 K=3 K-Level Reasoning의 두 게임(Guessing 0.8 of the Average, Survival Auction Game) 성능 비교(표 8).
  - 직접 추론(Direct) 상대로 K-R[K=3]은 G0.8A에서 승률 감소(-0.05), SAG에서는 성능 유지.
    - 이는 과도한 심사숙고(overthinking)의 가능성을 시사.
  - K-R[K=3]은 K-R[K=2] 상대에 대해 두 게임 모두에서 유의미한 성능 향상을 보여줌.
    - 상대보다 한 단계 더 깊은 사고가 전략적 우위를 제공함.
    - 그러나 두 단계 이상 깊은 사고는 과도한 예상으로 인해 효용이 감소할 수 있음.
  - 실제 인터랙티브 환경에서 상대의 사고 수준 파악은 어렵고, 다양한 수준에 적응하는 K-Level Reasoning 활용이 미래 연구 방향임.
  - 높은 사고 수준은 재귀적 프롬프트 구현으로 계산 비용 증가를 수반함.
    - 계산 비용과 관련 내용은 부록 G에 자세히 설명됨.

---

# 6 Related Work

- ## 6.1 Reasoning with LLMs
  - 대규모 언어 모델(LLM)은 수학적 추론(Miao et al., 2021; Patel et al., 2021), 상식 추론(Talmor et al., 2022; Bhakthavatsalam et al., 2021), 상징적 추론(Srivastava et al., 2022; Suzgun et al., 2022) 등 다양한 복잡한 추론 작업에서 뛰어남.
  - 복잡한 질문을 일련의 중간 단계로 분해하는 Chain-of-Thought(CoT) 기법이 대표적(Wei et al., 2022; Kojima et al., 2022).
  - CoT를 확장하는 시도로 Tree of Thought(ToT)(Yao et al., 2023), Graph of Thought(GoT)(Besta et al., 2023), Skeleton-of-thought(Ning et al., 2023) 등이 제안됨.
  - Self-Refine(Madaan et al., 2023), Reflexion(Shinn et al., 2023)과 같은 접근법으로 LLM이 스스로 답변을 검토 및 개선하여 CoT의 일관성을 향상시킴.
  - LLM에 페르소나 정보를 통합하면 추론 성능이 크게 향상됨(Deshpande et al., 2023).
  - 페르소나 정보를 더 많이 반영하여 LLM의 합리성과 지식 능력을 강화하는 연구가 이루어짐(Fu et al., 2023; Wang et al., 2023).
  - 이러한 방법들은 주로 정적 작업에 적용됐으나, 다중 에이전트 환경과 같은 동적 문제에서의 추론 능력 검증은 부족함.

- ## 6.2 Strategic Reasoning within Multiple Agent System
  - 동적 문제는 다수의 참여자가 여러 라운드에 걸쳐 상호작용할 때 발생.
  - 다중 에이전트 시스템(MAS)은 환경과의 동시 상호작용, 계산 복잡도(Ding and Dong, 2020), 비정상성(Papoudakis et al., 2019), 부분 관찰성(Mahajan et al., 2019; Foerster et al., 2016), 그리고 크레딧 할당 문제(Sunehag et al., 2017) 등 단일 에이전트 시스템보다 다양한 문제에 직면함(Wong et al., 2021).
  - LLM을 활용한 추론에서는 환경의 비정상성 문제가 특히 중요한 도전 과제임.
  - 사회적 행위(Zhou et al., 2024; Hua et al., 2023), 경제 시뮬레이션(Zhao et al., 2023; Li et al., 2023), 게임 이론(Duan et al., 2024; Xu et al., 2023a), 게임 플레이(Ma et al., 2023; Xu et al., 2023b) 등 다양한 MAS 분야에서 LLM 기반 전략적 추론 연구가 진행 중.
  - 전략적 추론에서 LLM의 성능 향상을 위해 마음 이론(Theory of Mind, ToM)(Gandhi et al., 2023; Guo et al., 2023)과 강화 학습(Reinforcement Learning)(Xu et al., 2023c; Zhang et al., 2024a)이 활용됨.
  - 이 접근법들은 전략적 과제의 복잡성을 인지하도록 LLM을 유도하는 기법으로, 본 연구의 Prediction Chain-of-Thought 베이스라인과 유사함.
  - 그러나 실험 결과, 해당 방법이 재귀적이고 심층적인 전략적 사고에 필요한 명확한 인지 계층 구조를 형성하지 못함을 확인함.
  
- ## 수식 예시
  - CoT 관련 단계 별 추론 과정은 다음과 같이 표현 가능:  
  $$ \text{Answer} = f\big(f(...f(q_1)\vert q_2)...\vert q_n\big) $$  
  여기서 $$ q_i $$는 중간 추론 단계, $$ f $$는 LLM의 함수적 처리 과정을 나타냄.

---

# 7 Conclusion

- 본 논문은 대형 언어 모델(LLM)의 전략적 추론 능력을 이해하고 향상시키는 데 중요한 진전을 이룬다.
- "K-Level Reasoning with LLMs"라는 새로운 접근법을 제안하였다.
- 이 방법은 재귀적 메커니즘을 활용하여 LLM 내에서 다양한 사고 수준을 구현함으로써, 더 깊은 전략적 사고가 가능하게 한다.
- 광범위한 실험을 통해 이 방법의 우수성을 검증하였다.
- 본 연구는 LLM에서 마음 이론(theory of mind)과 전략적 추론에 관한 미래 연구의 기초를 마련한다.
- 주요 수식은 다음과 같이 표현된다:  
  $$K\text{-Level Reasoning} = \text{Recursive Mechanism for thinking levels}$$  
  $$\text{Depth of strategic thinking} \propto K$$

---


# Limitations

- K-Level Reasoning (K-R) 프레임워크의 유효성은 게임 이론과 사회적 지능 두 가지 관점에서 검증되었으나, 
  다양한 환경, 전략적 요소, 행동 집합에 걸쳐 대형 언어 모델(LLM)의 few-shot 에이전트 모델링 성능에 대한 추가 연구가 필요함.
- K-R은 상대방의 가장 가능성 높은 행동을 예측하기 위해 새로운 LLM 추론 세션을 시작하며, 
  다단계 전략 깊이를 구현하기 위한 재귀적 메커니즘은 계산 비용을 증가시킴.
- 계산 비용 증가와 관련된 내용은 부록 G에서 자세히 다루어지며, 다양한 추론 방법과의 비교도 포함되어 있음.
- 계산 비용 증가에도 불구하고, K-R은 유사한 계산 비용을 요구하는 다른 방법들보다 우수한 성능을 보임.
