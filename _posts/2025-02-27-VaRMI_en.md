---
title: "[Paper Review] Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning (EMNLP 2023)"
date: 2025-02-26 00:00:00 +0900
categories:
  - Paper Review
tags:
  - LLM
  - NLP
  - EMNLP
---

This paper proposes an offline reinforcement learning framework that combines supervised learning benefits with RL-driven adjustments to improve persona consistency in dialogue systems, introducing a variance-reducing importance sampling method to enhance training efficiency and effectiveness.

---

# 1 Introduction

Recent advancements in large language models have greatly enhanced dialogue agents, enabling them to produce fluent and natural responses. These systems are primarily trained with large amounts of unlabeled text data and fine-tuned on dialogue tasks, which helps them learn language patterns but still suffer from consistency issues. Addressing these, some efforts have focused on grounding dialogue generation in a defined persona to improve consistency.

Traditional methods to enhance persona consistency have involved supervised learning and online reinforcement learning (RL). Supervised learning promotes persona-consistent examples but fails to effectively handle contradictions, while online RL, though potentially more adept, is expensive due to the need for continuous sample generation and evaluation by accurate critics.

In response to these limitations, we propose an offline RL framework to improve persona consistency in open domain dialogue systems. Offline RL stands out by punishing contradictory utterances directly and eliminating the need for sample generation during training, thus reducing costs. It leverages pre-existing datasets with human-annotated rewards, which mitigate risks of training failures due to policy divergence.

Despite its benefits, offline RL can suffer from high variance brought by the necessity of importance sampling. To counteract this, we present VaRMI, an importance sampling method designed to reduce variance in training weights.

Our contributions include developing an offline RL approach for persona-consistent dialogue agents using human-annotated rewards, introducing the VaRMI method for variance reduction in offline RL, and demonstrating improvements in persona consistency and overall dialogue quality in dialogue models like BlenderBot3 (BB3) through both automatic and human evaluations.

---

# 2 Related Work

## Persona Consistent Dialogue

Persona-based dialogue generation often relies on the PersonaChat dataset. Traditionally, models are fine-tuned on this dataset using supervised learning to maintain persona consistency; however, this approach has limitations concerning consistency. Recent efforts have aimed to enhance consistency by encouraging entailing utterances and discouraging contradictory ones. Strategies include using online reinforcement learning (RL) methods, such as a natural language inference (NLI) classifier, and frameworks like multistage re-writing and Bayesian rational speech acts (RSA). Multistage re-writing struggles with multi-turn persona consistency, while Bayesian RSA faces high computational costs, affecting response times and quality. Unlikelihood training has also been utilized but falls short as it does not explicitly reward entailing utterances.

## Offline RL

The application of offline RL in dialogue tasks mainly focuses on task-oriented dialogue, employing methods like Q-learning. These require additional models for steering dialogue policy efficiently. We propose a policy-gradient based offline RL framework with fixed rewards, simplifying training and deployment by treating it as a modified supervised learning task. Despite the benefits of this method—specifically its simplicity and efficiency—their use has been limited due to variance issues from importance sampling. Prior studies have examined variance reduction, and we contribute to this by introducing VaRMI to enhance offline RL training by reducing importance weight variance.

---

# 3 Method

This section presents an offline reinforcement learning (RL) framework aimed at improving persona consistency and introduces a novel method of importance sampling.

## 3.1 Offline RL

The offline RL training uses a policy-gradient method optimized to the RL objective, formulated as:

$$
J(θ) = E_{τ∼p(πθ(τ))} \left[ \sum_{t=0}^{T}γ^t r(s_t, a_t) \right]
$$

Here, τ represents a trajectory of states $$ s_t $$ and actions $$ a_t $$, and γ is the discount factor. The gradient of the RL objective is derived directly from the policy, allowing updates based on the rewards assigned to dialogue pairs, such as being persona-consistent (reward of 1) or contradictory (reward of -1).

## 3.2 VaRMI Importance Sampling

The VaRMI method addresses the high variance issue in policy-gradient estimates due to importance sampling. By initializing policy πθ using maximum likelihood estimation (MLE), minimal distributional shifts between πθ and the offline behavioral policy πb are assumed, setting $$ w_t $$ importance weights to simplify training:

- $$ w_t = 1 $$ for positive reward candidates.
- $$ w_t = πθ(a_t|s_t) $$ for negative reward candidates.

## 3.3 Framework

The framework involves a critic utilizing human-annotated rewards with dialogue natural language inference (DNLI) and PersonaChat datasets to ensure persona-consistent training samples. The dataset used for training involves combining personas and dialogue contexts to create valid samples while filtering out contradictory persona sets.

## 3.4 Implementation

Implemented on BlenderBot3 (BB3), a state-of-the-art open-domain dialogue system. BB3 was retrained using the proposed methods, where modules for dynamic memory and internet search were disabled to enhance consistency. The overall goal was to improve dialogue performance by focusing responses strictly based on persona information.

This method demonstrates practical improvements for dialogue systems in offline RL settings, particularly in maintaining persona consistency across dialogues.

---

# 4 Experiments

In this section, we evaluate the effectiveness of our offline Reinforcement Learning (RL) framework for ensuring persona consistency in dialogue models. We utilize both automatic and human evaluations to assess the performance improvements.

## 4.1 Evaluation Datasets

### DNLI Evaluation Set

This evaluation set, based on the work by Welleck et al. (2019b), assesses persona consistency in dialogue models. It comprises 542 dialogues derived from the PersonaChat evaluation set and includes utterance candidates with varying levels of entailment: entailing, contradictory, neutral, and the actual next utterance. The goal is to rank gold and entailing utterances highest.

### Mapped DNLI-PersonaChat Dataset

We also evaluate on 5,000 dialogues from this dataset, focusing on how well models encourage entailing utterances and discourage contradictions.

## 4.2 Automatic Evaluation

### Results on Mapped DNLI-PersonaChat Dataset

Figure 3 depicts training loss trajectories for positive and negative utterances. The GOLD method increases loss sensitivity to contradictions but could disincentivize entailing choices. VaRMI training effectively decreases loss on positive candidates and nearly doubles it on negatives, indicating its better performance.

### Results on DNLI Evaluation Dataset

Table 2 compares various models, showing that both GOLD and VaRMI methods outperform the baselines in reducing contradictions and enhancing ranking of gold and entailing utterances. All improvements with offline training were statistically significant, but no significant difference was found between GOLD and VaRMI.

## 4.3 Human Evaluation

We conducted human evaluations involving 90 participants, assigning each to interact with one of our models. The evaluations, calibrated to correct for bias, reveal improvements in persona consistency, particularly with GOLD, but note that VaRMI balances dialogue quality and consistency better.

## 4.4 User Comments and Error Analysis

User feedback highlighted issues such as awkward language and abrupt topic changes. While the GOLD method showed strong persona representation, it sometimes overshadowed conversation quality. VaRMI could achieve a better balance, avoiding these pitfalls.

Overall, results suggest that while persona consistency is essential, care must be taken not to compromise dialogue quality. The optimal balance may vary with the context and length of conversations, where methods like VaRMI show promising results.

---

# 5 Conclusion and Future Work

In this paper, we demonstrated that offline Reinforcement Learning (RL) can effectively enhance the quality and utility of open-domain dialogue systems. By applying offline RL to a persona consistency task, we showed improvements in both persona consistency and dialogue quality compared to systems trained with only imitation learning. We introduced a persona consistency critic, leveraging human-annotated labels, and a novel importance sampling method named VaRMI. Our evaluations, both automatic and human, indicate that our framework successfully enhances the persona consistency of BB3 and improves overall dialogue quality.

For future work, it is promising to extend our framework to address other aspects of open-domain dialogue, such as reducing hallucinations and offensive language. With the capability of large language models (LLMs) to generate high-quality synthetic data, these enhancements can be achieved without the need for additional human conversation data. Moreover, exploring the generalizability of VaRMI to other tasks is worthwhile. Offline policy gradient methods often face high variance issues, and it would be valuable to investigate whether VaRMI can mitigate these issues more effectively across different applications.

---

# 6 Limitations

The major limitation of our framework is the fixed number of training samples. Expanding the dataset requires either gathering more data from human users or generating synthetic data using a large language model (LLM), both of which are more costly than online reinforcement learning methods that can produce unlimited samples without additional cost over time.

Our human experiments were constrained by the size of our language model. Due to resource limitations, we used the 3 billion parameter version of BB3, which is significantly smaller than many state-of-the-art models. The next available version of BB3 is 30 billion parameters, which exceeds our current resource capabilities for training. In future developments, prioritizing larger language models may help alleviate some of the quality issues reported.

---

# 7 Ethical Concerns

Assigning a persona to a language model may lead it to imitate human behavior, potentially misleading users into thinking they are interacting with a human. Such scenarios could result in a language model denying its true nature as a bot if questioned, as acknowledging being a bot might contradict its assigned persona. It is crucial to clearly inform users upfront that they are engaging with a chatbot. In our experiments, we ensured that users were aware they were interacting with a bot and advised them against disclosing any personally identifiable information.
