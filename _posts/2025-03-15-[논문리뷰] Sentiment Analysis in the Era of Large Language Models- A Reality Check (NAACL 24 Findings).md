---
title: "[논문리뷰] Sentiment Analysis in the Era of Large Language Models- A Reality Check (NAACL 24 Findings)"
date: 2025-03-15 16:00:00 +0900
categories:
  - Paper Review
tags:
  - NAACL 2024 Findings
  - Sentiment Analysis
  - Benchmark
---

요약: 이 논문은 대형 언어 모델(LLM)이 기존 감정 분석 작업에서 보여주는 성능을 평가하고, 복잡한 감정 분석에서의 한계를 지적하며, LLMs의 잠재력을 한정된 주석 자원에서의 소수 샘플 학습 환경에서 보여줍니다. 또한 감정 분석 능력을 평가하기 위한 새로운 벤치마크인 SENTI EVAL을 제안합니다.

---

# 1 Introduction

- 감정 분석(Sentiment Analysis, SA)은 자연어 처리(NLP) 분야에서 오랜 역사를 가진 연구 영역임
- SA는 사람의 의견, 감정, 감성을 연구하기 위한 컴퓨터 방법론으로 사용됨 (Liu, 2015; Poria et al., 2020)
- 이 분야는 제품 리뷰 분석 및 소셜 미디어 게시물에서 인사이트 획득 등 다양한 응용 프로그램을 통해 학계 및 산업에서 큰 관심을 끌고 있음 (Barbieri et al., 2020; Zhang et al., 2022)
- 감정 분석을 통한 인간의 주관적 감정의 깊은 이해는 인공지능 일반화(AGI) 개발에 중요한 단계로 여겨짐 (Bubeck et al., 2023)
- 최근 대형 언어 모델(LLMs)은 다양한 NLP 태스크에서 인상적인 성능을 보여줌 (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023)
  - 이들은 zero-shot 또는 few-shot 상황에서 강력한 성능을 달성함
- LLMs를 감정 분석에 적용하려는 초기 시도들이 있었지만, 제한된 특정 작업 및 다양한 모델과 데이터셋 사용으로 인해 현재 상황이 모호함 (Deng et al., 2023; Zhong et al., 2023; Wang et al., 2023)
  
## 연구 목표
- 현재 LLM 시대의 감정 분석 상태를 점검하고자 함
  - 연구 질문: 
    1. 다양한 감정 분석 문제의 성숙도는 어떤가?
    2. 특정 데이터에 훈련된 소형 모델(small models)과 비교할 때 LLM의 성과는 어떤가?
    3. 현재 감정 분석 평가 관행은 LLM 시대에 적합한가?

## 연구 방법
- 13개의 감정 분석 작업과 26개의 데이터셋을 체계적으로 검토
  - 포함된 작업: 일반적인 감정 분류(SC), 세부 측면 감정 분석(ABSA), 주관적 텍스트의 다면적 분석(MAST)
- LLMs에 대해 Flan-T5, Flan-UL2 및 GPT-3.5 (ChatGPT, InstructGPT) 등을 고려
- LLMs의 성과를 작은 언어 모델(SLMs)과 비교함 (모델 매개변수 기준: 3B 미만은 소형, 3B 이상은 대형)

## 주요 발견
- LLMs는 zero-shot 상황에서 강력한 감정 분석 능력을 보임
- 그러나 복잡한 작업에서는 여전히 SLMs에 비해 뒤처짐
- 제한된 주석 데이터 하의 few-shot 설정에서는 LLMs가 지속적으로 SLMs를 능가함

## 평가 한계 및 제안
- 현재 감정 분석 모델의 평가 관행에 몇 가지 한계가 있음
  - 특정 작업이나 데이터셋에만 국한됨
  - 다양한 연구에서 일관성 없는 프롬프트 사용
- 이러한 문제를 해결하기 위한 새로운 벤치마크(SENTI EVAL) 제안
  - 다양한 SA 작업을 포함하여 모델의 종합적 평가가 가능함
  - 다양한 작업 지침을 사용하여 평가 환경을 개선함
  - 실세계 사용 사례에 적합한 평가 환경을 조성함

---

# 2 Background

- **감정 분석(Sentiment Analysis, SA)** 
  - 초기부터 많은 주목을 받으며 활발한 연구 분야로 자리 잡음 (Turney, 2002; Yu and Hatzivassiloglou, 2003; Hu and Liu, 2004)
  - 인간의 주관적인 감정과 의견을 이해하는 것이 인간 수준의 지능 달성에 중요함 (Bubeck et al., 2023)
  - 고객 리뷰 분석 (Keung et al., 2020; Zhang et al., 2022) 및 소셜 미디어 의견 분석 (Yue et al., 2019; Barbieri et al., 2020) 등 폭넓은 실용적 응용 가능

- **SA의 주요 작업**
  - 감정 분류: 주어진 텍스트의 전체 감정 극성 판별 (Turney, 2002)
  - 단면 기반 감정 분석(Aspect-based Sentiment Analysis, ABSA) (Hu and Liu, 2004; Zhang et al., 2022)
  - 주관적 텍스트의 다면적 분석(Multifaceted Analysis of Subjective Texts, MAST) (Liu, 2015)
  - 이러한 작업들은 언어의 감정을 포괄적으로 이해하는 데 기여하며 SA의 다양한 작업 범위를 보여줌

- **대형 언어 모델(Large Language Models, LLMs)**
  - 최근 LLM의 발전, 예: GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), Flan-UL2 (Tay et al., 2022), LLaMA (Touvron et al., 2023), ChatGPT
  - LLM의 SA 작업 평가 초기 시도
    - Zhong et al. (2023): LLM의 제로샷 성능이 미세 조정된 BERT 모델과 유사함
    - Wang et al. (2023): ChatGPT의 극성 변화, 개방형 시나리오 및 감정 추론 문제 처리 능력 연구
    - Zhao et al. (2023): ChatGPT의 감정 대화 능력, 감정적 응답 생성에서 유망한 결과 제시
    - Deng et al. (2023): LLM으로 약한 레이블 생성을 위한 작은 학생 모델의 미세 조정 탐색, 최종 모델이 기존 감독 모델과 동등한 성능 발휘
  - 그러나 기존 노력의 범위는 특정 작업에 국한되며 다양한 데이터 세트 및 실험 설계를 포함함
  - LLM의 SA에 대한 진정한 능력은 여전히 불확실함.

---

# 3 Investigated Tasks and Datasets

- 다양한 SA(Sentiment Analysis) 작업에 대한 광범위한 조사를 실시
- 작업을 세 가지 유형으로 분류:
  - 감정 분류(Sentiment Classification, SC)
  - 요소 기반 감정 분석(Aspect-based Sentiment Analysis, ABSA)
  - 주관적 텍스트의 다면적 분석(Multifaceted Analysis of Subjective Texts, MAST)
- 각 유형의 조사된 작업, 데이터셋 및 평가 지표 간단히 설명
- 세부 설명은 부록 A.1에 포함
- 각 데이터셋에서 원본 테스트 세트에서 최대 500개의 샘플을 추출하여 다양한 작업과 데이터셋에서 균형 보장

---

# 3.1 Sentiment Classification

- 감정 분류(SC)는 주어진 텍스트에 대해 정의된 감정 클래스(예: 긍정, 부정, 중립)를 할당하는 것을 목표로 함 (Liu, 2015).
- SC는 감정을 분석하는 세부 레벨에 따라 세 가지 작업으로 추가 분류됨:
  - **문서 수준 SC**: 영화 리뷰와 비즈니스 리뷰가 포함된 세 개의 널리 사용되는 데이터셋(IMDb, Yelp-2, Yelp-5) 사용.
  - **문장 수준 SC**: 다양한 유형의 의견 텍스트를 포함하는 여러 데이터셋(MR, SST2, SST5, Twitter) 선택.
  - **관점 수준 SC**: 특정 관점이나 개체에 대한 감정을 식별하는 데 초점을 맞춤. 노트북 및 레스토랑 리뷰로 구성된 두 개의 널리 사용되는 데이터셋(Lap14, Rest14) 존재.
- 이 데이터셋들은 다양한 수의 감정 클래스를 포함함.
- SC 작업에 대한 평가 메트릭으로 정확도 점수를 사용함.

---

# 3.2 Aspect-based Sentiment Analysis

- **Aspect-based sentiment analysis (ABSA)**: 사람들의 감정을 더 세부적인 측면 수준에서 분석하는 과정.
  - 다양한 감정 요소의 분석 포함: 
    - 측면 용어
    - 측면 카테고리
    - 의견
    - 감정 극성 (Zhang et al., 2022)

- **세 가지 복합 ABSA 작업**:
  1. **Unified Aspect-based Sentiment Analysis (UABSA)**: 
     - 측면과 그에 대한 감정 극성을 동시에 추출하는 작업.
     - SemEval-2014, SemEval-2015, SemEval-2016에서의 네 개 데이터셋을 평가에 사용.
  
  2. **Aspect Sentiment Triplet Extraction (ASTE)**: 
     - UABSA 작업을 기반으로 의견 용어를 추가로 추출.
     - 특정 측면에 대한 예측된 감정을 설명.
     - Xu et al. (2020)에 의해 소개된 데이터셋 사용.
  
  3. **Aspect Sentiment Quadruple Prediction (ASQP)**: 
     - 완전한 측면 수준 감정 구조 제공: (카테고리, 측면, 의견, 감정) 쿼드러플.
     - 두 개의 음식점 데이터셋 사용.

- **평가 메트릭**:
  - Micro-F1 점수 사용.
  - 예측된 튜플은 모든 감정 요소가 정답 레이블과 정확히 일치해야 정답으로 간주됨.

---

# 3.3 Multifaceted Analysis of Subjective Text

- **다양한 감정 분석**: MAST(Multifaceted Analysis of Subjective Text)는 인간의 주관적 감정의 다양한 측면을 반영하는 과제들로 구성됨.
- **감정 분석 범위**: MAST는 단순히 긍정적 또는 부정적인 감정을 식별하는 것을 넘어서, 더 넓은 범위의 인간 감정 상태를 인식하고 이해하는 데 중점.
- **사용된 데이터셋**:
  1. **Implicit sentiment analysis**: Li et al. (2021)
  2. **SemEval2019 HatEval challenge**: 증오 발언 탐지를 위한 데이터셋 (Basile et al., 2019)
  3. **Subtask 3A of SemEval2018**: Irony detection을 위한 데이터셋 (Hee et al., 2018)
  4. **SemEval2019 OffensEval dataset**: 공격적 언어 식별을 위한 데이터셋 (Zampieri et al., 2019)
  5. **SemEval2016 shared task on Detection Stance in Tweets**: 입장 감지를 위한 데이터셋 (Mohammad et al., 2016)
  6. **CS19 dataset**: 비교 의견 마이닝 작업을 위한 데이터셋 (Panchenko et al., 2019)
  7. **TweetEval benchmark**: 감정 인식을 위한 데이터셋 (Barbieri et al., 2020)
- **평가 방식**: 각 과제에 대해 가장 일반적인 평가 지표를 사용하며, 자세한 내용은 부록 A.1에 제공됨. 평가 지표의 요약은 표 4에 나열됨.

---

# 4.1 Models

- **대형 언어 모델 (LLMs)**  
  - Flan 모델 가족에서 두 가지 모델을 사용:  
    - Flan-T5 (XXL 버전, 13B) (Chung et al., 2022)  
    - Flan-UL2 (20B) (Tay et al., 2022)  
  - 이 모델들은 오픈소스이며 제로샷 및 몇 샷 성능이 뛰어남.  
  - 추론을 위해 Huggingface에 호스팅된 체크포인트 사용.
  - OpenAI의 두 가지 모델도 포함:  
    - ChatGPT (gpt-3.5-turbo3)  
    - text-davinci-003 모델 (text-003, 175B)

- **소형 언어 모델 (SLMs)**  
  - SLMs로 T5 (large 버전, 770M) (Raffel et al., 2020) 사용.  
  - 통합 텍스트-투-텍스트 형식으로 여러 SA 작업을 수행하는 데 뛰어난 성능 발휘.  
  - 특정 작업 디자인 없이 모든 SA 작업에 대해 일관된 SLM을 사용하여 LLMs와의 공정한 비교 가능.  
  - 각 데이터셋에 대해 도메인 특정 데이터로 T5 모델 훈련:  
    - 전체 훈련 세트 또는 몇 샷 설정의 샘플 데이터 사용.  
  - Adam 옵티마이저 사용, 학습률 1e-4, 모든 작업에 대해 고정 배치 크기 4 설정.  
  - 전체 훈련 설정에서 3 에폭, 몇 샷 훈련 설정에서 100 에폭으로 설정.  
  - 안정적인 비교를 위해 SLMs의 두 설정에서 각각 다른 랜덤 시드로 세 번 실행하고 평균 결과 보고.

---

# 4.2 Prompting Strategy

- LLM(대형 언어 모델)은 의미적으로 유사한 프롬프트에 대해서도 매우 다른 응답을 생성할 수 있음.
- 프롬프트에 대한 선호는 LLM마다 다르기 때문에, 이 연구에서는 모든 데이터셋에 대해 상대적으로 일관된 프롬프트를 제공하는 것이 목표임.
- 프롬프트 설계는 간단하고 명확하며 직관적이어야 함.
- 프롬프트에 포함되는 필수 요소:
  - **작업 이름**: 특정 작업의 이름을 명시.
  - **작업 정의**: 각 작업의 정의 및 주석 가이드라인을 기반으로 구성되고, 모델의 응답을 위한 옵션 집합(라벨 공간) 포함.
  - **출력 형식**: 예상 출력 구조를 정의하여 모델의 응답을 원하는 형식으로 디코딩 가능하도록 함.
  
- Few-shot 학습의 경우, 추가적인 "시연" 부분이 포함됨 (점선 박스 내부의 내용).
  - 각 클래스에 대한 k 개의 예시와 그에 따른 금색 레이블 포함.

- 더 자세한 정보와 예시는 Appendix A.6에서 확인 가능.

---

# 5.1 Zero-shot Results

- 다양한 LLM의 제로샷 성능이 테이블 1에 요약되어 있음.
  - 비교를 위해 랜덤 및 다수결 기준선이 포함됨.
  - SLM은 전체 훈련 세트를 사용하여 모델을 훈련한 후 동일한 테스트 세트에서 추론을 진행함.
  
- LLM은 강력한 제로샷 성능을 보임:
  - ChatGPT와 같은 모델이 이항 감정 분류 및 MAST 작업과 같은 간단한 SC 작업을 처리하는 능력이 뛰어남.
  - 예를 들어, ChatGPT는 각 데이터 세트에 대해 전체 훈련 세트로 미세 조정된 T5 모델과 유사한 결과를 얻음.
    - 평균적으로 ChatGPT의 SC 작업 성능은 T5 성능의 97%, MAST 작업에서는 85%에 달함.
  - Flan-UL2는 가장 큰 모델이 아니지만, 여러 작업에서 larger model인 text-003과 비슷하거나 뛰어난 성능을 기록함.

- LLM은 세밀한 구조적 감정 정보를 추출하는 데 어려움을 겪음:
  - Flan-T5 및 Flan-UL2는 ABSA 작업에서 유의미한 성과를 내지 못함.
  - text-003 및 ChatGPT는 더 나은 결과를 제공하나, 미세 조정된 소형 언어 모델에는 미치지 못함.
    - text-003은 ABSA 작업에서 미세 조정된 T5 모델 성능의 약 54%에 해당함.
  
- 일부 SA 작업은 성숙 단계에 도달함:
  - 이항 감정 분류와 같은 일부 SA 작업에서 만족스러운 성과를 달성할 수 있음.
  - 이러한 관찰은 해당 SA 작업들이 어느 정도 성숙에 도달했음을 나타내며, LLMs가 여전히 어려움을 겪고 있는 복잡한 과제에 초점을 맞추어야 함을 시사함.

---

# 5.2 Analysis of Sensitivity on Prompt Design

- 적절한 프롬프트 디자인은 대규모 언어 모델을 특정 작업에 활용할 때 매우 중요함.
- 프롬프트 디자인이 성능에 큰 차이를 초래할 수 있음 (Perez et al., 2021; Lu et al., 2022).
- SA(Sentiment Analysis) 작업에 대한 민감성을 조사하기 위해 각 작업에 대해 추가적으로 5개의 프롬프트를 구성함.
- ChatGPT를 사용하여 성능 변화를 평가하기 위한 실험을 진행함.
- GPT-4 (OpenAI, 2023)를 프롬프트 생성에 사용함, 이는 효과적인 프롬프트 및 지침 생성에 기여함 (Peng et al., 2023).
- 수동으로 작성된 프롬프트의 잠재적 편견을 완화할 수 있음.
- 프롬프트 생성에 대한 세부 사항은 부록 A.2에 기재됨.

- ChatGPT의 5개 다른 프롬프트 결과는 박스플롯 형식으로 Figure 2에 표시됨.
- 다양한 프롬프트의 성능 영향이 작업별로 다름:
  - SC(Sentiment Classification) 작업에서는 프롬프트 선택의 영향이 적음. 
  - ABSA(Aspect-Based Sentiment Analysis) 작업에서는 구조적이고 세분화된 출력이 필요해 프롬프트 디자인에 따라 성능이 크게 변동함.
- SC 작업의 간단함에도 불구하고, 모델은 특정 프롬프트에 민감성을 보이며 일부 SC 데이터셋에서 noticeable outlier가 존재함 (즉, 그림의 원 모양).
- 세부적인 조사 결과, 모델은 특정 단어에 민감하게 반응, 예를 들어 “analyze”와 같은 단어에서 긴 설명을 생성하는 경향이 있음, 명시적으로 지시를 해도 그러함.

---

# 5.3 Few-shot Results

- **목표 및 설정**
  - LLMs(대형 언어 모델)와 SLMs(소형 언어 모델)의 성능 비교를 위해 몇 샷 실험을 수행.
  - 1-shot, 5-shot, 10-shot의 세 가지 K-shot 설정을 고려.

- **실험 방법**
  - 각 감정 유형에 대해 K개의 샘플 추출. ASQP 작업에서는 각 측면 카테고리별로 K개의 예제를 샘플링.
  - 추출된 샘플은 LLM의 인컨텍스트 학습 샘플과 SLM의 훈련 데이터로 사용.

- **결과 요약**
  - Table 2에 실험 결과 요약됨.
  - LLMs는 거의 모든 경우에서 SLMs를 초과하는 성능을 보임.
  - 특히 ABSA(Aspect-Based Sentiment Analysis) 작업에서 LLMs의 우위가 두드러짐.

- **결론**
  - LLMs가 SLMs(예: T5)에 비해 K-shot 설정에서 지속적으로 더 나은 성능을 기록.
  - ChatGPT는 10-shot 설정에서도 강력한 기준을 설정하고, T5는 비슷한 성능을 내기 위해 거의 5배에서 10배의 데이터를 필요로 함.

- **샷 수에 따른 성능 변화**
  - SLMs는 샷 수가 증가할수록 성능이 일관되게 개선됨.
  - 반면 LLMs의 성능 증가는 작업에 따라 다름:
    - SC(감정 분류)와 같은 상대적으로 쉬운 작업에서는 추가 샷의 이점이 불분명.
    - ABSA 작업과 같이 더 깊은 이해가 요구되는 과제에서는 샷 수 증가가 성능을 크게 향상시킴.
    - MAST 작업에서는 추가 예제가 성능 감소를 초래할 수 있음, 이는 예시의 편향 때문일 수 있음.

- **참고 사항**
  - LLMs는 긴 문서에서 사용하기에 제약이 있으며, SLMs가 더 적합할 수 있음.
  - LLMs의 뛰어난 성능이 모든 과제에 적용되는 것은 아님, 과제의 복잡성에 따라 달라짐.

---

# 6 SENTI EVAL Benchmark

- **감정 분석 능력 평가의 재고**
  - LLM의 감정 분석 능력을 평가하기 위한 다양한 실험을 수행함.
  - 기존 평가 방법의 몇 가지 공통적인 결함을 발견.
  - 평가의 포괄성 부족: 현재 많은 평가가 특정 감정 분석 작업이나 데이터 세트에 국한되어 있음.
  - 자연스러운 상호작용 방법의 필요성: 기존 감정 분석 작업이 단일 문장과 감정 레이블로 구성되지만, 사용자의 다양한 작성 스타일 고려 필요.
  - 프롬프트 설계의 민감성: 프롬프트 설계의 변동성이 LLM의 성능에 큰 영향을 미침.

- **SENTI EVAL: 구축**
  - LLM 시대에 적합한 더 나은 감정 분석 평가를 위한 새로운 벤치마크인 SENTI EVAL 제안.
  - 주요 아이디어:
    - 개별 감정 분석 작업 간의 경계를 허물어 통합 테스트 벤치마크 구축.
    - 다양한 스타일로 제공되는 자연어 지침을 사용하여 모델을 테스트.
    - 고정된 다양한 지침 제공하여 성능 비교의 보다 안정성과 신뢰성 확보.

- **SENTI EVAL: 재평가**
  - SENTI EVAL 벤치마크를 기반으로 다양한 LLM의 성능을 재평가.
  - 결과는 정확한 일치 점수로 보고됨.
  - 모델의 성능에 눈에 띄는 차이가 나타남.
  - ChatGPT는 높은 감정 분석 능력과 지침 준수 능력을 보여줌.

- **결론**
  - LLM의 감정 분석 작업에 대한 종합적인 평가를 통해 그 능력을 better 이해함.
  - LLM이 단순 작업에서 좋은 성능을 보이나 복잡한 작업에서 어려움을 겪는 경향을 보임.
  - SENTI EVAL 벤치마크 도입을 통해 현재 평가 관행의 한계를 강조하고 보다 포괄적인 평가 도구를 소개함.

- **제한사항**
  - 13개의 작업과 26개의 데이터 세트를 선택하였으나 모든 감정 분석 관련 작업을 포괄하지 않음.
  - 영어 데이터만 사용하였으며, 언어 및 문화적 배경에 따라 감정 현상이 다르게 나타날 수 있음.

---

# 6.1 Rethinking SA Capability Evaluation

- **현 평가 관행의 공통 결함**: LLM의 감정 분석 역량을 평가하기 위한 광범위한 실험 후, 현재의 평가 방식에서 몇 가지 공통적인 결함을 발견함.
  
- **포괄적인 평가 필요성**:
  - 현재의 대부분의 평가는 특정 감정 분석 작업이나 데이터 세트에 국한됨.
  - 이 같은 평가는 LLM의 감정 분석 능력의 특정 측면에 대한 유용한 통찰을 제공할 수 있지만, 모델의 전체적인 능력을 포착하지 못함.
  - 예를 들어, 감정 분류 능력이 만족스러운 모델이 증오 발언 탐지에서 성능을 보장하지 않음.

- **모델과의 자연스러운 상호작용**:
  - 전통적인 감정 분석 작업은 종종 한 문장과 그에 따른 감정 레이블의 쌍으로 구조화됨.
  - 이는 LLM이 텍스트 생성 모델인 점을 잘 반영하지 않음.
  - 사용자는 다양한 글쓰기 스타일을 보여주며, 이러한 다양성을 평가 과정에서 반영할 필요가 있음.

- **프롬프트 디자인의 민감성**:
  - 프롬프트 디자인의 변동성이 ChatGPT와 같은 모델의 성능에 큰 영향을 미침.
  - 동일한 프롬프트가 모든 모델의 능력을 반영하는 데 보편적으로 적합하지 않을 수 있음.
  - 이러한 편향은 각 연구에서 다양한 프롬프트를 사용하는 경우 더욱 복잡함.

- **SENTI EVAL 구축**:
  - 위의 한계를 완화하기 위해, LLM 시대에서 감정 분석 평가를 위한 새로운 벤치마크인 SENTI EVAL을 제안함.
  - SENTI EVAL의 주요 아이디어:
    - 감정 분석 작업 간 경계를 허물어 통합된 테스트 벤치마크를 설정함.
    - 다양한 스타일로 제시된 자연어 지침을 사용하여 모델을 테스트함.
    - 성능 비교의 안정성과 신뢰성을 높이기 위해 다양한 고정 지침을 제공함.

- **SENTI EVAL 재평가**:
  - SENTI EVAL 벤치마크에 맞춰 다양한 LLM의 평가를 재검토함.
  - 결과는 모델 간의 상대적인 성능 차이를 보여줌.
  - 예를 들어, Flan-UL2는 특정 작업에서 ChatGPT와 유사한 성능을 보였지만, SENTI EVAL에서는 큰 격차가 있음.
  - 이는 SENTI EVAL이 모델이 다양한 스타일의 설명을 이해하도록 요구하기 때문임.

- **전반적인 개선 필요성**:
  - SENTI EVAL 벤치마크는 복잡한 작업, 예를 들어 ABSA 및 MAST 작업에 대한 개선의 여지가 많음을 보여줌.

---

# 6.2 SENTI EVAL: Construction

- LLM의 감정 분석(SA) 능력을 평가하기 위한 새로운 벤치마크인 SENTI EVAL을 제안함.
- SENTI EVAL의 주요 아이디어:
  - 개별 감정 분석 작업 간의 경계를 허물어 통합된 테스트 벤치마크를 구축하여 모델의 감정 분석 능력을 포괄적으로 평가.
  - 다양한 스타일로 제시된 자연어 지침을 사용하여 모델을 테스트, 이는 인간이 모델과 상호작용하는 실제 사례를 모사.
  - 다양한 고정 지침을 통해 서로 다른 LLM과 연구 간의 성과 비교를 보다 안정적이고 신뢰할 수 있게 만듦.
- SENTI EVAL은 12,224개의 데이터 샘플로 구성되며, 각 샘플은 원본 텍스트, 특정 작업에 대한 지침 및 선택적인 몇 가지 예시를 포함.
- 성능 비교를 위한 일관된 벤치마크 설정으로, 프롬프트 변동에 덜 영향을 받도록 함.
- 10개의 후보 프롬프트를 각 작업에 대해 총 10개로 생성하여, 모든 작업의 데이터 샘플마다 무작위로 하나의 프롬프트를 선택하여 모델에 대한 질의를 형성. 
- 50% 확률로 몇 개의 예시를 현재 프롬프트와 함께 제시함. 

이러한 구성은 모델의 감정 분석 능력을 보다 공정하고 포괄적으로 평가하는 데 도움을 줄 것으로 기대됨.

---

# 6.3 SENTI EVAL: Re-evaluate

- SENTI EVAL 벤치마크를 사용하여 여러 LLM의 성능 평가를 재조명
- 결과는 레이블과 예측 간의 정확한 일치를 보여주며, 다양한 작업 유형에 대한 결과도 보고됨
- 다양한 모델의 상대적인 성능 차이를 확인 가능
  - 예) Flan-UL2는 SC 작업에서 ChatGPT와 비슷한 성능을 보였으나 SENTI EVAL에서 큰 차이가 나타남
  - SENTI EVAL에서 모델이 다양한 스타일의 지시사항을 이해해야 하므로 성능 차이가 발생
- ChatGPT는 강력한 성능 기준을 설정하며, SA 역량과 지시사항 준수 능력을 보여줌
- ABSA 및 MAST와 같은 복잡한 작업에서 개선의 여지가 많음
- 향후 LLM의 기본 성능 평가에서 더 세분화된 이해가 필요함

---

# 7 Conclusions

- 본 연구에서는 LLMs를 사용한 다양한 감정 분석 작업에 대한 체계적인 평가를 수행함.
- 실험 결과, LLMs는 제로샷 설정에서 비교적 간단한 작업에서 잘 수행하지만, 복잡한 작업에서는 어려움을 겪음.
- 몇 샷 학습 맥락에서 LLMs는 SLMs를 지속적으로 능가하여 주석 리소스가 부족한 상황에서도 그 가능성을 제시함.
- 현재 평가 관행의 한계를 강조하고, 보다 포괄적이고 현실적인 평가 도구로서 SENTI EVAL 벤치마크를 소개함.

---

# Limitations
- 연구에 사용된 13개의 작업과 26개의 데이터셋은 감정 분석 관련 작업의 포괄적인 목록이 아님.
- 다양한 감정 측면이나 형식에 초점을 맞춘 더 많은 작업을 포함하면 LLM의 강점과 한계를 더 잘 보여줄 수 있음.
- 모든 데이터셋은 영어로 제공되며, 감정 현상은 표현되는 언어 및 문화적 배경과 밀접한 관련이 있음.
- 다른 언어 또는 다국어 환경으로 연구를 확장하면 다양한 언어 및 문화적 맥락에서 LLM의 성능에 대한 포괄적인 이해를 얻을 수 있음.

---

# 독자 의견

- 본 연구는 LLM의 감정 분석 능력을 평가하기 위한 SENTI EVAL 벤치마크를 제안함.
- 본 연구로서 더 다양한 감정에 대한 분석이 가능해질 것임.