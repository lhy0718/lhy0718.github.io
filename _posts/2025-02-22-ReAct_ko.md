---
title: "ReAct: Synergizing Reasoning and Acting in Language Models (Korean)"
date: 2025-02-22 00:00:00 +0900
categories:
  - Agents
tags:
  - Agents
  - LLM
---

요약: 추론 추적(reasoning traces)과 작업별 행동(task-specific actions)을 교차적으로 생성하여 두 가지 간의 더 큰 시너지를 가능하게 하는 방법인 ReAct를 제안한다.

---

# 1. 서론

## 연구의 배경

- 인간의 지능은 **추론**(reasoning)과 **행동**(acting)을 자연스럽게 결합하여 작업을 수행하는 능력을 포함한다.
- 예를 들어, 요리를 할 때 “재료를 다 썰었으니, 이제 물을 끓여야겠다”라고 생각(추론)한 후, 실제로 행동(물 끓이기)한다.
- 이러한 과정에서 우리는 **현재 상황을 추적하고**, **계획을 조정하며**, **필요한 정보를 검색**하는 등 **추론과 행동을 결합**한다.

## 기존 대규모 언어 모델(LLM)의 한계

- **추론(Chain-of-Thought, CoT) 방식**: 모델이 내부 지식만 활용하여 논리를 전개하지만, 외부 정보를 업데이트하지 못해 오류(환각, hallucination)가 발생할 수 있음.
- **행동(Action-only) 방식**: 모델이 외부 환경과 상호작용할 수는 있지만, 복잡한 논리적 사고가 부족하여 비효율적인 정보 검색이 발생함.

## ReAct 접근법의 제안

- **ReAct**(**Re**asoning + **Act**ing)는 LLM이 **추론 과정과 행동을 교차적으로 수행**하는 새로운 방식이다.
- 이를 통해 모델은 다음과 같은 이점을 얻는다.
  - **추론(trace)**: 모델이 현재 진행 상황을 인식하고 조정할 수 있음.
  - **행동(action)**: 외부 환경과 상호작용하여 추가 정보를 수집할 수 있음.
  - **더 나은 의사결정**: 행동을 통해 얻은 정보를 활용하여 보다 정확한 판단을 수행할 수 있음.

## 주요 실험 및 결과

- ReAct는 다양한 작업에서 기존 방법보다 **더 나은 성능과 해석 가능성**을 제공한다.
  - **질문 응답(HotpotQA) 및 사실 검증(Fever)**: 환각 오류를 줄이고, 외부 정보를 활용하여 더 신뢰할 수 있는 답변을 생성.
  - **상호작용 기반 의사결정(ALFWorld, WebShop)**: 강화 학습 및 모방 학습을 뛰어넘는 성능을 보이며, 소수의 예제만으로도 효과적인 학습이 가능.

## 연구 기여

1. 추론과 행동을 결합하는 **ReAct 패러다임** 제안.
2. 다양한 작업에서 ReAct의 **우수한 성능과 해석 가능성**을 실험적으로 입증.
3. 추론이 행동을 돕고, 행동이 추론을 강화하는 **상호보완적 관계**를 분석.
4. 향후 **강화 학습 및 대규모 AI 시스템과의 결합 가능성**을 제시.

---

# 2. `ReAct`

## ReAct 개념 및 원리

- ReAct는 **추론(Reasoning)**과 **행동(Acting)**을 결합하여 **더 효과적으로 작업을 수행할 수 있도록 하는 접근법**이다.
- 기존 방법들은 추론(CoT)과 행동(Action)을 각각 독립적으로 수행했지만, **ReAct는 이를 교차적으로 수행함으로써 상호보완적인 장점을 극대화**한다.

## 기존 접근법의 한계

- **추론(CoT)만 수행하는 방식**

  - 모델이 논리적으로 사고할 수 있지만, 외부 환경과 상호작용할 수 없어 새로운 정보를 업데이트할 수 없음.
  - 결과적으로, **잘못된 정보(환각, hallucination)가 포함될 가능성이 높음**.

- **행동(Action-only) 방식**
  - 모델이 외부 데이터를 검색할 수 있지만, 어떤 정보를 찾아야 하는지에 대한 체계적인 계획이 부족함.
  - 정보 검색이 비효율적이며, 중요한 맥락을 놓칠 가능성이 큼.

## ReAct의 핵심 원리

- **모델의 행동 공간(action space) 확장**

  - 일반적으로 AI 모델은 주어진 입력에 대한 **행동(action)만을 예측**함.
  - ReAct는 행동뿐만 아니라 **자연어 기반의 추론(trace)**도 함께 생성하여 더 나은 의사결정을 가능하게 함.

- **추론과 행동을 교차적으로 수행**
  - **추론 → 행동**: 모델이 먼저 상황을 분석하고 어떤 행동을 할지 결정함.
  - **행동 → 추론**: 모델이 외부 환경과 상호작용한 후, 얻은 정보를 바탕으로 다시 추론을 진행함.

## ReAct의 장점

1. **더 나은 의사결정**
   - 행동을 수행하기 전에 **논리적 사고**를 통해 더 효율적인 행동을 선택할 수 있음.
2. **환각 오류(hallucination) 감소**
   - 외부 환경에서 직접 정보를 검색하여 잘못된 정보를 줄일 수 있음.
3. **해석 가능성(Interpretability) 증가**
   - 모델이 내린 결정을 사람이 쉽게 이해할 수 있도록 함.
4. **다양한 작업에 적용 가능**
   - 질문 응답, 사실 검증, 게임 플레이, 웹 탐색 등 다양한 작업에서 활용할 수 있음.

---

# 3. Knowledge-Intensive Reasoning Tasks

## 실험 개요

- ReAct를 **지식 기반 추론(knowledge-intensive reasoning) 작업**에 적용하여 효과를 평가함.
- 대표적인 두 가지 작업을 선정하여 실험 진행:

  1. **HotpotQA**: 다단계(multi-hop) 질문 응답 시스템
  2. **FEVER**: 사실 검증(fact verification)

- 두 작업 모두 외부 지식(위키피디아 등)을 활용하여 정답을 도출해야 하므로, **추론과 행동의 결합이 중요한 문제**이다.

## 실험 설정

- **데이터셋**

  - **HotpotQA**: 두 개 이상의 위키피디아 문서를 참조해야 정답을 찾을 수 있는 질문 응답 데이터셋.
  - **FEVER**: 주어진 주장(claim)이 참인지 거짓인지 판단하는 데이터셋.

- **ReAct의 동작 방식**

  - 모델은 질문을 받은 후, **(1) 논리적으로 추론(Thought)**하고 **(2) 행동을 통해 정보를 검색(Action)**한 후, **(3) 최종적인 답변을 도출(Final Answer)**하는 구조.
  - 예를 들어, “A와 B가 같은 대학을 나왔나요?”라는 질문이 주어지면:
    1. **추론**: "먼저 A의 학력을 검색해야 한다."
    2. **행동**: A의 위키피디아 페이지 검색 → 결과 확인
    3. **추론**: "이제 B의 학력을 검색해야 한다."
    4. **행동**: B의 위키피디아 페이지 검색 → 결과 확인
    5. **추론**: "A와 B가 같은 대학을 나왔다면 '예', 아니라면 '아니오'를 답해야 한다."
    6. **최종 답변** 제출

- **행동(Action) 공간 정의**
  - 모델이 사용할 수 있는 행동은 아래와 같이 정의됨:
    1. **search[entity]**: 특정 개체(entity)의 위키피디아 페이지 검색
    2. **lookup[string]**: 문서 내 특정 단어 검색
    3. **finish[answer]**: 최종 답변 제출

## 실험 결과

### 1) ReAct vs. 기존 방법 비교

| 방법                                | HotpotQA (정확도) | FEVER (정확도) |
| ----------------------------------- | ----------------- | -------------- |
| **Standard (기본 LLM)**             | 28.7%             | 57.1%          |
| **CoT (Chain of Thought, 추론만)**  | 29.4%             | 56.3%          |
| **CoT-SC (CoT + Self-Consistency)** | 33.4%             | 60.4%          |
| **Act-only (행동만)**               | 25.7%             | 58.9%          |
| **ReAct (추론 + 행동 결합)**        | 27.4%             | 60.9%          |
| **CoT-SC → ReAct (조합 기법)**      | **34.2%**         | **64.6%**      |

- **CoT (추론-only) 방식**이 단순 LLM(Standard)보다 다소 우수했지만, **환각 오류(hallucination)**가 발생하여 신뢰성이 떨어짐.
- **행동-only 방식**은 검색 기능을 활용하지만, 논리적인 사고 없이 단순히 정보를 가져오기 때문에 성능이 낮음.
- **ReAct (추론+행동 결합 방식)**은 FEVER에서 CoT보다 높은 성능을 보이며, **더 정확하고 신뢰할 수 있는 답변을 제공**함.
- 특히 **ReAct + CoT-SC 조합 방식**이 **HotpotQA와 FEVER에서 가장 높은 정확도를 달성**함 → **내부 지식과 외부 정보의 결합이 효과적**임을 입증.

### 2) ReAct의 장점 분석

- **환각 오류(hallucination) 감소**
  - CoT 방식은 내부 지식만 활용하여 잘못된 정보를 생성하는 경우가 많았으나, ReAct는 외부 지식을 검색하여 오류를 줄임.
- **추론 능력 향상**
  - 행동을 통해 정보를 검색함으로써 더 논리적인 사고가 가능해짐.
- **해석 가능성(Interpretability) 증가**
  - 사람이 모델의 답변 과정을 쉽게 따라갈 수 있도록 **추론과 행동을 명확하게 나열**함.

## 결론 및 시사점

1. **ReAct는 지식 기반 추론 작업에서 더 신뢰할 수 있는 답변을 생성함.**
2. **단순히 추론(CoT)만 수행하거나 행동(Action)만 수행하는 방식보다 효과적임.**
3. **내부 지식(CoT)과 외부 지식 검색(Acting)을 결합하면 최상의 성능을 달성할 수 있음.**
4. **미래 연구 방향**
   - ReAct의 행동 방식을 강화 학습(RL)과 결합하여 더 발전시킬 가능성이 있음.
   - 더 많은 훈련 데이터를 사용하여 모델 성능을 추가로 개선할 수 있음.

---

# 4. Decision-Making Tasks

## 실험 개요

- ReAct를 **언어 기반 의사결정 작업(language-based decision-making tasks)**에 적용하여 성능을 평가.
- 두 가지 복잡한 상호작용 환경에서 실험 수행:

  1. **ALFWorld**: 가상 가정환경에서 목표를 수행하는 텍스트 기반 게임.
  2. **WebShop**: 실제 온라인 쇼핑 웹사이트에서 사용자의 요구에 맞는 상품을 찾는 환경.

- 이 두 작업은 **단순한 질문 응답과 달리** 여러 단계를 거쳐 목표를 달성해야 하며, 모델이 **추론(reasoning)과 행동(acting)을 결합**하여 더 효과적으로 수행할 수 있음.

## 실험 설정

- **ALFWorld** (가상 가정 환경 탐색 및 목표 수행)

  - 주어진 목표(예: "싱크대 아래의 종이를 꺼내라")를 달성하기 위해 모델이 여러 단계를 거쳐 수행해야 함.
  - 예시:
    1. **추론**: "싱크대 아래에 종이가 있을 수 있다."
    2. **행동**: "싱크대를 확인한다."
    3. **관찰**: "싱크대 안에 종이가 있다."
    4. **행동**: "종이를 집는다."
    5. **최종 답변**: "종이를 집었다."

- **WebShop** (실제 온라인 쇼핑 시뮬레이션)
  - 주어진 쇼핑 요청(예: "서랍이 있는 니켈 마감의 협탁을 찾으세요")을 만족하는 제품을 찾아야 함.
  - 예시:
    1. **추론**: "‘협탁’이라는 키워드로 검색해야겠다."
    2. **행동**: "‘협탁’을 검색한다."
    3. **관찰**: "몇 가지 협탁이 나타났다."
    4. **추론**: "옵션에서 니켈 마감이 있는지 확인해야겠다."
    5. **행동**: "니켈 마감 옵션이 있는 제품을 선택한다."
    6. **최종 답변**: "적절한 제품을 구매한다."

## 실험 결과

### 1) ReAct vs. 기존 방법 비교

#### ALFWorld (성공률, %)

| 방법                            | Pick | Clean | Heat | Cool | Look | Pick 2 | 전체 평균 |
| ------------------------------- | ---- | ----- | ---- | ---- | ---- | ------ | --------- |
| **Act-only (행동만)**           | 88   | 42    | 74   | 67   | 72   | 41     | 45        |
| **ReAct (추론 + 행동 결합)**    | 92   | 58    | 96   | 86   | 78   | 41     | **71**    |
| **BUTLER (강화학습 기반 모델)** | 46   | 39    | 74   | 100  | 22   | 24     | 37        |

- ReAct는 ALFWorld에서 **행동만 수행하는 방식(Act-only)보다 훨씬 높은 성공률(71% vs. 45%)**을 기록.
- BUTLER(강화 학습 기반 모델)보다도 높은 성능을 보임.
- 주요 원인: ReAct는 **현재 상태를 추론하고 행동을 결정하는 능력**을 가지고 있어, **더 논리적으로 목표를 달성할 수 있음**.

#### WebShop (성공률, %)

| 방법                             | 평균 점수(Score) | 성공률(Success Rate) |
| -------------------------------- | ---------------- | -------------------- |
| **Act-only (행동만)**            | 62.3             | 30.1                 |
| **ReAct (추론 + 행동 결합)**     | **66.6**         | **40.0**             |
| **IL (모방 학습 모델)**          | 59.9             | 29.1                 |
| **IL+RL (모방 + 강화학습 모델)** | 62.4             | 28.7                 |
| **Human (인간 전문가 성능)**     | **82.1**         | **59.6**             |

- ReAct는 WebShop에서 Act-only 및 IL/IL+RL 모델보다 훨씬 높은 성능을 기록함.
- 인간 전문가(59.6%)에는 미치지 못하지만, 가장 높은 자동화된 모델 성능(40.0%)을 달성.
- 주요 원인: ReAct는 단순히 행동을 반복하는 것이 아니라, **추론을 통해 필요한 정보를 필터링하고 적절한 행동을 결정하는 능력**을 가짐.

## ReAct vs. 기존 모델

- 기존 모델들은 **내부 추론 없이 행동만 수행**하여, **불필요한 행동 반복** 및 **비효율적인 탐색**을 수행함.
- ReAct는 행동과 추론을 결합하여 **더 체계적이고 논리적인 탐색이 가능**.
- **기존의 강화학습(RL) 모델보다도 높은 성능을 보이며, 일반적인 텍스트 기반 환경에서도 활용 가능**.

## ReAct의 핵심 장점

1. **더 효과적인 목표 달성**
   - 추론을 통해 **현재 상태를 분석하고, 필요한 정보를 필터링하며, 최적의 행동을 선택**함.
2. **불필요한 행동 최소화**
   - 단순한 탐색이 아니라, **목표에 맞는 행동을 유도**하여 성능을 향상시킴.
3. **일반화 가능성 증가**
   - ALFWorld와 WebShop 등 **다양한 환경에서 강한 성능을 보이며**, 인간과 유사한 사고 방식을 적용할 수 있음.
4. **강화학습 없이도 높은 성능**
   - 많은 데이터를 필요로 하는 강화학습(RL) 없이도, **소수의 예제만으로 뛰어난 성능을 달성**.

## 결론 및 시사점

1. **ReAct는 단순 행동 모델보다 훨씬 높은 성능을 기록하며, 다양한 작업에 적용 가능함.**
2. **추론과 행동을 결합함으로써, 강화학습 없이도 효과적인 의사결정을 수행할 수 있음.**
3. **향후 연구 방향**
   - 더 복잡한 환경에서 ReAct를 적용하여 실험 확장 가능.
   - 강화학습과 결합하여 더욱 강력한 모델 개발 가능.
   - 실제 웹 검색 및 로봇 제어 등 다양한 응용 분야에서 활용 가능.

---

# 5. Related Work

## 언어 모델을 활용한 추론

- 최근 연구들은 **대규모 언어 모델(LLM)이 복잡한 추론 작업을 수행할 수 있음을 보여줌**.
- 대표적인 방법:
  1. **Chain-of-Thought (CoT)** (Wei et al., 2022)
     - 모델이 **추론 과정(reasoning steps)을 언어 형태로 표현**하여 더 복잡한 문제를 해결하도록 유도.
     - 하지만 **내부 지식만을 사용하므로 잘못된 정보(환각, hallucination)를 생성할 위험이 있음**.
  2. **Least-to-Most Prompting** (Zhou et al., 2022)
     - **복잡한 문제를 더 작은 단위로 분할하여 해결**하는 방식.
  3. **Faithful Reasoning (Creswell & Shanahan, 2022)**
     - 모델이 더 신뢰할 수 있는(reasoning-faithful) 방식으로 추론할 수 있도록 설계.
  4. **Scratchpad (Nye et al., 2021)**
     - 모델이 중간 계산 과정을 기록하도록 하여 더 정확한 정답을 유도.

### ReAct의 차별점

- **기존 방법은 모두 추론 과정만을 다루고 있으며, 외부 환경과의 상호작용을 고려하지 않음**.
- 반면, **ReAct는 추론뿐만 아니라 행동(acting)까지 결합하여, 필요할 경우 외부에서 정보를 검색하고 행동을 수행할 수 있도록 함**.

## 언어 모델을 활용한 의사결정

- 최근 연구들은 LLM을 활용하여 다양한 의사결정 및 행동 계획(task planning) 문제를 해결하려는 시도를 하고 있음.
- 대표적인 연구:
  1. **WebGPT (Nakano et al., 2021)**
     - LLM이 웹 브라우저를 탐색하여 정보를 검색하고 질문에 답변하는 방식.
     - 하지만 **명확한 추론 과정 없이 행동만 수행**하는 방식이므로 논리적 사고 부족.
  2. **SayCan (Ahn et al., 2022)**
     - 로봇이 **언어 모델을 활용하여 행동을 계획**하고, 실제 환경에서 실행할 수 있도록 학습.
  3. **Inner Monologue (Huang et al., 2022b)**
     - 로봇이 수행한 행동에 대한 내부 피드백(inner monologue)을 활용하여 행동을 조정.
     - 하지만 **실제 사고(thinking) 과정이 아니라 환경에서 받은 피드백을 단순히 언급하는 수준**.

### ReAct의 차별점

- 기존 연구들은 행동을 수행할 수 있도록 모델을 학습하지만, **명확한 추론(reasoning) 과정이 부족하거나, 외부 환경과의 상호작용이 제한됨**.
- ReAct는 **추론과 행동을 유기적으로 결합**하여, 필요한 정보를 검색하고 이를 바탕으로 더 논리적인 의사결정을 수행할 수 있음.

## 강화학습 및 인터랙티브 AI

- **강화학습(RL)**을 활용한 연구들은 LLM이 외부 환경과 상호작용하는 방법을 학습하는 데 집중.
- 대표적인 연구:
  1. **BUTLER (Shridhar et al., 2020b)**
     - 가상 환경에서 강화학습 기반의 행동 모델을 학습.
     - 하지만 대규모 데이터를 필요로 하고, 일반화 능력이 부족함.
  2. **Interactive Agents (Abramson et al., 2020)**
     - 인간과 상호작용하는 AI 모델을 강화학습으로 학습.
  3. **Generalist Agent (Reed et al., 2022)**
     - 하나의 AI 모델이 여러 가지 작업을 수행할 수 있도록 설계.

### ReAct의 차별점

- 기존 강화학습(RL) 모델들은 **대규모 데이터와 긴 학습 시간이 필요**함.
- ReAct는 **소수의 예제만으로도 효과적인 의사결정을 수행할 수 있음 (few-shot learning 가능)**.
- 또한, 강화학습과 달리 **추론과 행동을 자연스럽게 결합할 수 있어, 보다 직관적인 방식으로 모델을 제어 가능**.

## 결론

1. **ReAct는 기존 연구들과 달리, 추론(reasoning)과 행동(acting)을 결합하여 더 효과적인 문제 해결이 가능함.**
2. **기존 CoT 방식(추론 중심)과 WebGPT 방식(행동 중심)의 단점을 극복하고, 두 가지를 조화롭게 결합한 새로운 접근법을 제시.**
3. **향후 연구에서는 ReAct를 강화학습과 결합하여 더 강력한 AI 시스템을 개발할 가능성이 있음.**

---

# 6. Conclusion

## 연구 요약

- 본 연구에서는 **ReAct**라는 새로운 방법론을 제안하여, **대규모 언어 모델(LLM)이 추론(reasoning)과 행동(acting)을 결합하여 문제를 해결할 수 있도록 함**.
- 기존 방식(추론-only, 행동-only)과 비교하여, ReAct는 **더 신뢰할 수 있고 해석 가능한 방식으로 작업을 수행할 수 있음**.
- ReAct는 다음과 같은 다양한 작업에서 우수한 성능을 보임:
  - **질문 응답 (HotpotQA)**: 정보 검색을 통해 더 정확한 답을 도출.
  - **사실 검증 (FEVER)**: 환각 오류(hallucination) 감소 및 신뢰성 향상.
  - **상호작용 기반 의사결정 (ALFWorld, WebShop)**: 목표 지향적인 행동 계획 및 성공률 증가.

## ReAct의 핵심 기여

1. **추론과 행동의 결합**
   - 기존 방법과 달리, ReAct는 **언어 모델이 직접 추론을 수행하면서, 필요한 경우 외부 환경과 상호작용할 수 있도록 설계**.
2. **환각 오류(hallucination) 감소**
   - 내부 지식에만 의존하는 기존 CoT 방식과 달리, ReAct는 외부 데이터를 활용하여 사실성을 높임.
3. **소수 예제(few-shot learning)에서도 우수한 성능**
   - 많은 데이터를 필요로 하는 강화학습(RL) 방식과 달리, **소수의 예제만으로도 효과적인 의사결정이 가능**.
4. **다양한 작업에 적용 가능**
   - 질문 응답, 웹 검색, 로봇 행동 계획 등 다양한 환경에서 활용 가능.
5. **해석 가능성과 신뢰성 향상**
   - 모델의 행동을 사람이 쉽게 분석하고 조정할 수 있음.

## 향후 연구 방향

- **ReAct를 다양한 환경에서 확장**
  - 더 복잡한 상호작용 환경(예: 물리적 로봇, 실제 웹 탐색 등)에서의 적용 가능성 탐색.
- **ReAct와 강화학습(RL)의 결합**
  - ReAct의 추론 및 행동 방식을 강화학습과 결합하면, **더 강력한 AI 에이전트 개발 가능**.
- **더 나은 프롬프트 설계 및 모델 학습**
  - 더 많은 인간 주석 데이터를 활용하여, ReAct 모델을 미세 조정(finetuning)하여 성능을 더욱 개선할 수 있음.

---

# 독자 의견

- ReAct 구조는 LLM의 추론에 환경에 대한 피드백을 더함으로써 LLM의 capacity를 증가시킴.
- 행동 및 관찰에서의 사실 관계에 대한 추가적인 검증 필요해보임.
- 외부 데이터가 희박한 경우나, 또는 외부 데이터를 얻는 과정이 복잡하여 지연시간 및 병목이 존재하는 문제상황에 대한 해결방안이 필요함.
- 또한 위 경우에 대한 벤치마크 또한 필요함.
