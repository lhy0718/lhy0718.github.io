---
title: "[논문리뷰] Code Models are Zero-shot Precondition Reasoners (NAACL 2024)"
date: 2025-05-27 12:32:33 +0900
categories:
  - Paper Review
tags:
  - NAACL 2024
  - Decision Making
---

이 연구는 코드 표현을 활용해 행동의 선행 조건을 추론하고, 이를 바탕으로 정책의 행동 샘플링을 선행 조건에 맞게 조정하여 작업 지향 대화 및 텍스트월드 환경에서 적은 학습으로도 성능을 향상시키는 방법을 제안한다.

---

# 1 INTRODUCTION

- 순차적 의사결정 환경에서 최적 에이전트 정책 학습의 핵심은 다양한 행동의 타당성을 이해하는 것임.
- 예를 들어, 음식점 추천 대화 에이전트는 위치와 음식 종류 같은 기본 정보가 필요하며, 이를 바탕으로 데이터베이스 조회 가능.
- 행동 수행에 필요한 조건(예: 위치와 음식 종류)은 문헌에서 사전조건 추론(precondition inference) 또는 가능성 학습(affordance learning)이라고 함(Ahn et al., 2022; Sohn et al., 2020).
- 사전조건에 대한 지식은 정책 학습 및 안전성 적용에 광범위한 영향을 미침.
- Few-shot 프롬프트 기반 대형 언어 모델(LLMs)은 작업 계획(task planning)에서 강력한 능력을 보였으나(Logeswaran et al., 2022; Huang et al., 2022a), 행동 사전조건 추론에 대한 체계적인 메커니즘은 부족함.
- 일부 연구들은 코드 어서션(assertion) 또는 자연어 근거(rationale)를 예제로 제공해 LLM에게 사전조건을 학습시키려 시도함(Singh et al., 2022; Huang et al., 2022b).
- 하지만 이러한 사전조건이나 근거는 추론 중 동적으로 생성되어 이를 검증하거나 행동과의 일관성을 보장하기 어려움.
- 고전 AI 계획 문헌(Aeronautiques et al., 1998; Fikes & Nilsson, 1993)과 코드 표현을 활용한 최근 연구에서 영감을 받아, 우리는 코드 표현을 통해 사전조건을 추론함.
- 프로그램은 이벤트 시퀀스 모델링에 적합하며, 어서션 형태로 이벤트 간의 의존성 제약 조건을 표현할 수 있음(Liang et al., 2022).
- 프로그램이 어서션을 만족하는지 확인하는 것은 프로그램을 실행해 성공적으로 동작하는지 검증하는 과정과 같음.
- 코드 표현은 자연어 등 다른 표현 방식에 비해 실행 및 제약 만족도 검증이 용이함.
- 코드의 절차적 구문으로 표현된 사전조건은 투명성, 통제 가능성, 그리고 신경망 함수와 같은 다른 가능성 표현보다 미지의 상황에 대한 일반화 능력을 향상시킴.
- 또한, 코드 이해 모델이 가진 강력한 사전지식을 정책 학습 문제에 활용할 수 있음.

- 본 연구에서는 다음과 같은 방법과 기여를 제시함:
  - 전문 데모(전문가 시연)로부터 사전조건을 추출하는 문제를 코드 완성 문제로 정식화하고, 사전학습된 코드 모델을 이용한 제로샷(precondition inference zero-shot) 방법을 제안함.
  - 추론한 사전조건을 활용한 사전조건 인지(action prediction) 정책 학습 기법을 도입하여, 사전조건과 일관성 있는 행동 예측을 보장함.
  - 제안한 접근법은 Task-oriented dialog와 Embodied textworld 벤치마크에서 기존 방법들 대비 우수한 성능을 보임.


---

# 2 PROBLEM SETTING

- 순차적 의사결정 환경에서 에이전트는 관찰값 $$o_i \in O$$를 받고 행동 $$a_i \in A$$를 수행함.
- 관찰 공간과 행동 공간은 이산적(discrete)임.
- 에이전트의 궤적(trajectory)은 관찰과 행동의 시퀀스 $$\tau = (o_1, a_1, o_2, a_2, \ldots, o_n, a_n)$$로 표현됨.
- Few-shot 학습 환경을 고려하며, 데모 시퀀스 $$D = \{\tau_1, \ldots, \tau_n\}$$가 주어짐.
- 히스토리(history)를 $$\tau_{<t} = (o_{1:t}, a_{1:t-1})$$로 정의함.
- 학습 목표는 두 가지임:
  - 각 행동 $$a$$에 대해, 해당 행동이 주어진 맥락 $$\tau_{<t}$$에서 타당한지 여부를 나타내는 전제조건 함수 $$g_a(\tau_{<t}) \in \{0,1\}$$를 추정함.
    - $$g_a(\tau_{<t}) = 0$$이면 해당 행동은 타당하지 않음.
    - $$g_a(\tau_{<t}) = 1$$이면 해당 행동은 타당함.
  - 추정된 전제조건 집합 $$G = \{g_a \mid a \in A\}$$을 바탕으로, 정책 함수 $$\pi(a_t \vert \tau_{<t}, D, G)$$를 추정하여 다음 행동을 예측함.
- 궁극적인 목표는 테스트 궤적 집합 $$D_{\text{test}}$$에 대해 정책 $$\pi$$가 일반화되도록 하는 것임:
  
  $$
  \max \mathbb{E}_{\tau \in D_{\text{test}}} \left[ \log \pi(a_t \vert \tau_{<t}, D, G) \right]
  $$

---

# 3 APPROACH

- 섹션 3.1에서는 코드 표현 방법에 대해 논의한다.
- 섹션 3.2에서는 전제 조건 추론(precondition inference) 문제에 대한 접근 방식을 설명한다.
- 섹션 3.3에서는 행동 예측(action prediction) 문제에 대한 접근 방식을 다룬다.
- 전체적인 접근 방법에 대한 개요는 그림 1에서 확인할 수 있다.

---

# 3.1 REPRESENTING AGENT TRAJECTORIES AS PROGRAMS

- 에이전트의 환경과의 상호작용을 프로그램으로 표현함.
- 모든 행동(action)과 관찰(observation)을 미리 정의된 변수 집합 $$v$$을 수정하는 하나 이상의 함수 호출로 표현.
  - 변수 $$v$$는 에이전트의 경험(예: 관찰 $$o_{1:t}$$ 및 행동 $$a_{1:t-1}$$)의 요약을 담음.
- 이 표현은 이산적(discrete) 관찰 공간과 행동 공간을 가정하기 때문에 가능.
  - 예를 들어, 관찰 공간 $$O$$ 및 행동 공간 $$A$$의 각 문자열마다 별도의 함수를 정의할 수 있음.
- 관찰에 대응하는 함수 집합 $$F_O$$, 행동에 대응하는 함수 집합 $$F_A$$ 가 정의되어 있다고 가정.
- 이에 따라 한 궤적(trajectory) $$\tau$$는 함수 호출들의 순서(sequence)로 이루어진 프로그램으로 간주 가능.
- 논문 내 Figure 1과 부록(Appendix)에 예시 프로그램들이 제시되어 있음.

---

# 3.2 PRECONDITION INFERENCE

- **전제조건 함수 개념**  
  - 전제조건 함수 $$g_a(\tau_{<t}) \in \{0,1\}$$는 주어진 컨텍스트 $$\tau_{<t}$$ 내에서 행동 $$a$$가 타당한지를 나타냄.
  - 프로그램 표현 내 대응 함수 $$f_a \in \mathcal{F}_A$$에서는 변수 $$v$$ (→ $$\tau_{<t}$$ 요약)와 함수 인자 관점에서 assertion 문(assert 문)을 식별하려고 함.
  - 예: Figure 1의 FindRestaurants 행동 전제조건은  
    ```assert user.informed['city'] and user.informed['cuisine']``` 로 표현됨.
  - 각 행동 $$a \in A$$에 대해 독립적으로 전제조건을 예측하며, 아래 과정이 각 행동에 반복 적용됨.

- **전제조건 추론 방법**  
  1. 후보 생성 (Candidate Generation)  
  2. 후보 검증 (Candidate Validation)  
  3. 후보 순위 매김 (Candidate Ranking)

---

### 후보 생성 (Candidate Generation)

- 시연 궤적 집합 $$\mathcal{D}$$ 주어지면, 사전 학습된 코드 생성 모델에 프롬프트를 주어 전제조건 후보를 생성함.
- 프롬프트는 다음 세 부분 포함:  
  1) 시연 궤적 $$\tau \in \mathcal{D}$$  
  2) 함수 집합 $$\mathcal{F}_O$$ 정의  
  3) assert 키워드 포함 $$f_a$$ 함수 정의  
- assert 키워드는 모델이 assertion 문장을 생성하도록 강제함(임의 코드가 아니라).  
- 시연 프로그램을 다양화하고 여러 후보를 샘플링하여 초기 후보 풀 $$H^{\text{initial}}_a$$ 생성.  
- 사전 학습 모델은 함수 사용 맥락에 대한 이해를 바탕으로 적절한 assertion 문장을 생성하도록 기대됨.
- 한계점:  
  - 생성 문장에 문법 오류 등으로 의미 없는 문장이 포함될 수 있음.  
  - 후보가 정적 프로그램 분석에 기반해 생성되어, 실행과 상태 변화에 대한 암묵적 추론이 요구됨.

---

### 후보 검증 (Candidate Validation)

- 프로그램 표현의 핵심 장점: 실행 가능성  
- 각각의 후보 assertion을 적용하여 시연 프로그램들이 정상 실행되는 지 검증함.  
  - 함수 몸체를 후보 assertion으로 교체 후 실행 시도.  
- 실행 실패한 후보는 버려지고, 남은 후보들이 $$H^{\text{valid}}_a$$로 선정됨.  
- 단순하고 쓸모없는 후보(예: `assert True`)도 포함될 수 있으므로 추후 추가 조치 필요.

---

### 후보 순위 매김 (Candidate Ranking)

- 유효 후보 $$H^{\text{valid}}_a$$ 중에서 정책 구축에 유용한 소수의 전제조건 후보를 선별함.  
- 핵심 아이디어:  
  - 두 후보 $$h_1, h_2$$가 있을 때, 만약 $$h_1$$가 만족되면 항상 $$h_2$$가 만족된다면(즉, $$h_1 \Rightarrow h_2$$)  
  - $$h_1$$이 더 바람직하며, 더 상황을 잘 구분(discriminative)한다고 판단.  
- 실제로 $$h_1 \Rightarrow h_2$$ 성립 여부 검증은 일반적으로 비현실적(intractable)임.  
- 따라서 시연 궤적 내 발생하는 상황만 살펴보는 근사 방법 적용:  
  - 전제조건 함수 $$g_a(\cdot; h)$$에서 $$h$$가 전제조건일 때,  
  - $$C^h_a = \{(i,j) \mid g_a(\tau_i^{<j}; h) = 1, \tau_i \in \mathcal{D}\}$$ 를 만족 시점의 집합으로 정의.  
  - $$h_1 \Rightarrow h_2$$를 근사하여 $$C^{h_1}_a \subseteq C^{h_2}_a$$ 인지 판단.  
- 최적 후보 집합 $$\displaystyle H^{\text{opt}}_a$$는 다음과 같이 정의됨:

  $$
  H^{\text{opt}}_a = \{h \in H^{\text{valid}}_a \mid \nexists h' \in H^{\text{valid}}_a \text{ s.t. } C^{h'}_a \subset C^h_a \}
  \tag{1}
  $$

- 동등한 후보가 여러 개 있을 경우 랜덤하게 한 개만 선택해 $$H^{\text{opt}}_a$$에 포함.  
- $$H^{\text{opt}}_a$$ 내 assertion들의 논리적 결합은 $$H^{\text{valid}}_a$$ 전체와 동일.  
- 소수의 assertion 선별은 해석 용이성과 제한된 문맥 길이 모델에 대한 프롬프트 공급 측면에서 장점.

---

# 3.3 PRECONDITION -AWARE ACTION PREDICTION

- 행동 예측을 코드 완성 문제로 제시함.
  - 부분적인 에이전트 궤적이 주어지면,
  - 코드 모델이 가능한 다음 행동(함수 집합 $$F_A$$에서 적절한 인자와 함께)을 제안하는 과제임.
  
- 과거 행동과 관찰을 기준으로 다음 행동을 예측하는 임의의 정책이 주어졌을 때,
  - 사전조건(precondition) 지식을 활용하여 정책을 보완하는 간단한 방법을 고려함.

- 다음 행동 후보는 정책에서 샘플링됨.
  - 사전조건과 일치하는 행동이 발견되거나 최대 시도 횟수에 도달할 때까지 반복함.
  - 첫 시도는 탐욕적 샘플링(greedy sampling)을 사용하고,
  - 이후 시도는 무작위 샘플링(random sampling)을 수행함.
  - 무작위 샘플링에서도 조건에 맞는 행동이 없으면, 탐욕적 행동으로 기본 설정함.

- 행동 샘플링은 토큰을 생성하여 줄바꿈 토큰(newline token)이 나올 때까지 진행함.

- 자세한 알고리즘 의사코드는 부록 E에 수록됨.

---

# 4 EXPERIMENTS

- 평가에서 두 가지 주요 질문에 답하려고 시도함:
  1. 에이전트 행동 시연에서 행동의 전제조건(preconditions)에 대한 정보를 추출할 수 있는가?
  2. 추론된 전제조건 정보가 더 나은 에이전트 정책(policy) 구축에 유용한가?

- 실험에서 코드 표현으로 파이썬 언어를 사용:
  - 단순성과 인기
  - 최근 주로 파이썬에 집중한 코드 모델의 출시 때문

- 사용한 오픈소스 사전학습 코드 모델:
  - CodeGen 2B (Nijkamp et al., 2022)
  - StarCoder 16B (Li et al., 2023)

- 이 모델들은 초기 다중 프로그래밍 언어 사전학습 후 파이썬 코드에 대해 추가 사전학습된 버전을 사용함

---

# 4.1 BENCHMARKS

- **Task Oriented Dialog Benchmark**
  - 목표 지향 대화 시스템의 의사결정 구성요소인 대화 매니저는 대화 행위(dialog acts)로 표현된 발화 시퀀스를 입력으로 받아 다음 행동을 예측함.
  - 대화는 사용자와 시스템(에이전트)이 번갈아 가며 발화하며, 사용자의 발화가 에이전트의 관찰이 됨.
  - 실험에는 SGD 데이터셋(Rastogi et al., 2020)을 사용.
  - 데이터셋에는 11개의 사용자 행위와 11개의 시스템 행위가 정의되어 있으며, 각 행위는 인자 없이, 단일 슬롯 인자 또는 단일 의도 인자를 가짐.
  - 각 행위에 대응하는 함수 FO(관찰 함수)와 FA(행동 함수)를 객체 지향 방식으로 사용자 클래스와 시스템 클래스로 그룹화.
  - 각 사용자 행동에 대해 해당 행동이 수행되었는지 기록하는 변수 v 정의 (예: informed_slot[], requested_slot[]).
  - SGD 데이터셋의 10개 도메인(스키마)에서 실험 진행.
  - 10개 인스턴스는 데모용, 50개 인스턴스는 테스트용으로 사용.
  - 평가를 위해 각 시스템 행동의 진실 조건(precondition)을 대화 행위에 대한 사전 지식을 바탕으로 수작업으로 정의.
  - 주의: 진실 조건은 평가용이며, 모델 학습 시 감독 신호로 사용되지 않음.

- **Embodied Textworld Benchmark**
  - ALFworld(Shirdhar et al., 2020)라는 멀티모달 embodied textworld 벤치마크를 사용.
  - 에이전트가 환경과 상호작용하여 물체 조작 작업 수행.
  - 관찰과 행동은 자연어 문장으로 표현되고, 에이전트는 자연어 지시문(예: "move the keys to the table")에 따라 작업 수행.
  - 9가지 유형의 행동이 있으며, 이들은 상호작용 및 이동 관련 행동.
  - 각 행동 타입은 객체 인자와/또는 용기(receptacle) 인자 가짐. 이에 상응하는 함수 FA 정의.
  - 보조 함수 FO도 정의하여 에이전트 인벤토리의 객체 추가/제거 및 에이전트가 볼 수 있는 객체 집합 상태 업데이트 담당.
  - 에이전트 경험을 요약하는 변수 3개 정의: 
    - 보이는 객체 집합, 
    - 에이전트 인벤토리, 
    - 환경 내 객체 상태(예: 열림/닫힘).
  - 데이터셋에는 6가지 작업 유형이 포함됨.
  - 각 작업 유형당 2개 인스턴스를 데모로 사용하고, 벤치마크의 표준 테스트 세트로 평가.
  - 벤치마크는 PDDL(Planning Domain Definition Language) 형식의 행동 진실 조건을 제공하며, 평가를 위해 이를 Python assertion 문으로 변환하여 사용.
  - 전체 프로그램 사양은 부록 A에 제공.

- **표 1 해석 (Buses 도메인, SGD 벤치마크)**
  - 표 1은 각각의 행동에 대한 진실 조건 (ground-truth preconditions)과 예측된 진실 조건을 보여줌.
  - 예: 
    - $$$INFORM(self, slot): \; assert \, self.user.requested\_slot[slot]$$$
    - $$$REQUEST(self, slot): \; assert \, \neg self.user.informed\_slot[slot]$$$
  - 각 행동마다 Precision과 Recall 지표를 통해 예측 성능 평가.
  - 일부 행동(예: OFFER_INTENT)에서는 낮은 정밀도 또는 재현율을 보임.

- **요약**
  - 이 챕터에서는 실제 대화와 환경 상호작용을 모델링하는 두 가지 벤치마크를 사용하여 작업 지향 대화 및 embodied textworld에서의 모델 성능 평가 방식을 설명.
  - 진실 조건 기반 평가를 위해 각 벤치마크 별로 독립적으로 함수와 변수들을 정의하고, 평가를 위한 ground-truth precondition을 수작업으로 준비함.
  - 자연어 기반 행동 및 관찰과 객체지향 함수 정의를 연계하여 모델의 의도 인식 및 행동 선택 능력을 실험함.

---

# 4.2 PRECONDITION INFERENCE

- **정의 및 지표**
  - 행동 $$a \in A$$의 전제조건 $$g_a(\tau_{<t}) \in \{0,1\}$$는 컨텍스트 $$\tau_{<t} = (o_{1:t}, a_{1:t-1})$$에서 해당 행동이 가능함을 나타냄.
  - 테스트 궤적의 시간 단계 $$j$$에서 행동 $$a$$의 전제조건이 만족되는 인스턴스 집합을 
    $$C_a = \{(i, j) \mid g_a(\tau_i < j) = 1, \tau_i \in D_{\text{test}}\}$$ 
    으로 정의.
  - 행동 $$a$$에 대한 정밀도(Precision)와 재현율(Recall)은 다음과 같이 정의됨:
    $$
    \text{Prec} = \frac{|C_a^{\text{pred}} \cap C_a^{\text{gt}}|}{|C_a^{\text{pred}}|}, \quad
    \text{Rec} = \frac{|C_a^{\text{pred}} \cap C_a^{\text{gt}}|}{|C_a^{\text{gt}}|}
    $$
    여기서 $$C_a^{\text{pred}}$$, $$C_a^{\text{gt}}$$는 각각 예측된 전제조건 집합과 실제 전제조건 집합.
  - F1 스코어는 정밀도와 재현율의 조화평균이며, 모든 행동에 대해 매크로 평균하여 최종 지표 산출.

- **베이스라인 방법**
  - **Neural Precondition baseline**: $$(\tau_{<t}, a, l)$$ 형태의 라벨 데이터가 있을 때, 이진 분류기 $$h_a$$를 학습하여 행동의 전제조건 만족 여부를 예측.
  - 그러나 실제로는 긍정적 사례만 존재: 
    $$
    D_{\text{pos}} = \{ (\tau_{<t}, a_t, 1) \mid \tau \in D, t \le n \}
    $$
  - 부정적 사례는 궤적에서 실제 행동이 아닌 미래 행동들은 전제조건 미충족 가정 하에 생성:
    $$
    D_{\text{neg}} = \{ (\tau_{<t}, a_{t'}, 0) \mid t' > t, a_{t'} \neq a_t, \tau \in D \}
    $$
  - $$D_{\text{pos}}$$와 $$D_{\text{neg}}$$를 균등 샘플링하여 분류기를 학습.
  - **Prompting baseline**: 코드 생성 모델에 부분적인 프로그램 명세를 입력하여 assertion 문을 예측하고, 이를 탐욕적 디코딩으로 수행.

- **실험 결과 (Table 2)**
  - 제안 모델은 높은 정밀도 달성; 본 논문이 채택한 랭킹 기준은 고정밀도 솔루션을 유도.
  - 예시: SGD Restaurants 도메인의 INFORM(slot) 행동의 실제 전제조건은 requested_slot[slot] 임.
    - 예측된 전제조건은 $$(requested_slot[slot] \text{ and } query_success == \text{True})$$.
    - 모든 실제 발생 사례에서 $$query_success == \text{True}$$여서 정밀도는 1.0.
    - 다만, $$query_success \neq \text{True}$$ 경우를 다루지 못해 재현율은 0.57로 낮음.
  - Neural Precondition baseline은 다른 방법 대비 성능 저조.
    - 부정적 샘플 생성 시 가정이 항상 성립하지 않음.
    - 특히 Alfworld 벤치마크에서 탐색용 내비게이션 행동들이 여러 개 가능해 이슈 발생.
  - 생성된 데이터 라벨과 실제 라벨 간 (정밀도, 재현율)은 각각 SGD에서 (1, 0.74), Alfworld에서 (1, 0.60)임.
    - 긍정적 사례 라벨은 100% 정확한 반면, 부정적 사례 라벨은 불완전.
  - 분류기 학습에 실제 전제조건 라벨을 사용할 경우 F1: SGD 0.81, Alfworld 0.67로 향상되나, 미지 시나리오에서의 일반화는 제한적.

- **정성적 결과**
  - SGD의 Buses 도메인에서 모든 행동 유형에 대해 전제조건을 예측함.
  - 제안 모델은 전제조건에 대한 직접적인 감독 없이 전문가 행동 시연만 관찰하여 후보를 예측.

- **추가 분석**
  - 추론된 행동 전제조건이 향상된 에이전트 정책 구축에 어떻게 기여하는지 후속 분석 진행.

---

# 4.3 PRECONDITION-AWARE AGENT POLICY

- **평가지표 (Metrics)**  
  - SGD 벤치마크에서는 행동 시연(demonstrations)에 나타난 행동을 기준으로 정책 성능을 평가.  
  - 에이전트가 한 번에 여러 행동을 예측해야 하므로, 시연 행동을 정답으로 하여 F1 점수를 계산.  
  - Embodied textworld 작업에서는 시뮬레이션 환경을 제공하며 성공률(SR, success rate)을 정의하여 주어진 작업을 성공적으로 완료한 빈도를 측정.  
  - 전제조건 호환성(precondition compatibility)을 정의하여 예측된 행동이 정답 전제조건을 위반하지 않고 호환되는 빈도를 측정.

- **기준 정책 (Baselines)**  
  - **BC (Behavioral Cloning)**  
    - 전문가 시연에 대해 코드 모델을 LoRA로 미세조정하여 언어 모델링 학습 목표로 학습한 기준점.  
  - **Act (Yao et al., 2022)**  
    - 훈련 시연을 프롬프트로 사용하여 사전학습된 모델이 다음 행동을 예측하는 몇 샷 프롬프트 기반 방법.  
  - **ReAct (Yao et al., 2022)**  
    - 자연어 사유(chain-of-thought rationale)를 행동 예측 전 예측하는 변형된 프롬프트 기법.  
    - 'think' 프롬프트를 코드 주석으로 시연에 삽입하며, 매 단계별 에이전트가 선택적 코드 주석('think' 단계)을 생성 후 행동을 예측.  

- **모델 및 전제조건 사용**  
  - 모든 정책 방법에는 기본적으로 StarCoder 16B 모델을 사용.  
  - 모든 실험에서 StarCoder 16B 모델이 예측한 전제조건을 활용.

- **결과 (Results)**  
  - Table 3에 각 기준 정책(BC, Act, ReAct) 및 각 정책에 우리의 전제조건 기반 방법을 추가한 성능 결과를 제시.  
  - 전제조건을 활용한 행동 샘플링 전략이 각 기준 정책 대비 일관된 성능 향상을 가져옴.  
  - 특히 제안된 전제조건 기반 추론이 모델이 정답 전제조건과 더욱 정확하고 일관된 행동을 생성하는 데 도움을 줌.  
  - Alfworld 벤치마크에서 React 에이전트의 성공률이 44%에서 48%로, 전제조건 호환성은 81%에서 96%로 향상.  
  - 자연어 사유 생성과 행동 예측 단계의 시너지 효과를 보여줌.  
  - 전제조건을 명시적으로 추론하는 것이 더 나은 정책 구축에 기여함을 증명.

- **의의 및 확장 가능성**  
  - 몇 샷 프롬프트는 제한된 감독으로 언어/코드 모델이 작업 수행 가능하게 하지만, 컨텍스트 창에 맞는 시연 개수에 한계가 있음.  
  - 여러 궤적의 정보를 소수의 규칙(예: 전제조건)으로 압축하는 것은 이러한 한계를 뛰어넘을 수 있음.  
  - 이 아이디어는 일반적으로 다른 작업들에도 적용 가능할 것으로 기대됨.

---

# 4.4 ABLATIONS AND ANALYSIS

- **목적**: 파이프라인 내 다양한 구성 요소의 영향을 이해하기 위해 일련의 ablation 실험 수행.

- **전제조건(precondition) 정보 통합 방법 분석 (Figure 2)**  
  - (i) 프롬프트에 전제조건 정보 포함  
  - (ii) 전제조건 인지 기반 행동 샘플링 전략  
  - 결과:  
    - 프롬프트에 전제조건 정보를 포함하는 것은 전반적으로 도움이 되지만,  
    - 제안된 전제조건 인지 행동 샘플링 전략이 더 일관되고 유의미한 성능 향상을 가져옴.  
    - 1-shot 설정에서 예를 들어, 예측된 전제조건을 사용할 때 F1 점수가 0.64에서 0.68로, 정확한 전제조건(grouth-truth) 사용 시 0.73으로 향상됨.  
    - 전제조건 호환성(precondition compatibility)은 2B 및 16B 모델 모두에서 0.9 이상으로 개선됨.

- **감독 데이터 양의 영향 (Figure 3)**  
  - CodeGen 2B 모델은 최대 컨텍스트 크기 2048 토큰 제한으로 인해 최대 4개의 시연(demonstrations)만 사용 가능.  
  - 전제조건에 대한 지식은 시연 수가 적을 때 특히 유용.  
  - 시연 수 증가 시에도 전제조건에 대한 명시적 추론이 모델 성능 향상에 지속적으로 기여.  
  - 정확한 전제조건(ground-truth)을 활용할 경우 정책(policy) 성능이 더 향상됨을 확인.

- **모델 규모 영향**  
  - 작은 모델(2B)이 큰 모델(16B)보다 전제조건 지식으로부터 더 큰 혜택을 받음.  
  - 작은 모델의 추론 능력 강화가 중요하며, 큰 모델은 비용과 계산량이 많이 요구됨.  
  - 전제조건 정보 활용과 실행 기반 검증은 작은 모델 성능 향상을 위한 유망한 전략임.

- **표 (Figure 2) 요약:**

  | 모델 크기 | Prec. prompt | Prec. sample | 1-shot F1 ↑ | 1-shot Cmp.↑ | 10-shot F1 ↑ | 10-shot Cmp.↑ |
  |--------|--------------|--------------|-------------|--------------|--------------|---------------|
  | 2B     | ✗            | ✗            | 0.51        | 0.73         | -            | -             |
  | 2B     | ✓            | ✗            | 0.52        | 0.75         | -            | -             |
  | 2B     | ✓            | ✓            | 0.58        | 0.92         | -            | -             |
  | 16B    | ✗            | ✗            | 0.64        | 0.78         | 0.89         | 0.97          |
  | 16B    | ✓            | ✗            | 0.65        | 0.77         | 0.90         | 0.97          |
  | 16B    | ✓            | ✓            | 0.68        | 0.90         | 0.91         | 0.99          |

- **그래프 (Figure 3) 요약**  
  - 시연 수 (1~10) 변화, 모델 크기(CodeGen 2B vs StarCoder 16B), 전제조건 정보 종류(없음 / 예측된 / 정확한) 별 정책 성능(F1 점수)을 비교.  
  - 결과: 전제조건 정보를 더 많이, 정확히 알수록 그리고 모델 크기가 클수록 성능 개선 폭이 증가.  
  

- **수식 표현 예 (전제조건 호환성 개선):**  
  $$
  Cmp. > 0.9 \quad \text{(both 2B and 16B models)}
  $$

---

# 5 RELATED WORK

- **프로그램을 정책으로 보는 접근 (Programs as Policies)**  
  - 기존 연구(Liang et al., 2022; Singh et al., 2022)는 로봇 정책을 자연어가 아닌 코드/프로그램으로 보는 관점을 제시함.  
  - LLM은 예제 작업에 해당하는 프로그램을 보여줘 프롬프트하며, 질의 작업에 대한 프로그램을 생성하도록 요구됨.  
  - 프로그램은 대상 로봇 API나 서드파티 라이브러리 함수로 구성될 수 있음.  
  - 코드 주석은 고수준 작업을 세부 작업으로 분해하며, 어설션을 통해 환경 피드백을 반영하고 오류 복구 메커니즘을 제공.  
  - 이들 연구는 전제 조건(precondition) 정보를 프롬프트에 명시적으로 포함시키는 반면, 본 연구는 행동 궤적(action trajectories)에서 전제 조건 정보를 발견하려 시도함.

- **검증을 통한 추론 (Reasoning with Verification)**  
  - 자연어 모델의 성능 향상을 위해 프로그램 실행과 결합하는 연구가 있음.  
  - Liu et al. (2022): LLM과 물리 시뮬레이터를 결합하여 물리 질문에 답함.  
    - 질의에 대해 텍스트-코드 모델이 감독학습으로 프로그램을 생성 → 시뮬레이터에서 실행 → 출력값을 LLM에 입력하여 응답 생성.  
  - Gao et al. (2022): 산술 추론 문제에서 자연어 체인오브생각(chain-of-thought) 문장과 프로그램 문장을 교차 사용하여 8단계 계산 수행.  
  - 본 연구도 실행을 통한 검증이 특히 소형 언어 모델의 성능 개선에 도움이 됨을 확인함.

- **프롬프트를 통한 추론 (Reasoning via Prompting)**  
  - 체인오브생각 프롬프트(Wei et al., 2022)는 언어모델이 단계별 추론을 수행하도록 하는 강력한 기법.  
  - 이러한 기법은 계획 문제(Huang et al., 2022b; Yao et al., 2022)에도 적용돼, 에이전트가 몇 개의 시범을 보고 행동에 대한 합리적 근거를 생성하도록 함.  
  - 체인오브생각 근거는 동적으로 문제별 생성되는 반면, 본 연구는 행동 전제 조건을 포착하는 규칙 집합을 식별하여 행동 생성 과정에 대한 통제력을 높이고 규칙을 검토/수정 가능하게 함.

- **프로그램을 활용한 구조적 예측 (Structured Prediction with Programs)**  
  - 프로그램은 자연어처리(NLP)에서 구조적 예측 과제의 표현 방식으로 활용됨(Wang et al., 2022; Madaan et al., 2022; Zhang et al., 2023).  
  - 코드 모델은 절차적 실제 활동(스크립트 생성 문제)을 모델링하는 데 사용되며, 소량의 감독으로 이벤트 시퀀스를 잘 추론함.

- **서브태스크 그래프 프레임워크 (Subtask Graph Framework)**  
  - Sohn et al. (2018; 2020)는 데모로부터 서브태스크 전제 조건을 학습하기 위해 boolean 식을 이용하는 서브태스크 그래프 모델 제안.  
  - 전제 조건은 완료 여부를 나타내는 불리언 서브태스크 변수들의 논리식으로 모델링됨.  
  - ILP(귀납 논리 프로그래밍) 알고리즘이 최적의 불 대수를 식별함.  
  - Jang et al. (2023); Logeswaran et al. (2023)는 이를 실제 절차 활동 모델링으로 확장함.  
  - 본 연구는 불리언 식 대신 더 폭넓은 시나리오를 표현할 수 있는 프로그램 표현을 사용하며, 이들 연구에서 영감을 받아 전제 조건 추론 컴포넌트를 설계 및 평가함.

---

# 6 CONCLUSION

- 본 연구에서는 순차적 의사결정 환경에서 에이전트 정책 학습을 위해 프로그램을 활용하여 행동 전제 조건(action preconditions)을 추론하는 새로운 접근법을 제시하였음.
- 에이전트의 관찰과 행동을 프로그램으로 표현하는 방식을 제안하고, 전제 조건 추론 및 행동 예측을 코드 완성(code-completion) 문제로 정식화하였음.
- 사전학습된 코드 모델의 강력한 사전 지식을 활용하여, 추가적인 감독 없이 시연 궤적(demonstration trajectories)에서 행동 전제 조건을 추론하는 방법을 고안함.
- 예측된 전제 조건을 활용하는 전제 조건 인지 행동 예측 전략은 전제 조건과 일치하는 행동을 예측하도록 하여, 기존 방법들에 비해 더 높은 과제 수행 능력을 달성하였음.
- 본 연구는 코드 모델을 활용하여 행동 전제 조건을 추론하는 새로운 활발한 연구 방향을 제시함.