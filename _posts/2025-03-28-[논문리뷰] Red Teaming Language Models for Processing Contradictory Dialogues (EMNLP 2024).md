---
title: "[논문리뷰] Red Teaming Language Models for Processing Contradictory Dialogues (EMNLP 2024)"
date: 2025-03-28 17:00:00 +0900
categories:
  - Paper Review
tags:
  - EMNLP 2024
  - Persona-based Dialogue
---

대부분의 언어 모델이 대화 중 자기 모순을 일으키는 문제를 해결하기 위해, 이 연구는 모순적 대화 처리 작업을 통해 대화 내 모순적 발언을 탐지하고 수정하는 방법을 탐구합니다. 모순적 대화를 포함한 데이터셋을 개발하여 이러한 대화를 탐지 및 설명하고 수정하는 프레임워크를 제시하며, 실험 결과는 탐지 능력과 설명 제공 능력의 향상을 보여줍니다.

---


# 1 Introduction

- 대화 시스템은 언어 모델링과 학습 기술의 빠른 발전으로 최근 몇 년간 큰 진전을 이루었다.
- 인간과 기계 간의 대화를 이해하고 분석하는 데 중점을 두며, 지능형 상호작용 시스템 개발의 중요한 구성 요소로 자리 잡고 있다.
- ChatGPT와 같은 대형 언어 모델(LLMs)의 등장은 대화 관련 연구에 중요한 역할을 했다.
- 이러한 모델들은 복잡한 맥락을 이해하고 유창한 대화 응답을 생성하는 데 있어 인상적인 능력을 보여준다.
- 모델은 요약, 설명, 문의, 역할극 등의 다양한 작업에서 뛰어난 제어력을 보여준다.
- 그러나 대화 내의 의미적 갈등, 예를 들어 모순과 사실 오류는 언어 모델(LMs)이 인식하고 해결하는 데 어려움을 겪으며, 인간-기계 상호작용의 경험을 저하시킨다.
- 논리학 연구에 따르면 모순은 두 개 이상의 진술이 동시에 참일 수 없는 상황을 지칭한다.
- 이러한 불일치는 주로 기계에 의해 발생하며, 때로는 LLM도 만족스러운 응답을 제공하지 못한다.
- 예를 들어, 대화 에이전트가 처음에는 매운 음식을 못 먹는다고 했으나, 나중에는 매운 음식을 매일 먹는다고 주장하는 모순이 발생할 수 있다.
- 이러한 모순은 텍스트 이해력을 향상시키기 위한 핵심 요소로, 대화보다는 텍스트 생성, 환각 탐지, 논리적 추론 같은 분야에서 많이 연구되었다.
- 본 연구의 첫 번째 기여는 대화 내 모순적인 상황을 다루기 위한 새로운 대화 처리 과제를 제안하는 것이다.
- 이러한 과제는 인간의 의사소통 요구를 충족시킬 수 있는 발화를 생성하는 것을 목표로 한다.
- 모순이 대화에서 발생할 경우, 의미가 충돌하는 두 개 이상의 발화가 반드시 존재한다.
- 개선을 위해서는 모순적인 발화를 감지하고 적절히 수정하는 두 가지 노력이 필요하다.
- 이는 환각 처리에 관한 최근 연구에서 영감을 받아 LLM을 활용하여 대화 내 잠재적 모순을 감지하고 수정하는 방식으로 진행된다.
- 두 번째 기여로, 본 논문은 ChatGPT3와 Wikipedia를 활용한 자기모순 대화 데이터셋을 개발하였다.
- 데이터셋은 12,000개 이상의 완전한 '인간-기계' 대화를 포함하며, 그 중 6,000개 이상의 대화는 하나 이상의 모순적인 맥락을 포함한다.
- 대화 시나리오 및 15개의 일상 대화 주제를 포함하며, 모순이 있는 각 대화는 모순을 위치시키고 그 구체적인 표현을 설명하는 진술을 제공한다.
- 세 번째 기여로, 주요 대화 모델 외에 모순을 감지하고 설명하는 또 다른 분석기 LM을 통해 Red Teaming 프레임워크를 제안한다.
- 이 프레임워크는 모델의 모순 인식 및 감지 능력을 향상시키기 위해 원래 LLM을 처음으로 개선한다.
- 모순의 전반적인 이해를 보장하기 위해 모델은 형식화된 진술을 제공해야 한다.
- Red Teaming LM은 대화 내 모순을 감지할 뿐만 아니라 모순을 설명하고 진술에 근거한 수정 요청을 한다.
- 실험 결과 제안된 Red Teaming 프레임워크가 여러 LLM 시리즈의 대화 모순 감지 정확도와 완전성을 크게 향상시킴을 입증하였다.


---

# 2 Task and dataset

- **대화 모순 해결 작업 정의**
  - 대화 내에서 모순을 감지하고 해결하는 과제
  - 대화의 흐름을 유지하며 일관성을 확보하는 것이 목표

- **기여한 데이터셋 소개**
  - 다양한 대화 시나리오를 포함
  - 모순이 존재하는 대화와 그에 대한 해법을 제공
  - 학습 및 평가 목적을 위한 데이터 구성

- **데이터셋 구성 요소**
  - 대화 발화 두 개 $$u_1$$, $$u_2$$가 주어졌을 때, 모순성 판단
  - 레이블로 '모순' 또는 '모순 아님'을 표기

- **평가 기준**
  - 정확도: 모델의 모순 감지 및 해결 능력 측정
  - 대화 이해 및 자연어 처리 모델 개선을 목표

- **도메인 적용 가능성**
  - 고객 서비스, 비서 시스템 등 다양한 응용 분야
  - 사용자 경험 향상을 위한 대화 시스템의 필수 요소

- **데이터셋의 한계점 및 개선 방향**
  - 실제 대화 상황을 반영한 시나리오 추가 필요
  - 다양한 언어와 문화적 맥락 고려

이 내용들을 통해 대화 모순 해결 문제를 더 깊이 이해하고 정확한 모델 개발을 지원할 수 있습니다.

---


# 2.1 Task Definition

- 대화(C)는 $$\{u_0, u_1, \ldots, u_{\vert C \vert}\}$$으로, 여기서 $$u_i$$는 대화의 발화를 나타냄.
- 목표: 대화에서 모순 해결 과정을 간소화하는 것.
- 두 개의 하위 작업으로 나뉨: 모순 탐지와 모순 수정.

### 모순 탐지 (Contradiction Detection)
- 입력: 대화 C.
- 출력: y, 대화 C에 자기모순이 있는지 여부를 나타냄.
- 언어 모델(LM)에 따라, 출력은 이진 레이블 y이며, 대화 C에 모순이 있는지 여부를 나타냄.
- y는 "yes" 또는 "no"로 생성된 텍스트 레이블로 표현되며, 이는 각각 모순이 있음/없음을 의미함.

### 모순 수정 (Contradiction Modification)
- 모순이 탐지되면, LM은 모순 발화를 수정하여 대화 C에 있는 어떤 두 문장 사이에도 논리적 불일치가 없도록 해야 함.
- $$u_i$$가 $$u_{i+k}$$와 모순될 경우, 이는 보통 기계에 의해 생성된 자기모순임.
- $$u_i/u_{i+k}$$ 또는 둘 다 ($$u_i, u_{i+k}$$) 수정 가능.
- 두 가지 수정 전략:
  1. **직접 수정 (Direct Edit):** 모순이 발생한 $$u_i$$ 또는 $$u_{i+k}$$를 수정.
  2. **공동 수정 (Joint Edit):** $$u_i$$와 $$u_{i+k}$$를 동시에 수정하여 모순 해결.


---

# 2.2 Dataset

- **데이터 수집**:
  - 인간 논리에 반하는 모순을 효과적으로 작성하기 어려움.
  - 영화, 음식, 관광, 스포츠 등 일상 대화 주제에 대한 Wikipedia에서 키워드 추출.
  - 주제에 따라 키워드를 분류.

- **모순 대화 생성**:
  - Wei et al. (2022), Ouyang et al. (2022) 연구를 참고하여 ChatGPT를 사용하여 합성 데이터 생성.
  - ChatGPT는 지침과 구체적 예시를 통해 원하는 형식의 데이터를 제공 가능.
  - ChatGPT 사용 이유:
    - 데이터 중복 최소화 및 대화 품질 보장.
    - 인간 수준의 모순 탐지 및 설명 성과.
    - 고품질 모순 설명 생성 및 비용 및 편향 감소.

- **데이터셋 구성**:
  - 총 12,387개의 대화 포함: 모순 및 비모순 대화.
  - 모순 대화 6,130개 수집, 각 대화에 설명 포함.
  - 한 대화 평균 단어 수: 46.5 (4문장), 문장 당 평균 11.6단어.
  - 설명의 평균 단어 수: 16.7.
  - DailyDialog와 Wizard of Wikipedia 주제를 참고하여 15개의 일상 주제 선택.
  - 훈련 세트: 모순 4,839개, 비모순 4,820개.
  - 테스트 세트: 모순 1,291개, 비모순 1,437개.

- **퀄리티 평가**:
  - 생성된 모순 대화 중 200개를 무작위로 선택하여 인간 평점자 두 명이 평가.
  - 평가 일치도: 카파 계수 0.76 및 0.72.

- **주제 분포**:
  - 다양한 주제를 포함하여 내용 다양성 및 품질 보장을 목표로 함. 

$$
S(e, e_g) = S_1(e, e_g) + \eta S_2(e, e_g)
$$

- $$S(e, e_g)$$에서 $$\eta$$는 스케일 팩터, $$(0 < \eta < 1)$$.

---


# 3 Red Teaming Language Models for Contradictory Dialogue

- **레드팀 프레임워크 학습 단계**
  - **단계 1**: 기본 언어 모델(LM)을 검출 작업 목표에 맞게 미세 조정.
  - **단계 2**: 미세 조정된 LM(분석기 LM 혹은 aLM)이 대화에서 서로 모순되는 설명을 생성하고 그것을 검증.
  - **단계 3**: 레드팀 LM(rLM)을 사용하여 대화에서 모순이 있는 부분을 수정.
    - 이전 단계에서 생성된 설명문을 활용하여 논리적 프롬프트 부족을 보완.

- **과정의 효과**
  - 대화에서 모순을 식별하고 이해하는 LLM의 능력 향상.

- **개별 단계의 기술적 세부 사항**: 대화 모순 해결을 위한 각 단계의 기술적 세부 사항을 제시.


---


# 3.1 Framework Overview

- 대화 모순 해결 하위 작업을 다루기 위해 프레임워크는 세 가지 단계를 거침.
  1. 모순 탐지
  2. 모순 설명
  3. 대화 수정


---


# 3.2 Resolving Dialogue Contradiction

- **모델 세부사항**
  - theaLM 모델 파인튜닝: 대화 맥락에 기초한 의미적 레이블 생성.
  - 모델의 추론 능력과 파라미터 수 고려 (Kaplan et al., 2020).

- **데이터 전처리**
  - 모든 모순 대화는 수동으로 검토 및 레이블링.
  - DailyDialog와 Wizard of Wikipedia의 대화는 모순이 없다고 가정 (Nie et al., 2021).

- **모델 선택**
  - 7~13억 개 파라미터 모델 선택 (Chung et al., 2022; Touvron et al., 2023b; Chiang et al., 2023; Taori et al., 2023; Jiang et al., 2023).
  - 오토리그래시브 LM이 마스킹 LM보다 모순 감지에 유리한 장점.
  - 설명 생성이 가능한 오토리그래시브 LM을 통해 문제 맥락 파악.

- **aLM 구현**
  - 제로샷 및 퓨샷 방법으로 모델의 모순 감지 능력 평가.
  - 탐지 프로세스 프롬프트 $$p$$ 구성: 대화 $$C$$와 지시 $$i$$.
  - 퓨샷 시나리오를 위한 모순 대화들 $$(C_m, C_n)$$ 추가.
  - 대화 $$C$$에서 모순 여부 판단 요청.

- **훈련 단계**
  - 주어진 대화 $$C$$에 대해 바닐라 LM을 인스트럭션 튜닝으로 파인튜닝.
  - 판단 레이블 $$s: p(s\vert C,i)$$ 생성.
  - $$s$$는 'yes' 또는 'no'로 모순 여부 판단.
  - 인스트럭션 $$i$$는 제로샷 테스트와 동일.
  - 혼합된 모순 및 비모순 대화 데이터셋 랜덤 튜닝: 주제 분포 효과 방지를 위해.



---


# 3.2.1 Contradiction Detection

- 이진 분류는 모델이 모순점을 이해하는 정도를 반영하지 못함.
- aLM을 훈련하여 모순과 관련된 특정 설명 $$e$$를 생성하도록 함.
- 모순 설명이 활성화되면, aLM은 판단 레이블 $$s$$와 해당 설명 $$e$$를 생성하도록 훈련됨.
- 설명 $$e$$는 다음 조건을 만족해야 함:
  - $$s$$와 의미적으로 일치해야 함 (즉, $$s$$와 $$e$$ 사이에 의미적 충돌 없음).
  - 대화 $$C$$ 내에서 어떤 발화들이 모순적인지 명시해야 함.
  - 모순에 대한 구체적인 이유를 포함해야 함.
- 생성된 설명의 적합성 평가가 중요함.
- 생성된 설명 $$e$$를 데이터셋의 레이블된 설명 $$e_g$$와 비교하여 평가.
- $$S(e,e_g)$$는 다음과 같이 정의됨: $$S(e,e_g) = S_1(e,e_g) + \eta S_2(e,e_g)$$
  - 여기서 $$\eta$$는 스케일 팩터이고 $$0 < \eta < 1$$.
  - $$S_1$$과 $$S_2$$는 생성된 텍스트 $$e$$와 참조 텍스트 $$e_g$$ 사이의 의미적 유사성 점수.
  - 두 평가 방법의 가중 합을 이용하여 편향을 피함.
- $$S(e,e_g) > \tau$$일 경우, LM이 생성한 설명 $$e$$는 대화 $$C$$의 모순점을 설명할 수 있다고 봄.
- $$S(e,e_g) \leq \tau$$이면, $$e$$는 대화의 모순을 설명하기에 불충분하다고 간주됨.
- $$\tau$$의 값은 인간 평가의 점수 기준에 따라 설정됨.


---


# 3.2.2 Contradiction Explanation

- 모순 감지 후, Red Teaming rLM과 $$S(e, eg) \ge \tau$$을 사용하여 C에서 모순을 수정.
- 프롬프트 내 지침을 제공.
  - 수정 시 $$e$$ 사용 여부 및 수정 전략 포함.
- §2.1의 Direct Edit:
  - $$u_{i+k}$$을 $$u_i$$에 일치하도록 일반 수정.
  - 대화 생성을 통해 구문이 일관된 논리를 따름.
- Joint Edit:
  - 설명을 통해 모순 진술 식별하고 rLM이 모순 부분 조정.
  - 논리적 일관성 유지를 위해 모순 진술 사이의 $$c_{i+1,i+k-1} = \{u_{i+1}, u_{i+2}, ..., u_{i+k-1}\}$$을 수정할 필요가 있음.


---


# 3.2.3 Dialogue Modification

- 제안된 Red Teaming 프레임워크를 모순된 대화 처리에서 평가하기 위해 여러 비교 실험을 수행.
- LM(언어 모델)이 생성한 설명이 수정 과정에서 도움이 될 수 있음을 발견.
- 문제 정의상 이를 명시적으로 요구하지는 않음 (§2.1).
- 11615 데이터셋을 사용하여 실험을 진행.
- 모순된 대화 탐지 및 설명 평가뿐만 아니라 LM 생성 설명이 최종 작업에 미치는 영향도 평가 (§4.2-§4.4).
- 생성된 결과를 보여주기 위해 사례 연구도 제공.


---

# 4 Experiments

- Lora (Hu et al., 2022)를 사용하여 BF16에서 기본 LMs를 미세 조정.
- 설정:
  - $$\text{lora\_r} = 4$$
  - $$\text{lora\_alpha} = 8$$
  - $$\text{lora\_dropout} = 0.05$$
  - 학습률: $$2 \times 10^{-5}$$
- 7B LM을 네 개의 A10 GPU에서 3 에폭 동안 학습.

---


# 4.1 Experiment Details

- **실험 개요**: 대화에서 모순을 탐지하는 여러 베이스라인과 오픈 소스 LLM의 정확도, 리콜, F1 점수를 비교.
- **모델**:
  - BERT (Devlin et al., 2019)
  - RoBERTa (Liu et al., 2019)
  - LLaMA2-7B/13Bchat (Touvron et al., 2023b)
  - Vicuna-7B (Chiang et al., 2023)
  - Mistral-7B (Jiang et al., 2023)
  - LLaMA3-8B-Instruct
- **평가 기준**:
  - 기본 LMs는 인간 평가에 따라 구분 기준이 설정 (부록 §A.2에 설명).
  
- **결과**:
  - 표 1에 모순 대화 탐지 결과 제시.
  - LLaMA2는 BERT와 RoBERTa 같은 작은 인코더보다 모순 탐지 능력이 부족함을 보임, 심지어 13B 모델 스케일에서도 동일.
  - 상대적으로 성능이 좋은 Vicuna와 Mistral도 확신할 만한 결과를 내지 못함.
  - 이는 대화 생성을 위한 생성 불안정성과 모순 및 추론 판단 능력 부족 때문.
  - 이러한 문제로 인해 대화 자가모순 발생 빈도가 증가.
  - 반면, 미세 조정은 LLM을 모순 탐지 작업에 맞춤.
  - Vicuna와 LLaMA2의 높은 리콜은 이 모델들이 대화에서 모순을 탐지할 때 더 기울어짐을 시사.
  
- **추가 정보**:
  - 탐지 결과에 대한 추가 정보는 §4.3에서 제공.


---

# 4.2 Dialogue Contradiction Detection

- **모델 분석**: 대화에서 모순을 탐지하고 설명하는 모델들의 성능 분석.
- **선택한 모델**: LLaMA2-chat 시리즈 사용.
- **모형 성능 결과**:
  - **BERT**: 정확도 67.4, F1 65.2, 재현율 60.9
  - **RoBERTa**: 정확도 68.3, F1 64.1, 재현율 62.2
  - **LLaMA2-7B-chat**: 정확도 33.6, F1 43.2, 재현율 54.2
  - **LLaMA2-13B-chat**: 정확도 50.6, F1 65.5, 재현율 92.8

- **파인튜닝된 모델 성능** $$*$$:
  - **Vicuna-7B$$*$$**: 정확도 96.7, F1 95.8, 재현율 92.6
  - **Mistral-7B$$*$$**: 정확도 97.4, F1 96.3, 재현율 94.5
  - **LLaMA2-7B-chat$$*$$**: 정확도 95.2, F1 95.3, 재현율 90.9

- **평가 기준**:
  - **레이블 일관성**: 2 (일치), 1 (일부 관련), 0 (무관)
  - **유창성**: 2 (읽기 쉬움), 1 (문법적으로 형성됨), 0 (읽기 어려움)
  - **완전성**: 2 (완전한 설명), 1 (불완전), 0 (실질적 설명 없음)

- **자동 평가**:
  - **메트릭**: BERTScore, BARTScore 사용.
  - $$P_{\alpha}$$: 설명의 비율 $$S>\alpha$$.

- **인간 평가**:
  - **기준**: 레이블 일관성, 유창성, 완전성 기준으로 각 모델의 설명 평가.
  - **결과**:
    - **LLaMA2-chat**: 모든 평가 지표에서 뛰어난 성능.
    - **LLaMA3-Instruct**: 유창성에서 낮은 성능, 하지만 레이블 일관성 양호.

- **결과 해석**:
  - **Vanilla 모델**들은 모순 탐지 성능에 비해 설명 성능이 낮음.
  - **LLaMA2-chat**은 **Vicuna**와 **Mistral**보다 더 나은 설명 능력.
  - 대화 데이터 맞춤이 성능 향상에 기여했을 가능성.
  - **LLaMA3-8B-Instruct**는 반복적이고 관련 없는 텍스트 생성.
  - 추가 정보 생성과 대화 맥락 반복이 설명 성능 저하에 기여.

- **일반적 결론**:
  - 레이블 일관성이 높을수록 모델의 설명 성능이 개선됨.
  - 더 큰 모델이 더 나은 설명 능력 보임.
  - **LLaMA3-Instruct**는 대화 정렬 최적화로 인해 설명 성능이 낮음.

---


# 4.3 Contradiction Explanation

- **기본 내용**:
  - 모순 수정의 마지막 하위 작업을 평가.
  - 미세 조정된 LM을 rLM으로 사용하여 모순된 대화를 수정.
  
- **설정 및 평가 방식**:
  - 설명이 있는 경우와 없는 경우 두 가지 프롬프트 설정 사용.
  - 자동 및 인간 방법으로 평가.
  - 자동 평가: 수정 전후 모순 대화의 비율 변화를 비교.
  
- **결과**:
  - 모든 rLMs는 주어진 모순 대화를 수정하는 능력 일부를 보여줌.
  - 설명이 포함된 프롬프트는 그렇지 않은 경우보다 더 나은 수정 범위를 보여줌.
  - 이는 모순 설명의 품질과 대화 내 효과적인 로컬라이제이션을 반영.
  
- **세부 모델 성능**:
  - LLaMA2-chat는 일련의 더 나은 결과를 보임.
  - 기본 13B 모델은 미세 조정된 7B 모델과 거의 유사한 성능을 보임.
  
- **통계 및 수치 (Tab. 5 참고)**:
  - (설명 없는 경우) 수정되지 않은 모순 비율 49.34%.
  - Vicuna-7B의 경우, 설명 포함시 5.21%.
  - Mistral-7B의 경우, 설명 포함시 4.25%.
  - LLaMA2-7B-chat의 경우, 설명 포함시 3.85%.
  
- **추가 정보**:
  - Mistral-7B 미세 조정 버전을 탐지 모델로 사용.
  - 시험 중 프롬프트는 §3.2.3의 수정 전략 포함하지 않음.
  - 생성된 모순 설명의 구체적인 내용과 모순 수정 결과는 Appx. §A.5에서 상세히 다룸.


---


# 4.4 Contradiction Modification

- **Red Teaming의 필요성**
  - 책임 있는 대형 언어 모델(LLM)에 대한 요구가 증가하면서 Red Teaming이 주목받고 있음.
  - Red Teaming은 수동 검토를 보완하고, LMs의 부적절한 부분 검출을 자동화하여 오류를 줄이는 데 도움을 줌.
  - 예시: LLMs가 악의적인 역할을 수행하도록 하여 모델의 한계를 노출시키는 연구가 있음.

- **대화에서의 모순 처리**
  - 이전 연구에서는 성격, 지식, 주제 시나리오 간의 일관성을 개선하는 방법을 탐구하였음.
  - 모순 상황을 직접 해결하려는 노력은 드물었음.
  - Qin et al. (2021)은 임무 지향 대화 시나리오에 제한된 솔루션을 제안.
  - Zheng et al. (2022)는 다양한 모순 유형에 기반한 데이터셋을 개발했으나, 대화의 길이와 주제 다양성은 제한적이었음.
  - Nie et al. (2021)은 대화 모순 탐지를 위한 데이터셋과 방법을 제안했으나, 우리 데이터셋은 각 모순 대화에 대한 설명을 포함하고 있음.

- **제안된 프레임워크**
  - 이진 모순 탐지에서 확장하여, 탐지된 모순을 설명하고 해결하는 것을 포함함.
  - 이진 탐지만으로는 모델이 모순을 이해하는 정도를 충분히 포착할 수 없음.


---


# 5 Related work

- **대화 모순 탐지 연구 분야**:
  - 과거 연구에서는 대화의 논리적 일관성에 초점.
  - 대화 모순 탐지 및 수정은 상대적으로 덜 연구됨.
  - 기존의 연구들은 주로 자연어 추론(NLI)을 기반으로 대화 모순을 식별.

- **자연어 추론(NLI)와 대화 모순**:
  - NLI는 문장 쌍의 관계를 추론하여 모순 여부를 결정.
  - 대표적인 NLI 데이터셋으로 SNLI, MNLI 등을 사용.
  - 이러한 데이터셋은 대화보다는 문장 기반으로 설계됨.

- **대화 데이터셋**:
  - Dialogue NLI: 대화 맥락 내 모순 탐지를 위한 데이터셋.
  - 기존 대화 데이터들은 모순 탐지보다는 이해 및 생성에 중점.

- **강화 학습 및 모델 훈련**:
  - 최근 대화 모델은 주로 강화 학습을 통해 일관성 향상.
  - 강화 학습은 대화 에이전트의 반응과 환경 상호작용에 효과적.

- **담화 구조와 의미 분석**:
  - 의미론적 프레임워크와 담화 구조 분석도 일관성 체크에 중요.
  - 의미론적 역량을 바탕으로 한 대화 모순식별이 연구되고 있음.

- **관련 기술 및 응용**:
  - Google, Amazon 등의 기술기업은 대화 AI 개발에 투자.
  - 인공지능의 윤리적 문제 해결을 위한 데이터셋 구축 및 표준화 필요.


---

# 독자 의견

- 본 논문에서는 대화 시스템이 모순된 발화를 하는 문제에 초점을 두었다.
- 제안하는 프레임워크는 모순대화를 감지하고 이에대한 설명을 생성하는 aLM과, 모순을 수정하는 rLM을 사용한다.
- 이러한 모순 수정 프레임워크는 Persona-based Dialogue 등의 분야에서 대화 일관성을 향상하는데에 사용될 수 있을 것이다.